[
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Mini-challenge 2 focuses on analyzing ship movements and shipping records to understand illegal fishing practices. FishEye analysts need help creating visualizations to show patterns of ship movements and identify suspicious behaviors. They also want to understand how the commercial fishing community changed after a company was caught fishing illegally.\nThe details of the mini challenge can be found here.\n\n\n\nFishEye analysts need your help to perform geographic and temporal analysis of the CatchNet data so they can prevent illegal fishing from happening again. Your task is to develop new visual analytics tools and workflows that can be used to discover and understand signatures of different types of behavior. Can you use your tool to visualize a signature of SouthSeafood Express Corp’s illegal behavior? FishEye needs your help to develop a workflow to find other instances of illegal behavior.\n\nFishEye analysts have long wanted to better understand the flow of commercially caught fish through Oceanus’s many ports. But as they were loading data into CatchNet, they discovered they had purchased the wrong port records. They wanted to get the ship off-load records, but they instead got the port-exit records (essentially trucks/trains leaving the port area). Port exit records do not include which vessel that delivered the products. Given this limitation, develop a visualization system to associate vessels with their probable cargos. Which vessels deliver which products and when? What are the seasonal trends and anomalies in the port exit records?\nDevelop visualizations that illustrate the inappropriate behavior of SouthSeafood Express Corp vessels. How do their movement and catch contents compare to other fishing vessels? When and where did SouthSeafood Express Corp vessels perform their illegal fishing? How many different types of suspicious behaviors are observed? Use visual evidence to justify your conclusions.\nTo support further Fisheye investigations, develop visual analytics workflows that allow you to discover other vessels engaging in behaviors similar to SouthSeafood Express Corp’s illegal activities? Provide visual evidence of the similarities.\nHow did fishing activity change after SouthSeafood Express Corp was caught? What new behaviors in the Oceanus commercial fishing community are most suspicious and why?\n\n\n\n\n\n\n\n\npacman::p_load(tidyverse, jsonlite, DT, lubridate,\n               igraph, tidygraph, ggraph, \n               visNetwork, sf)\n\n\n\n\nLoading the .json data using jsonlite package.\n\nmc2_data &lt;- fromJSON(\"data/MC2/mc2.json\")\n\nmc2 is a directed multigraph, consists of nodes dataframe and links dataframe.\n\noceanus_map &lt;- read_sf(\"data/MC2/Oceanus Information/Oceanus Geography.geojson\")\n\nLoading the oceanus map:\n\nggplot(oceanus_map) +\n  geom_sf(color = \"black\",\n          ) +\n  theme_void() +\n  geom_sf_text(aes(label = Name), size = 2,\n               vjust = 1.5)\n\n\n\n\n\n\n\n\n\n\n\n\nmc2_nodes_raw &lt;- as_tibble(mc2_data$nodes)\n\n\nmc2_edges_raw &lt;- as_tibble(mc2_data$links)\n\n\n\n\n\ncolSums(is.na(mc2_nodes_raw))\n\n                type      _last_edited_by          _date_added \n                   0                    0                    0 \n   _last_edited_date          _raw_source           _algorithm \n                   0                    0                    0 \n                name                   id                 Name \n                5627                    0                 5317 \n         Description           Activities                 kind \n                5623                    0                 5613 \n            qty_tons                 date         flag_country \n                 330                  330                 5341 \n             company              tonnage       length_overall \n                5458                 5359                 5354 \n               style fish_species_present \n                5635                    0 \n\n\n\ncolSums(is.na(mc2_edges_raw))\n\n             type              time             dwell   _last_edited_by \n                0             13101             13101                 0 \n      _date_added _last_edited_date       _raw_source        _algorithm \n                0                 0                 0                 0 \n           source            target               key              date \n                0                 0                 0            258542 \n      data_author          aphorism  holiday_greeting            wisdom \n           269156            269669            270639            269719 \nsaying of the sea \n           269750 \n\n\n\n\n\nAs the _date_added and _last_edited_date contains a mixture of format, we first extract the date in “yyyy-mm-dd” format using substr.\n\nmc2_nodes_raw &lt;- mc2_nodes_raw |&gt; \n  mutate(`_date_added` = substr(`_date_added`,1,10)) |&gt; \n  mutate(`_date_added` = ymd(`_date_added`)) |&gt; \n  mutate(`_last_edited_date` = substr(`_last_edited_date`,1,10)) |&gt; \n  mutate(`_last_edited_date` = ymd(`_last_edited_date`)) |&gt; \n  mutate(date = ymd(date))\n\n\nmc2_edges_raw &lt;- mc2_edges_raw %&gt;% \n  mutate(`_date_added` = substr(`_date_added`,1,10)) %&gt;% \n  mutate(`_date_added` = ymd(`_date_added`)) %&gt;% \n  mutate(`_last_edited_date`= substr(`_last_edited_date`,1,10)) %&gt;% \n  mutate(`_last_edited_date` = ymd(`_last_edited_date`)) %&gt;% \n  mutate(time = ymd_hms(time))\n\nUnderstanding the Data\n\nunique_nodes_type &lt;- mc2_nodes_raw |&gt;  distinct(type)\nunique_nodes_type\n\n# A tibble: 12 × 1\n   type                          \n   &lt;chr&gt;                         \n 1 Entity.Commodity.Fish         \n 2 Entity.Location.City          \n 3 Entity.Document.DeliveryReport\n 4 Entity.Vessel.FishingVessel   \n 5 Entity.Vessel.Other           \n 6 Entity.Vessel.Ferry.Passenger \n 7 Entity.Vessel.CargoVessel     \n 8 Entity.Vessel.Ferry.Cargo     \n 9 Entity.Vessel.Research        \n10 Entity.Vessel.Tour            \n11 Entity.Location.Point         \n12 Entity.Location.Region        \n\n\n\nunique_edges_type &lt;- mc2_edges_raw |&gt;  distinct(type)\nunique_edges_type\n\n# A tibble: 3 × 1\n  type                                \n  &lt;chr&gt;                               \n1 Event.TransportEvent.TransponderPing\n2 Event.Transaction                   \n3 Event.HarborReport                  \n\n\n\n\n\nTo get aggregated weight for the edges by unique source, target and type.\n\nmc2_edges_agg &lt;-\n  mc2_edges_raw %&gt;%\n  distinct() %&gt;%\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type)) %&gt;%\n  group_by(source, target, type) %&gt;%\n  summarise(weights = n()) %&gt;%\n  filter(source!=target) \n  ungroup\n\nfunction (x, ...) \n{\n    UseMethod(\"ungroup\")\n}\n&lt;bytecode: 0x00000252ca5223f8&gt;\n&lt;environment: namespace:dplyr&gt;\n\n\n\n\n\n\n\n\nFishEye analysts have long wanted to better understand the flow of commercially caught fish through Oceanus’s many ports.\n\nfishing_vessels_nodes &lt;- mc2_nodes_raw %&gt;% \n  filter(type == \"Entity.Vessel.FishingVessel\")\n\nThere is a total of 178 Fishing vessels in Oceanus.\n\n\n\n\nfishing_ground &lt;- mc2_nodes_raw %&gt;% \n  filter(kind == \"Fishing Ground\") \n\n\nnon_fishing_ground &lt;- mc2_nodes_raw %&gt;%  \n  filter(kind == \"Ecological Preserve\") \n\nThere are three fishing grounds:\n1. Cod Table\n2. Wrasse Beds\n3. Tuna Shelf\nFishing outside these fishing grounds are considered illegal fishing.\n\ncargo_vessels &lt;- mc2_nodes_raw %&gt;%  \n  filter(type == \"Entity.Vessel.CargoVessel\") %&gt;% \n  arrange(desc(tonnage))\ncargo_vessels\n\n# A tibble: 100 × 20\n   type        `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt;       &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 Entity.Ves… Melinda Manning   2035-02-01    2035-02-17          Oceanus Vess…\n 2 Entity.Ves… Harvey Janus      2034-12-02    2034-12-22          Oceanus Vess…\n 3 Entity.Ves… Olokun Daramola   2035-02-13    2035-03-02          Oceanus Vess…\n 4 Entity.Ves… Harvey Janus      2034-12-03    2034-12-15          Oceanus Vess…\n 5 Entity.Ves… Harvey Janus      2035-02-20    2035-03-03          Oceanus Vess…\n 6 Entity.Ves… Harvey Janus      2035-01-06    2035-01-21          Oceanus Vess…\n 7 Entity.Ves… Jack Inch         2035-02-01    2035-02-20          Oceanus Vess…\n 8 Entity.Ves… Jack Inch         2034-11-08    2034-11-21          Oceanus Vess…\n 9 Entity.Ves… Jack Inch         2034-12-12    2034-12-28          Oceanus Vess…\n10 Entity.Ves… Jack Inch         2034-12-31    2035-01-12          Oceanus Vess…\n# ℹ 90 more rows\n# ℹ 15 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, id &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;\n\n\nThere are 100 cargo vessels, with tonnage ranging from 2,100 to 76,300.\n\n\n\n\nmc2_edges_raw |&gt; \n  filter(source %in% c(\"Cod Table\",\"Wrasse Beds\",\"Tuna Shelf\")) |&gt; \n  group_by(target, source) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(desc(n))\n\n# A tibble: 557 × 3\n# Groups:   target [287]\n   target                    source          n\n   &lt;chr&gt;                     &lt;chr&gt;       &lt;int&gt;\n 1 saltyskippera9e           Wrasse Beds  1460\n 2 rainbowtroutraider4d0     Wrasse Beds  1413\n 3 europeanperchpirated9b    Tuna Shelf   1265\n 4 pinksalmonpirate2a3       Wrasse Beds  1256\n 5 tenchtaker595             Wrasse Beds  1183\n 6 pollockpirate212          Wrasse Beds  1147\n 7 fishtracker03e            Wrasse Beds  1115\n 8 yellowfintunataker08b     Tuna Shelf   1079\n 9 halibuthero9b9            Tuna Shelf   1059\n10 tigermuskellungemaster012 Wrasse Beds  1048\n# ℹ 547 more rows\n\n\n\n\n\n\nmc2_edges_raw  %&gt;%  \n  filter(source %in% c(\"Ghoti Preserve\",\"Nemo Reef\", \"Don Limpet Preserve\")) %&gt;% \n  group_by(target, source) %&gt;%  \n  summarise(n = n()) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 248 × 3\n# Groups:   target [219]\n   target                 source                  n\n   &lt;chr&gt;                  &lt;chr&gt;               &lt;int&gt;\n 1 europeanperchpirated9b Nemo Reef            1040\n 2 manatee17ea            Don Limpet Preserve   884\n 3 yellowfintunataker08b  Nemo Reef             863\n 4 anchovyassaulterb1c    Nemo Reef             843\n 5 halibuthero9b9         Nemo Reef             833\n 6 bluefishbandit8ec      Nemo Reef             782\n 7 herringharpooner843    Nemo Reef             763\n 8 pacificcodcapturer81c  Nemo Reef             719\n 9 welscatfishwrangler6ae Nemo Reef             613\n10 pompanoplunderere5d    Nemo Reef             596\n# ℹ 238 more rows\n\n\n\n\n\n\ntransponder_ping_edge_agg &lt;- mc2_edges_agg %&gt;% \n  filter(type == \"Event.TransportEvent.TransponderPing\")\n\n\ntransponder_ping_edge_agg %&gt;% \n  filter(str_detect(target, \"cargo\")) %&gt;%\n  group_by(target) %&gt;% \n  ggplot(aes(x = target, y = source)) +\n  geom_point(aes(size = weights, color = weights)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\ncargo_nodes &lt;- mc2_nodes_raw %&gt;% \n  filter(type == \"Entity.Vessel.CargoVessel\")\n\n\ndelivery_nodes &lt;- mc2_nodes_raw %&gt;% \n  filter(type == \"Entity.Document.DeliveryReport\")\n\n\ntransaction_edges &lt;- mc2_edges_raw %&gt;% \n  filter(type == \"Event.Transaction\")\n\n\nid1 &lt;- transaction_edges %&gt;% \n  select(source) %&gt;% \n  rename(id = source)\n\nid2 &lt;- transaction_edges %&gt;% \n  select(target) %&gt;% \n  rename(id = target)\n\ndelivery_nodes1 &lt;- rbind(id1,id2) %&gt;% \n  distinct() %&gt;% \n  left_join(delivery_nodes,\n            unmatched = \"drop\")\n\n\ndelivery_nodes1 \n\n# A tibble: 5,322 × 20\n   id    type  `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 carg… Enti… Junior Shurdlu    2035-11-04    2035-11-06          Tuna Shelf/e…\n 2 carg… Enti… Harvey Janus      2035-08-17    2035-08-19          Tuna Shelf/e…\n 3 carg… Enti… Junior Shurdlu    2035-08-21    2035-08-23          Tuna Shelf/e…\n 4 carg… Enti… Melinda Manning   2035-11-07    2035-11-09          Tuna Shelf/e…\n 5 carg… Enti… Harvey Janus      2035-08-24    2035-08-25          Tuna Shelf/e…\n 6 carg… Enti… Harvey Janus      2035-08-28    2035-08-28          Tuna Shelf/e…\n 7 carg… Enti… Jack Inch         2035-09-01    2035-09-01          Tuna Shelf/e…\n 8 carg… Enti… Junior Shurdlu    2035-09-05    2035-09-06          Tuna Shelf/e…\n 9 carg… Enti… Harvey Janus      2035-09-07    2035-09-07          Tuna Shelf/e…\n10 carg… Enti… Jack Inch         2035-09-12    2035-09-13          Tuna Shelf/e…\n# ℹ 5,312 more rows\n# ℹ 14 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;\n\n\n\n\n\nDevelop visualizations that illustrate the inappropriate behavior of SouthSeafood Express Corp vessels. How do their movement and catch contents compare to other fishing vessels? When and where did SouthSeafood Express Corp vessels perform their illegal fishing? How many different types of suspicious behaviors are observed? Use visual evidence to justify your conclusions.\n\n\n\nSouthSeafood Express Corp operates two fishing vessels by the id of “snappersnatcher7be” and “roachrobberdb6”.\n\ntransponder_ping_edge_agg %&gt;% \n  filter(target %in% c(\"snappersnatcher7be\",\"roachrobberdb6\")) %&gt;%\n  group_by(target) %&gt;% \n  ggplot(aes(x=target, y=source,\n             size = weights)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nsouthseafood_edge &lt;- mc2_edges_raw %&gt;% \n  filter(type == \"Event.TransportEvent.TransponderPing\") %&gt;% \n  filter(target %in% c(\"snappersnatcher7be\",\"roachrobberdb6\")) %&gt;% \n  arrange(target,time)\n\n\nssf_edges_agg &lt;-\n  southseafood_edge %&gt;%\n  distinct() %&gt;%\n  group_by(source, target, type) %&gt;%\n  summarise(weights = n()) %&gt;%\n  filter(source!=target) %&gt;% \n  ungroup\n\n\nid1 &lt;- ssf_edges_agg %&gt;% \n  select(source) %&gt;% \n  rename(id = source) \n\nid2 &lt;- ssf_edges_agg %&gt;% \n  select(target) %&gt;% \n  rename(id = target)\n\nmc2_nodes1 &lt;- rbind(id1,id2) %&gt;% \n  distinct() \n\n\nssf_graph &lt;- tbl_graph(nodes = mc2_nodes1,\n                       edges = ssf_edges_agg,\n                       directed = TRUE)\n\n\n# Add a color column to nodes\nssf_graph &lt;- ssf_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(color = case_when(\n    id %in% c(\"snappersnatcher7be\", \"roachrobberdb6\") ~ \"blue\",\n    TRUE ~ \"\"\n  ))\n\n# Create the plot\nssf_graph %&gt;% \n  activate(edges) %&gt;%\n  arrange(desc(weights)) %&gt;% \n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(color = \"\", \n                     linewidth = weights)) +\n  geom_node_point(aes(color = color, size = 10)) + \n  theme_graph() +\n  theme(\n    plot.background = element_rect(fill = \"white\", color = NA),\n    text = element_text(color = \"black\"))+\n  geom_node_text(aes(label = id), \n                 repel = TRUE, \n                 vjust = 1, \n                 hjust = 1,\n                 size = 3)\n\n\n\n\n\n\n\n\n\n\n\nThe Questions:\n\nHow did fishing activity change after SouthSeafood Express Corp was caught?\nWhat new behaviors in the Oceanus commercial fishing community are most suspicious and why?\n\nIn order to understand the change in fishing activities, we first have to determine the date where SouthSeafood Express Corp was caught. We will use this timeline as the\nThe final activities of SouthSeafood’s vessels are on 2035-05-16 (snappersnatcher7be) and 2035-05-16 (roachrobberdb6) according to the transponder pings. Hence, we can conclude that the SouthSeafood is caught for illegal fishing, and had ceased operating its fishing vessels since 2035-05-16."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#mini-challenge-2-creating-signatures-for-geo-temporal-patterns",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#mini-challenge-2-creating-signatures-for-geo-temporal-patterns",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Mini-challenge 2 focuses on analyzing ship movements and shipping records to understand illegal fishing practices. FishEye analysts need help creating visualizations to show patterns of ship movements and identify suspicious behaviors. They also want to understand how the commercial fishing community changed after a company was caught fishing illegally.\nThe details of the mini challenge can be found here."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#tasks-and-questions",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#tasks-and-questions",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "FishEye analysts need your help to perform geographic and temporal analysis of the CatchNet data so they can prevent illegal fishing from happening again. Your task is to develop new visual analytics tools and workflows that can be used to discover and understand signatures of different types of behavior. Can you use your tool to visualize a signature of SouthSeafood Express Corp’s illegal behavior? FishEye needs your help to develop a workflow to find other instances of illegal behavior.\n\nFishEye analysts have long wanted to better understand the flow of commercially caught fish through Oceanus’s many ports. But as they were loading data into CatchNet, they discovered they had purchased the wrong port records. They wanted to get the ship off-load records, but they instead got the port-exit records (essentially trucks/trains leaving the port area). Port exit records do not include which vessel that delivered the products. Given this limitation, develop a visualization system to associate vessels with their probable cargos. Which vessels deliver which products and when? What are the seasonal trends and anomalies in the port exit records?\nDevelop visualizations that illustrate the inappropriate behavior of SouthSeafood Express Corp vessels. How do their movement and catch contents compare to other fishing vessels? When and where did SouthSeafood Express Corp vessels perform their illegal fishing? How many different types of suspicious behaviors are observed? Use visual evidence to justify your conclusions.\nTo support further Fisheye investigations, develop visual analytics workflows that allow you to discover other vessels engaging in behaviors similar to SouthSeafood Express Corp’s illegal activities? Provide visual evidence of the similarities.\nHow did fishing activity change after SouthSeafood Express Corp was caught? What new behaviors in the Oceanus commercial fishing community are most suspicious and why?"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#loading-r-packages",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#loading-r-packages",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "pacman::p_load(tidyverse, jsonlite, DT, lubridate,\n               igraph, tidygraph, ggraph, \n               visNetwork, sf)"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#loading-the-data",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#loading-the-data",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Loading the .json data using jsonlite package.\n\nmc2_data &lt;- fromJSON(\"data/MC2/mc2.json\")\n\nmc2 is a directed multigraph, consists of nodes dataframe and links dataframe.\n\noceanus_map &lt;- read_sf(\"data/MC2/Oceanus Information/Oceanus Geography.geojson\")\n\nLoading the oceanus map:\n\nggplot(oceanus_map) +\n  geom_sf(color = \"black\",\n          ) +\n  theme_void() +\n  geom_sf_text(aes(label = Name), size = 2,\n               vjust = 1.5)"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#extracting-the-tibbles-for-nodes-and-links",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#extracting-the-tibbles-for-nodes-and-links",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "mc2_nodes_raw &lt;- as_tibble(mc2_data$nodes)\n\n\nmc2_edges_raw &lt;- as_tibble(mc2_data$links)"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#checking-for-missing-values-in-data",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#checking-for-missing-values-in-data",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "colSums(is.na(mc2_nodes_raw))\n\n                type      _last_edited_by          _date_added \n                   0                    0                    0 \n   _last_edited_date          _raw_source           _algorithm \n                   0                    0                    0 \n                name                   id                 Name \n                5627                    0                 5317 \n         Description           Activities                 kind \n                5623                    0                 5613 \n            qty_tons                 date         flag_country \n                 330                  330                 5341 \n             company              tonnage       length_overall \n                5458                 5359                 5354 \n               style fish_species_present \n                5635                    0 \n\n\n\ncolSums(is.na(mc2_edges_raw))\n\n             type              time             dwell   _last_edited_by \n                0             13101             13101                 0 \n      _date_added _last_edited_date       _raw_source        _algorithm \n                0                 0                 0                 0 \n           source            target               key              date \n                0                 0                 0            258542 \n      data_author          aphorism  holiday_greeting            wisdom \n           269156            269669            270639            269719 \nsaying of the sea \n           269750"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#parsing-the-time-with-lubridate",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#parsing-the-time-with-lubridate",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "As the _date_added and _last_edited_date contains a mixture of format, we first extract the date in “yyyy-mm-dd” format using substr.\n\nmc2_nodes_raw &lt;- mc2_nodes_raw |&gt; \n  mutate(`_date_added` = substr(`_date_added`,1,10)) |&gt; \n  mutate(`_date_added` = ymd(`_date_added`)) |&gt; \n  mutate(`_last_edited_date` = substr(`_last_edited_date`,1,10)) |&gt; \n  mutate(`_last_edited_date` = ymd(`_last_edited_date`)) |&gt; \n  mutate(date = ymd(date))\n\n\nmc2_edges_raw &lt;- mc2_edges_raw %&gt;% \n  mutate(`_date_added` = substr(`_date_added`,1,10)) %&gt;% \n  mutate(`_date_added` = ymd(`_date_added`)) %&gt;% \n  mutate(`_last_edited_date`= substr(`_last_edited_date`,1,10)) %&gt;% \n  mutate(`_last_edited_date` = ymd(`_last_edited_date`)) %&gt;% \n  mutate(time = ymd_hms(time))\n\nUnderstanding the Data\n\nunique_nodes_type &lt;- mc2_nodes_raw |&gt;  distinct(type)\nunique_nodes_type\n\n# A tibble: 12 × 1\n   type                          \n   &lt;chr&gt;                         \n 1 Entity.Commodity.Fish         \n 2 Entity.Location.City          \n 3 Entity.Document.DeliveryReport\n 4 Entity.Vessel.FishingVessel   \n 5 Entity.Vessel.Other           \n 6 Entity.Vessel.Ferry.Passenger \n 7 Entity.Vessel.CargoVessel     \n 8 Entity.Vessel.Ferry.Cargo     \n 9 Entity.Vessel.Research        \n10 Entity.Vessel.Tour            \n11 Entity.Location.Point         \n12 Entity.Location.Region        \n\n\n\nunique_edges_type &lt;- mc2_edges_raw |&gt;  distinct(type)\nunique_edges_type\n\n# A tibble: 3 × 1\n  type                                \n  &lt;chr&gt;                               \n1 Event.TransportEvent.TransponderPing\n2 Event.Transaction                   \n3 Event.HarborReport"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#extracting-the-required-columns-for-each-graph",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#extracting-the-required-columns-for-each-graph",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "In this section, we will extract the required column for the following graphs:\n\nVessel Movements\nHarbor Reports\nHarbor Import Records\n\n\n\n\nvessel_mvmt_vessel_nodes &lt;- mc2_nodes_raw |&gt; \n  filter(str_detect(type,c(\"Vessel\"))) \n\n\nvessel_mvmt_vessel_nodes\n\n# A tibble: 296 × 20\n   type        `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt;       &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 Entity.Ves… Olokun Daramola   2034-05-18    2034-09-20          Oceanus Vess…\n 2 Entity.Ves… Harvey Janus      2034-01-03    2034-05-19          Oceanus Vess…\n 3 Entity.Ves… juniorshurdlu794  2034-07-30    2034-08-16          Oceanus Vess…\n 4 Entity.Ves… Harvey Janus      2034-12-22    2035-03-29          Oceanus Vess…\n 5 Entity.Ves… Harvey Janus      2034-03-11    2034-07-23          Oceanus Vess…\n 6 Entity.Ves… Junior Shurdlu    2034-08-15    2034-11-01          Oceanus Vess…\n 7 Entity.Ves… Junior Shurdlu    2033-10-14    2034-02-28          Oceanus Vess…\n 8 Entity.Ves… Harvey Janus      2033-06-14    2033-09-10          Oceanus Vess…\n 9 Entity.Ves… Harvey Janus      2033-11-15    2034-01-16          Oceanus Vess…\n10 Entity.Ves… juniorshurdlu794  2034-04-17    2034-05-19          Oceanus Vess…\n# ℹ 286 more rows\n# ℹ 15 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, id &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;\n\n\n\nvessel_mvmt_location_nodes &lt;- mc2_nodes_raw |&gt; \n  filter(str_detect(type,c(\"Location\"))) \nvessel_mvmt_location_nodes \n\n# A tibble: 24 × 20\n   type        `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt;       &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 Entity.Loc… Greta Grass-Hill  2034-12-14    2035-11-05          Oceanus: Geo…\n 2 Entity.Loc… Greta Grass-Hill  2034-12-28    2035-03-08          Oceanus: Geo…\n 3 Entity.Loc… Greta Grass-Hill  2034-10-02    2034-11-14          Oceanus: Geo…\n 4 Entity.Loc… Kristin Baker     2034-11-13    2035-09-12          Oceanus: Geo…\n 5 Entity.Loc… Kristin Baker     2034-12-16    2035-09-11          Oceanus: Geo…\n 6 Entity.Loc… Greta Grass-Hill  2034-11-13    2035-10-19          Oceanus: Geo…\n 7 Entity.Loc… Kristin Baker     2034-10-13    2035-04-14          Oceanus: Geo…\n 8 Entity.Loc… Urashima Tarō     2034-12-20    2035-05-23          Oceanus: Geo…\n 9 Entity.Loc… Kristin Baker     2034-12-21    2035-07-20          Oceanus: Geo…\n10 Entity.Loc… Greta Grass-Hill  2034-12-13    2035-11-04          Oceanus: Geo…\n# ℹ 14 more rows\n# ℹ 15 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, id &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;\n\n\n\nvessel_mvmt_links &lt;- mc2_links_raw |&gt; \n  filter(type == \"Event.TransportEvent.TransponderPing\") \n\n\nvessel_mvmt_links\n\n# A tibble: 258,542 × 17\n   type               time                 dwell `_last_edited_by` `_date_added`\n   &lt;chr&gt;              &lt;dttm&gt;               &lt;dbl&gt; &lt;chr&gt;             &lt;date&gt;       \n 1 Event.TransportEv… 2035-09-16 04:06:48 1.15e5 Olokun Daramola   2035-09-16   \n 2 Event.TransportEv… 2035-09-20 05:21:33 4.13e5 Melinda Manning   2035-09-22   \n 3 Event.TransportEv… 2035-09-28 04:31:47 2.86e5 Olokun Daramola   2035-09-29   \n 4 Event.TransportEv… 2035-10-04 04:59:36 3.28e5 Jack Inch         2035-10-06   \n 5 Event.TransportEv… 2035-10-15 04:26:14 2.43e5 Jack Inch         2035-10-16   \n 6 Event.TransportEv… 2035-10-21 05:38:48 1.10e5 Melinda Manning   2035-10-22   \n 7 Event.TransportEv… 2035-11-01 04:04:53 1.15e5 Jack Inch         2035-11-02   \n 8 Event.TransportEv… 2035-11-05 04:56:02 2.41e5 Harvey Janus      2035-11-07   \n 9 Event.TransportEv… 2035-11-11 05:04:19 1.12e5 Harvey Janus      2035-11-12   \n10 Event.TransportEv… 2035-06-05 03:46:22 2.02e5 Junior Shurdlu    2035-06-07   \n# ℹ 258,532 more rows\n# ℹ 12 more variables: `_last_edited_date` &lt;date&gt;, `_raw_source` &lt;chr&gt;,\n#   `_algorithm` &lt;chr&gt;, source &lt;chr&gt;, target &lt;chr&gt;, key &lt;int&gt;, date &lt;chr&gt;,\n#   data_author &lt;chr&gt;, aphorism &lt;chr&gt;, holiday_greeting &lt;chr&gt;, wisdom &lt;chr&gt;,\n#   `saying of the sea` &lt;chr&gt;\n\n\n\n\n\n\nharbor_report_nodes &lt;- mc2_nodes_raw |&gt; \n  filter(str_detect(type,c(\"Vessel|Location\")))\nharbor_report_nodes\n\n# A tibble: 320 × 20\n   type        `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt;       &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 Entity.Loc… Greta Grass-Hill  2034-12-14    2035-11-05          Oceanus: Geo…\n 2 Entity.Loc… Greta Grass-Hill  2034-12-28    2035-03-08          Oceanus: Geo…\n 3 Entity.Loc… Greta Grass-Hill  2034-10-02    2034-11-14          Oceanus: Geo…\n 4 Entity.Loc… Kristin Baker     2034-11-13    2035-09-12          Oceanus: Geo…\n 5 Entity.Loc… Kristin Baker     2034-12-16    2035-09-11          Oceanus: Geo…\n 6 Entity.Ves… Olokun Daramola   2034-05-18    2034-09-20          Oceanus Vess…\n 7 Entity.Ves… Harvey Janus      2034-01-03    2034-05-19          Oceanus Vess…\n 8 Entity.Ves… juniorshurdlu794  2034-07-30    2034-08-16          Oceanus Vess…\n 9 Entity.Ves… Harvey Janus      2034-12-22    2035-03-29          Oceanus Vess…\n10 Entity.Ves… Harvey Janus      2034-03-11    2034-07-23          Oceanus Vess…\n# ℹ 310 more rows\n# ℹ 15 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, id &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;\n\n\n\nharbor_report_links &lt;- mc2_links_raw |&gt; \n  filter(type == \"Event.HarborReport\")\nharbor_report_links\n\n# A tibble: 2,487 × 17\n   type               time   dwell `_last_edited_by` `_date_added`\n   &lt;chr&gt;              &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt;             &lt;date&gt;       \n 1 Event.HarborReport NA        NA Junior Shurdlu    2035-09-21   \n 2 Event.HarborReport NA        NA Haenyeo Hyun-Ki   2035-08-20   \n 3 Event.HarborReport NA        NA Junior Shurdlu    2035-09-18   \n 4 Event.HarborReport NA        NA Haenyeo Hyun-Ki   2035-08-16   \n 5 Event.HarborReport NA        NA Haenyeo Hyun-Ki   2035-09-26   \n 6 Event.HarborReport NA        NA Junior Shurdlu    2035-04-11   \n 7 Event.HarborReport NA        NA Haenyeo Hyun-Ki   2035-04-14   \n 8 Event.HarborReport NA        NA Haenyeo Hyun-Ki   2035-04-16   \n 9 Event.HarborReport NA        NA Junior Shurdlu    2035-04-12   \n10 Event.HarborReport NA        NA Clepper Jessen    2035-04-11   \n# ℹ 2,477 more rows\n# ℹ 12 more variables: `_last_edited_date` &lt;date&gt;, `_raw_source` &lt;chr&gt;,\n#   `_algorithm` &lt;chr&gt;, source &lt;chr&gt;, target &lt;chr&gt;, key &lt;int&gt;, date &lt;chr&gt;,\n#   data_author &lt;chr&gt;, aphorism &lt;chr&gt;, holiday_greeting &lt;chr&gt;, wisdom &lt;chr&gt;,\n#   `saying of the sea` &lt;chr&gt;\n\n\n\n\n\n\nimport_location_nodes &lt;- mc2_nodes_raw |&gt; \n  filter(str_detect(type,c(\"Location\")))\nimport_location_nodes \n\n# A tibble: 24 × 20\n   type        `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt;       &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 Entity.Loc… Greta Grass-Hill  2034-12-14    2035-11-05          Oceanus: Geo…\n 2 Entity.Loc… Greta Grass-Hill  2034-12-28    2035-03-08          Oceanus: Geo…\n 3 Entity.Loc… Greta Grass-Hill  2034-10-02    2034-11-14          Oceanus: Geo…\n 4 Entity.Loc… Kristin Baker     2034-11-13    2035-09-12          Oceanus: Geo…\n 5 Entity.Loc… Kristin Baker     2034-12-16    2035-09-11          Oceanus: Geo…\n 6 Entity.Loc… Greta Grass-Hill  2034-11-13    2035-10-19          Oceanus: Geo…\n 7 Entity.Loc… Kristin Baker     2034-10-13    2035-04-14          Oceanus: Geo…\n 8 Entity.Loc… Urashima Tarō     2034-12-20    2035-05-23          Oceanus: Geo…\n 9 Entity.Loc… Kristin Baker     2034-12-21    2035-07-20          Oceanus: Geo…\n10 Entity.Loc… Greta Grass-Hill  2034-12-13    2035-11-04          Oceanus: Geo…\n# ℹ 14 more rows\n# ℹ 15 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, id &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;\n\n\n\nimport_commodity_nodes &lt;- mc2_nodes_raw |&gt; \n  filter(str_detect(type,c(\"Commodity\")))\nimport_commodity_nodes \n\n# A tibble: 10 × 20\n   type        `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt;       &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 Entity.Com… Clepper Jessen    2033-09-04    2035-01-25          \"\"           \n 2 Entity.Com… Clepper Jessen    2034-01-21    2035-01-04          \"\"           \n 3 Entity.Com… Haenyeo Hyun-Ki   2033-06-22    2035-01-14          \"\"           \n 4 Entity.Com… Haenyeo Hyun-Ki   2033-11-24    2035-01-14          \"\"           \n 5 Entity.Com… Haenyeo Hyun-Ki   2033-09-13    2034-11-30          \"\"           \n 6 Entity.Com… Clepper Jessen    2033-08-20    2035-01-15          \"\"           \n 7 Entity.Com… Haenyeo Hyun-Ki   2033-09-19    2034-12-02          \"\"           \n 8 Entity.Com… Clepper Jessen    2033-08-30    2034-12-28          \"\"           \n 9 Entity.Com… Haenyeo Hyun-Ki   2033-12-15    2034-12-01          \"\"           \n10 Entity.Com… Haenyeo Hyun-Ki   2033-08-20    2034-12-22          \"\"           \n# ℹ 15 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, id &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;\n\n\n\nimport_delivery_nodes &lt;- mc2_nodes_raw |&gt; \n  filter(str_detect(type,c(\"Document\")))\nimport_delivery_nodes \n\n# A tibble: 5,307 × 20\n   type        `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt;       &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 Entity.Doc… Junior Shurdlu    2035-11-04    2035-11-06          Tuna Shelf/e…\n 2 Entity.Doc… Harvey Janus      2035-08-17    2035-08-19          Tuna Shelf/e…\n 3 Entity.Doc… Junior Shurdlu    2035-08-21    2035-08-23          Tuna Shelf/e…\n 4 Entity.Doc… Melinda Manning   2035-11-07    2035-11-09          Tuna Shelf/e…\n 5 Entity.Doc… Harvey Janus      2035-08-24    2035-08-25          Tuna Shelf/e…\n 6 Entity.Doc… Harvey Janus      2035-08-28    2035-08-28          Tuna Shelf/e…\n 7 Entity.Doc… Jack Inch         2035-09-01    2035-09-01          Tuna Shelf/e…\n 8 Entity.Doc… Junior Shurdlu    2035-09-05    2035-09-06          Tuna Shelf/e…\n 9 Entity.Doc… Harvey Janus      2035-09-07    2035-09-07          Tuna Shelf/e…\n10 Entity.Doc… Jack Inch         2035-09-12    2035-09-13          Tuna Shelf/e…\n# ℹ 5,297 more rows\n# ℹ 15 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, id &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;\n\n\n\nimport_links &lt;- mc2_links_raw |&gt; \n  filter(type == \"Event.Transaction\")\nimport_links\n\n# A tibble: 10,614 × 17\n   type              time   dwell `_last_edited_by` `_date_added`\n   &lt;chr&gt;             &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt;             &lt;date&gt;       \n 1 Event.Transaction NA        NA Junior Shurdlu    2035-11-04   \n 2 Event.Transaction NA        NA Junior Shurdlu    2035-11-04   \n 3 Event.Transaction NA        NA Melinda Manning   2035-08-17   \n 4 Event.Transaction NA        NA Melinda Manning   2035-08-17   \n 5 Event.Transaction NA        NA Olokun Daramola   2035-08-21   \n 6 Event.Transaction NA        NA Olokun Daramola   2035-08-21   \n 7 Event.Transaction NA        NA Junior Shurdlu    2035-11-07   \n 8 Event.Transaction NA        NA Junior Shurdlu    2035-11-07   \n 9 Event.Transaction NA        NA Jack Inch         2035-08-24   \n10 Event.Transaction NA        NA Jack Inch         2035-08-24   \n# ℹ 10,604 more rows\n# ℹ 12 more variables: `_last_edited_date` &lt;date&gt;, `_raw_source` &lt;chr&gt;,\n#   `_algorithm` &lt;chr&gt;, source &lt;chr&gt;, target &lt;chr&gt;, key &lt;int&gt;, date &lt;chr&gt;,\n#   data_author &lt;chr&gt;, aphorism &lt;chr&gt;, holiday_greeting &lt;chr&gt;, wisdom &lt;chr&gt;,\n#   `saying of the sea` &lt;chr&gt;"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#flow-of-fishing-vessels",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#flow-of-fishing-vessels",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "FishEye analysts have long wanted to better understand the flow of commercially caught fish through Oceanus’s many ports.\n\nfishing_vessels_nodes &lt;- mc2_nodes_raw %&gt;% \n  filter(type == \"Entity.Vessel.FishingVessel\")\n\nThere is a total of 178 Fishing vessels in Oceanus."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#fishing-ground-locations",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#fishing-ground-locations",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "fishing_ground &lt;- mc2_nodes_raw %&gt;% \n  filter(kind == \"Fishing Ground\") \n\n\nnon_fishing_ground &lt;- mc2_nodes_raw %&gt;%  \n  filter(kind == \"Ecological Preserve\") \n\nThere are three fishing grounds:\n1. Cod Table\n2. Wrasse Beds\n3. Tuna Shelf\nFishing outside these fishing grounds are considered illegal fishing.\n\ncargo_vessels &lt;- mc2_nodes_raw %&gt;%  \n  filter(type == \"Entity.Vessel.CargoVessel\") %&gt;% \n  arrange(desc(tonnage))\ncargo_vessels\n\n# A tibble: 100 × 20\n   type        `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt;       &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 Entity.Ves… Melinda Manning   2035-02-01    2035-02-17          Oceanus Vess…\n 2 Entity.Ves… Harvey Janus      2034-12-02    2034-12-22          Oceanus Vess…\n 3 Entity.Ves… Olokun Daramola   2035-02-13    2035-03-02          Oceanus Vess…\n 4 Entity.Ves… Harvey Janus      2034-12-03    2034-12-15          Oceanus Vess…\n 5 Entity.Ves… Harvey Janus      2035-02-20    2035-03-03          Oceanus Vess…\n 6 Entity.Ves… Harvey Janus      2035-01-06    2035-01-21          Oceanus Vess…\n 7 Entity.Ves… Jack Inch         2035-02-01    2035-02-20          Oceanus Vess…\n 8 Entity.Ves… Jack Inch         2034-11-08    2034-11-21          Oceanus Vess…\n 9 Entity.Ves… Jack Inch         2034-12-12    2034-12-28          Oceanus Vess…\n10 Entity.Ves… Jack Inch         2034-12-31    2035-01-12          Oceanus Vess…\n# ℹ 90 more rows\n# ℹ 15 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, id &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;\n\n\nThere are 100 cargo vessels, with tonnage ranging from 2,100 to 76,300."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#which-fishing-vessels-frequent-the-fishing-sites",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#which-fishing-vessels-frequent-the-fishing-sites",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "mc2_edges_raw |&gt; \n  filter(source %in% c(\"Cod Table\",\"Wrasse Beds\",\"Tuna Shelf\")) |&gt; \n  group_by(target, source) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(desc(n))\n\n# A tibble: 557 × 3\n# Groups:   target [287]\n   target                    source          n\n   &lt;chr&gt;                     &lt;chr&gt;       &lt;int&gt;\n 1 saltyskippera9e           Wrasse Beds  1460\n 2 rainbowtroutraider4d0     Wrasse Beds  1413\n 3 europeanperchpirated9b    Tuna Shelf   1265\n 4 pinksalmonpirate2a3       Wrasse Beds  1256\n 5 tenchtaker595             Wrasse Beds  1183\n 6 pollockpirate212          Wrasse Beds  1147\n 7 fishtracker03e            Wrasse Beds  1115\n 8 yellowfintunataker08b     Tuna Shelf   1079\n 9 halibuthero9b9            Tuna Shelf   1059\n10 tigermuskellungemaster012 Wrasse Beds  1048\n# ℹ 547 more rows"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#which-fishing-vessels-frequent-the-non-fishing-sites",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#which-fishing-vessels-frequent-the-non-fishing-sites",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "mc2_edges_raw  %&gt;%  \n  filter(source %in% c(\"Ghoti Preserve\",\"Nemo Reef\", \"Don Limpet Preserve\")) %&gt;% \n  group_by(target, source) %&gt;%  \n  summarise(n = n()) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 248 × 3\n# Groups:   target [219]\n   target                 source                  n\n   &lt;chr&gt;                  &lt;chr&gt;               &lt;int&gt;\n 1 europeanperchpirated9b Nemo Reef            1040\n 2 manatee17ea            Don Limpet Preserve   884\n 3 yellowfintunataker08b  Nemo Reef             863\n 4 anchovyassaulterb1c    Nemo Reef             843\n 5 halibuthero9b9         Nemo Reef             833\n 6 bluefishbandit8ec      Nemo Reef             782\n 7 herringharpooner843    Nemo Reef             763\n 8 pacificcodcapturer81c  Nemo Reef             719\n 9 welscatfishwrangler6ae Nemo Reef             613\n10 pompanoplunderere5d    Nemo Reef             596\n# ℹ 238 more rows"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Ex 01",
    "section": "",
    "text": "For the identification of sub-market, you can navigate straight to 2.3.3 Identifying the sub-market\nFor the final product, you can navigate straight to 3.0 Data Visualisation.\nIf not, you can proceed to enjoy reading the step by step thought process for this visualisation.\n\n\n\n\nThere are two major residential property market in Singapore, namely public and private housing. Public housing aims to meet the basic need of the general public with monthly household income less than or equal to S$14,000. For families with monthly household income more than S$14,000, they need to turn to the private residential market.\n\n\n\nAssuming the role of a graphical editor of a media company, you are requested to prepare minimum two and maximum three data visualisation to reveal the private residential market and sub-markets of Singapore for the 1st quarter of 2024.\n\n\n\n\n\n\nT he pac::p_load() function to load the required R packages in the working environment. The following packages are used in this set up:\n\nggthemes: Extra themes, geoms, and scales for ggplot2.\ntidyverse: A collection of core packages designed for data science, used extensively for data preparation and wrangling.\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\ncolorspace:\nggiraph: for making ‘ggplot’ graphics interactive.\nplotly: R library for plotting interactive statistical graphs.\npatchwork: specially designed for combining separate ggplot2 graphs into a single figure.\nlubridate: for easy and fast parsing of Date / Time\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggdist: a ggplot2 extension specially design for visualising distribution and uncertainty\n\n\n# load tidyverse from pacman\npacman::p_load(tidyverse,dplyr,ggridges,\n               ggthemes,colorspace,ggiraph,\n               plotly,patchwork,lubridate, \n               ggrepel,ggdist)\n\n\n\n\nFive sets of data are provided for this exercise:\n\n2023 Quarter 1 Residential Transaction: ResidentialTransaction20240308160536.csv\n2023 Quarter 2 Residential Transaction: ResidentialTransaction20240308160736.csv\n2023 Quarter 3 Residential Transaction: ResidentialTransaction20240308161009.csv\n2023 Quarter 4 Residential Transaction: ResidentialTransaction20240308161109.csv\n2024 Quarter 1 Residential Transaction: ResidentialTransaction20240414220633.csv\n\nAll five sets of data consists of past transactions between Q1 of 2023 to Q1 of 2024, with 21 variables that will be explored in detail after the data sets are imported.\n\n# load transaction data using readr, part of tidyverse package\ndata.23Q1 &lt;- read_csv(\"data/ResidentialTransaction20240308160536.csv\")\ndata.23Q2 &lt;- read_csv(\"data/ResidentialTransaction20240308160736.csv\")\ndata.23Q3 &lt;- read_csv(\"data/ResidentialTransaction20240308161009.csv\")\ndata.23Q4 &lt;- read_csv(\"data/ResidentialTransaction20240308161109.csv\")\ndata.24Q1 &lt;- read_csv(\"data/ResidentialTransaction20240414220633.csv\")\n\nTo add new column Quarter into each of the csv file.\n\ndata.23Q1$Quarter &lt;- rep('23Q1',nrow(data.23Q1))\ndata.23Q2$Quarter &lt;- rep('23Q2',nrow(data.23Q2))\ndata.23Q3$Quarter &lt;- rep('23Q3',nrow(data.23Q3))\ndata.23Q4$Quarter &lt;- rep('23Q4',nrow(data.23Q4))\ndata.24Q1$Quarter &lt;- rep('24Q1',nrow(data.24Q1))\n\nCombining all the data set into one\n\ndf &lt;- rbind(data.23Q1,data.23Q2,data.23Q3,data.23Q4,data.24Q1)\n\nTo have a basic understanding of all 22 variables, glimpse() is used with 2024 Q1 data. We can see that there are 21 columns, with the following data types:\n\ndoubles: Transacted Price ($), Area (SQFT), Unit Price ($ PSF), Area (SQM), Unit Price ($ PSM), Number of Units\ncharacters: the rest of the 16 variables, including Quarter\n\nInterestingly, Nett Price is classified as characters, as it contains - character, although the rest of the columns should return doubles like Transacted Price ($)\n\nThe CodeThe Output\n\n\n\n# have a basic understanding for all 22 variables\nglimpse(df)\n\n\n\n\n\nRows: 26,806\nColumns: 22\n$ `Project Name`                &lt;chr&gt; \"THE REEF AT KING'S DOCK\", \"URBAN TREASU…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2317000, 1823500, 1421112, 1258112, 1280…\n$ `Area (SQFT)`                 &lt;dbl&gt; 882.65, 882.65, 1076.40, 1033.34, 871.88…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2625, 2066, 1320, 1218, 1468, 1767, 1095…\n$ `Sale Date`                   &lt;chr&gt; \"01 Jan 2023\", \"02 Jan 2023\", \"02 Jan 20…\n$ Address                       &lt;chr&gt; \"12 HARBOURFRONT AVENUE #05-32\", \"205 JA…\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Area (SQM)`                  &lt;dbl&gt; 82.0, 82.0, 100.0, 96.0, 81.0, 308.7, 42…\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 28256, 22238, 14211, 13105, 15802, 19015…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Condominium\", \"Executive…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 12/01/2021\", \"Freehold\", \"9…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"HDB\", \"Private\", \"HDB\", \"HDB\", \"HDB\", \"…\n$ `Postal Code`                 &lt;chr&gt; \"097996\", \"419535\", \"269343\", \"269294\", …\n$ `Postal District`             &lt;chr&gt; \"04\", \"14\", \"27\", \"27\", \"28\", \"19\", \"10\"…\n$ `Postal Sector`               &lt;chr&gt; \"09\", \"41\", \"26\", \"26\", \"79\", \"54\", \"27\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"East Region\", \"North …\n$ `Planning Area`               &lt;chr&gt; \"Bukit Merah\", \"Bedok\", \"Yishun\", \"Yishu…\n$ Quarter                       &lt;chr&gt; \"23Q1\", \"23Q1\", \"23Q1\", \"23Q1\", \"23Q1\", …\n\n\n\n\n\n\n\n\nOn top of the data types that is automatically assigned by R to help the program know how to process it, we can split the data into 4 types of data category to help ourselves for future analytics application. You can refer to the FAQ, Data Dictionary and Methodology in URA website.\n\n\n\n\n\n\n\n\n\nVariable\nType\nAppropriate Scale\nRemarks\n\n\n\n\nProject Name\nQualitative / Nominal\nDiscrete\n\n\n\nTransacted Price ($)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nArea (SQFT)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nUnit Price ($ PSF)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nSales Date\nDate / Ordinal\nDiscrete\n\n\n\nAddress\nQualitative / Nominal\nDiscrete\n\n\n\nType of Sale\nQualitative / Nominal\nDiscrete\n\n\n\nType of Area\nQualitative / Nominal\nDiscrete\n\n\n\nArea (SQM)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nUnit Price ($ PSF)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nNett Price($)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nProperty Type\nQualitative/ Nominal\nDiscrete\n\n\n\nNumber of Units\nQuantitative/ Numerical Discrete\nDiscrete\n\n\n\nTenure\nNominal + Date / Ordinal\nNone + Discrete\nMixture of remaining lease date and tenure type and duration\n\n\nCompletion Date\nNominal + Date / Ordinal\nNone + Discrete\nMixture of “Uncomplete” status and completion date.\n\n\nPurchaser Address Indicator\nQualitative / Nominal\nDiscrete\n\n\n\nPostal Code\nQualitative / Nominal\nDiscrete\n\n\n\nPostal District\nQualitative / Nominal\nDiscrete\n\n\n\nPostal Sector\nQualitative / Nominal\nDiscrete\n\n\n\nPlanning Region\nQualitative / Nominal\nDiscrete\n\n\n\nPlanning Area\nQualitative / Nominal\nDiscrete\n\n\n\nQuarter\nDate / Ordinal\nDiscrete\nCreated for ease of data analysis\n\n\n\nNotes\n\nContextually, some of the variables could be classified under ordinal. For example, I can choose to fill in the order for Purchaser Address Indicator as Private category is higher order than HDB category. The context is purchaser that address is Private is likely to be a person with higher Social Economic Status (SES) compared to a purchaser with HDB as address. Same could be applied to Postal Code, Postal District, Postal Sector, Planning Region and Planning Area.\nSpecial Case: Tenure is still a character data type as of now. If we choose to use this column for analysis, we do need to do data preparation on this. Tenure date could also be classified under continuous data for more granularity. This variable will be a mixture of nominal data type with categories like “Freehold”, and also quantitative data type using its Tenure date.\nNett Price($) is still a character data type as of now. If we choose to use this column for analysis, we do need to do data preparation on this.\nSpecial Case: Completion Date is classified as character, but a quick look at the data df, we will realise that this variable is a mix of quantitative and qualitative data type of Uncomplete and actual date of completion, which could be converted and analyse as doubles.\n\nFor simplicity of data analysis\n1. Transacted Price ($) and Nett Price($) are similar. In this analysis, we will use Transacted Price ($) for simplicity, as Nett Price($) has fair amount of missing data, and it is prices after deducting the value of the indirect discounts or benefits, and not representative for all transactions.\n\nArea (SQFT) and Area (SQM), and Unit Price ($ PSF) and Unit Price ($ PSM) are similar measures in different unit. We will use Area (SQFT) and Unit Price ($ PSF)\n\nNext we look at the basic statistics of the current data using summary()\n\nThe OutputThe Code\n\n\n\n\n Project Name       Transacted Price ($)  Area (SQFT)       Unit Price ($ PSF)\n Length:26806       Min.   :   440000    Min.   :   322.9   Min.   : 138      \n Class :character   1st Qu.:  1280000    1st Qu.:   721.2   1st Qu.:1384      \n Mode  :character   Median :  1660000    Median :   990.3   Median :1762      \n                    Mean   :  2143286    Mean   :  1191.6   Mean   :1852      \n                    3rd Qu.:  2320000    3rd Qu.:  1302.4   3rd Qu.:2260      \n                    Max.   :392180000    Max.   :144883.4   Max.   :5756      \n                                                                              \n  Sale Date           Address          Type of Sale       Type of Area      \n Length:26806       Length:26806       Length:26806       Length:26806      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Area (SQM)      Unit Price ($ PSM) Nett Price($)      Property Type     \n Min.   :   30.0   Min.   : 1484      Length:26806       Length:26806      \n 1st Qu.:   67.0   1st Qu.:14893      Class :character   Class :character  \n Median :   92.0   Median :18966      Mode  :character   Mode  :character  \n Mean   :  110.5   Mean   :19930                                           \n 3rd Qu.:  121.0   3rd Qu.:24327                                           \n Max.   :13460.0   Max.   :61962                                           \n NA's   :6                                                                 \n Number of Units     Tenure          Completion Date   \n Min.   : 1.000   Length:26806       Length:26806      \n 1st Qu.: 1.000   Class :character   Class :character  \n Median : 1.000   Mode  :character   Mode  :character  \n Mean   : 1.005                                        \n 3rd Qu.: 1.000                                        \n Max.   :60.000                                        \n                                                       \n Purchaser Address Indicator Postal Code        Postal District   \n Length:26806                Length:26806       Length:26806      \n Class :character            Class :character   Class :character  \n Mode  :character            Mode  :character   Mode  :character  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n Postal Sector      Planning Region    Planning Area        Quarter         \n Length:26806       Length:26806       Length:26806       Length:26806      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n\n\n\n\n\n# data summary\nsummary(df)\n\n\n\n\nFrom the continuous data above, we can conclude that there is a possibility of extreme outliers. The Number of Units has a max of 60, while the 3rd Quartile is 1.000. Let’s examine the data by sorting it in descending order using arrange(desc()) and take a look at data with Number of Units, as it greatly affect the analysis of Transaction Price ($).\n\ndf %&gt;%\n  filter(`Number of Units` &gt; 1) %&gt;%\n  arrange(desc(`Number of Units`)) \n\n# A tibble: 12 × 22\n   `Project Name`      `Transacted Price ($)` `Area (SQFT)` `Unit Price ($ PSF)`\n   &lt;chr&gt;                                &lt;dbl&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n 1 MEYER PARK                       392180000       144883.                 2707\n 2 BAGNALL COURT                    115280000        68491.                 1683\n 3 KEW LODGE                         66800000        25177                  2653\n 4 KARTAR APARTMENTS                 18000000         6964.                 2585\n 5 MONDO MANSION BUIL…                6280000         5490.                 1144\n 6 N.A.                              10600000         6747.                 1571\n 7 N.A.                              61080008        32149.                 1900\n 8 N.A.                              32200000        14123.                 2280\n 9 N.A.                               6150000         4342.                 1416\n10 EAST VIEW GARDEN                   6100000         8338.                  732\n11 N.A.                               8000000         3659.                 2187\n12 CLAYMORE PLAZA                     7000000         4209.                 1663\n# ℹ 18 more variables: `Sale Date` &lt;chr&gt;, Address &lt;chr&gt;, `Type of Sale` &lt;chr&gt;,\n#   `Type of Area` &lt;chr&gt;, `Area (SQM)` &lt;dbl&gt;, `Unit Price ($ PSM)` &lt;dbl&gt;,\n#   `Nett Price($)` &lt;chr&gt;, `Property Type` &lt;chr&gt;, `Number of Units` &lt;dbl&gt;,\n#   Tenure &lt;chr&gt;, `Completion Date` &lt;chr&gt;, `Purchaser Address Indicator` &lt;chr&gt;,\n#   `Postal Code` &lt;chr&gt;, `Postal District` &lt;chr&gt;, `Postal Sector` &lt;chr&gt;,\n#   `Planning Region` &lt;chr&gt;, `Planning Area` &lt;chr&gt;, Quarter &lt;chr&gt;\n\n\nWe have 12 rows that have transactions containing more than 1 unit sold, and a check on the Address shows that there are 5 sales that are en bloc cases which should be remove from the analysis. These en bloc cases are likely sales to companies for purpose of redevelopment rather than individual purchasers. See this link on the Meyer Park en bloc transaction. Otherwise, those 7 cases of transactions with more than one unit sold should be included as are likely the cases of purchase of multiple units or the purchase of multiple adjacent plots of land.\n\n\n\nFor the follow code chunk, we aim to remove all the enbloc sales, including the 5 transactions that we have found in section 2.3. While we have no idea if there are more than 5 cases of en bloc sales, the follow code chunk will effectively remove all the rows with the string “ENBLOC” in Address column.\n\ndf.c &lt;- df %&gt;%\n  filter(!grepl('ENBLOC', df$Address))\ndf.c\n\n# A tibble: 26,801 × 22\n   `Project Name`      `Transacted Price ($)` `Area (SQFT)` `Unit Price ($ PSF)`\n   &lt;chr&gt;                                &lt;dbl&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n 1 THE REEF AT KING'S…                2317000          883.                 2625\n 2 URBAN TREASURES                    1823500          883.                 2066\n 3 NORTH GAIA                         1421112         1076.                 1320\n 4 NORTH GAIA                         1258112         1033.                 1218\n 5 PARC BOTANNIA                      1280000          872.                 1468\n 6 NANYANG PARK                       5870000         3323.                 1767\n 7 PALMS @ SIXTH AVEN…                4950000         4521.                 1095\n 8 N.A.                               3260000         1555.                 2096\n 9 WHISTLER GRAND                      850000          441.                 1926\n10 NORTHOAKS                          1268000         1604.                  791\n# ℹ 26,791 more rows\n# ℹ 18 more variables: `Sale Date` &lt;chr&gt;, Address &lt;chr&gt;, `Type of Sale` &lt;chr&gt;,\n#   `Type of Area` &lt;chr&gt;, `Area (SQM)` &lt;dbl&gt;, `Unit Price ($ PSM)` &lt;dbl&gt;,\n#   `Nett Price($)` &lt;chr&gt;, `Property Type` &lt;chr&gt;, `Number of Units` &lt;dbl&gt;,\n#   Tenure &lt;chr&gt;, `Completion Date` &lt;chr&gt;, `Purchaser Address Indicator` &lt;chr&gt;,\n#   `Postal Code` &lt;chr&gt;, `Postal District` &lt;chr&gt;, `Postal Sector` &lt;chr&gt;,\n#   `Planning Region` &lt;chr&gt;, `Planning Area` &lt;chr&gt;, Quarter &lt;chr&gt;\n\n\nIn the df.c tibble, it showed that 5 rows had been removed. Now there is only 26,801 rows instead of the original 26,806 rows in the initial df. This confirm that there are only 5 en bloc sales in the data frame.\n\n\n\nAfter removing the 5 en bloc sales, we are still left with 7 transactions that has between 2 to 4 units sold per transaction. This has caused the Transacted Price ($) to be greatly inflated. In dealing with this, we can (1) choose to divide the Transacted Price ($) by the number of units sold, or (2) remove them from the analysis. In this analysis, we will be using method (2), to remove these transactions from the data set for further analysis. The reason to do so is that we cannot determine at this stage that each unit in the sales are equally priced, or whether each unit has the same Area size.\n\ndf.c &lt;- df.c %&gt;%\n  filter(`Number of Units` &lt;= 1)\n\n\n\n\nLand vs. Strata\nTo lend some background context, you can refer to this link: Landed vs. Strata Landed Basically, Strata means you only owns the property build on this land, but do not own the land itself. Land means you own the land and have more freedom and responsibility in terms of the constructions etc. The application of this knowledge for analysis is that Strata and Land classification can be identify as the sub-markets as owning a piece of land is likely to be more expensive for the purchaser compared to Strata title.\nThe plot below shows the 10 sub-markets in Singapore private housing market.\nLand\n\nApartment\nDetached House\nSemi-Detached House\nTerrace House\n\nStrata\n\nApartment\nCondominium\nDetached House\nExecutive Condominium\nSemi-Detached House\nTerrace House\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.c %&gt;%\n  select(`Transacted Price ($)`, `Type of Area`, `Property Type`) %&gt;%\n  group_by(`Property Type`, `Type of Area`) %&gt;%\n  summarise(`Median Transaction Price ($)` = median(`Transacted Price ($)`, .groups = 'drop')) %&gt;%\n  ggplot(aes(x = `Property Type`, \n             y = `Median Transaction Price ($)`, \n             color = `Type of Area`)) +  # Adding color aesthetic based on Type of Area\n  geom_point() +\n  theme_economist() +\n  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust=1)) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(color = \"Type of Area\")\n\n\n\n\nConclusion\nFrom the Median Transacted Price plot above, we observe that the 10 sub-markets are distinct in pricing, with Land Type of Area generally priced a lot higher than Strata, especially in the Apartment and Detached House categories.\n\n\n\n\n\n\nIn this section, we would want to focus on the Apartment(Strata) Condominium and Executive Condominium sub-market in Singapore, out of the 10 sub-markets identified in section 2.3.3. As stated in section 1.1 Setting the scene, for families with monthly household income more than S$14,000, they need to turn to the private residential market. However, due limited number of plots to be shown, we will take Condominium as example for the illustrations\nAssumptions and considerations for the selecting the sub-markets to visualise:\n\nAffordability - According to the this article, apartment(Strata), Executive Condominium and Condominium are the next most affordable type of private housing if the family is not eligible to purchase public housing. This corroborate with the Median Transaction Price plot above.\nTarget Audience - The target audience for this graphics will be the public who would like to consider housing option in the private residential market. The target audience are those who’s family household are above the limits and would like to look for relatively affordable housing option in the private residential market as mentioned above in point 1.\nInformation Required - Pricing, Location, Types of Sales, Unit Price, Size of Property\n\n\n\n\n\n\n\nPlot 1: Pricing for non-freehold Condominium\nThis first plot focus on Condominium that are not of Freehold Tenure. Compared to freehold property, these non-freehold Condominiums are relatively lower price. The plot below shows the distribution of Transacted Price ($) in the five Planning Region in Singapore, and reveal the median price of each region varies from 1 to 2 million, with North Region with a tighter spread at a lower price than the other four regions. The Central Region has the highest median at 2 million, with a widest spread. The 4th quantile of the Central Region consists of Transacted Price ($) between 2.5 to just after 9 million. This shows that the Condominiums with Lease are more affordable in the North and East Region in Singapore, with North East and West Region next, and finally the most expensive Condominium would be the ones found in Central Region.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.c %&gt;%\n  filter(`Type of Area` == 'Strata',`Property Type` == 'Condominium', `Quarter` == '24Q1', `Tenure`!= 'Freehold') %&gt;%\n  ggplot(aes(x = `Transacted Price ($)`,\n           y = `Planning Region`,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE,\n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\",\n                       option = \"A\") +\n  scale_x_continuous(name = \"Transacted Price in million ($)\",\n    labels = c(0,1,2,3,4,5,6,7,8,9,10), \n    breaks = c(0,1000000,2000000,3000000,4000000,5000000,6000000,7000000,8000000,9000000,10000000)) +\n  theme_economist() + \n  ggtitle(\"24Q1 Non-Freehold Condo Transacted Price by Planning Region\") +\n  theme(plot.title = element_text(size=12))\n\n\n\n\n\n\n\nPlot 2: Mean and Median Unit Price for non-freehold Condominium\nPlot 2 focus on the mean and median Unit Price ($ PSF) for the non-freehold Condominium in the Central Region. The Mean Unit Price ($ PSF) is represented by the red dot in each of the violin plot and boxplot. While we see a slight rise in the mean and median Unit Price ($ PSF) in 23Q2, where there are more transactions with Unit Price ($ PSF) in the $2,500 range, represented by the bulge at the top of the violin plot. For the rest of the quarters, the mean and median Unit Price ($ PSF) hovers around the range of $2,000. Despite the mean and median are at $2,000 for the 24Q1, there is also a sizable transaction of Unit Price ($ PSF) of $3,000 happening in 24Q1, as represented by the slight bulge at the top of the violin plot.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nquarter_means &lt;- df.c %&gt;%\n  filter(`Property Type` == 'Condominium',`Planning Region` == \"Central Region\", `Tenure`!= 'Freehold') %&gt;%\n  group_by(Quarter) %&gt;%\n  summarise(mean_uprice = mean(`Unit Price ($ PSF)`))\n\ndf.c %&gt;%\n  filter(`Property Type` == 'Condominium', `Planning Region` == \"Central Region\",`Tenure`!= 'Freehold') %&gt;%\n  ggplot(aes(y=`Unit Price ($ PSF)`,\n           x=`Quarter`)\n       )+\n  geom_violin() +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  geom_point(data = quarter_means, \n             aes(y = mean_uprice), \n             color = \"red\", \n             size = 3) +\n    scale_y_continuous(labels = scales::comma) +\n  theme_economist() +\n  ggtitle(\"Condominium Unit Price ($ PSF) for Central Region\") \n\n\n\n\n\n\n\nPlot 3: Purchaser Address Indicator for non-freehold Condominium\nPlot 3 focus on the top and bottom 5% of the different types of purchasers for non-freehold Condominium in the Central Region. We assume that the N.A type are purchasers who do not own any property in Singapore. The top 5% of the Private category purchase units priced at $4.5-$9 million. For HDB and N.A categories, the top 5% are purchasing units that is priced between $3 to 4 million. Both bottom 5% Private and HDB categories purchase unit of $1 million, while the bottom 5% N.A category are buying unit just above $1 million. The plausible reason is that purchaser in N.A. category may want to buy a unit for their own stay since they do not own any other properties. Hence, they are paying slightly more for a larger unit relative to other two types of purchasers, who might be buying studio units for rental income.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.c %&gt;%\n  filter(`Property Type` == 'Condominium', `Quarter` == '24Q1', `Planning Region` == \"Central Region\", `Tenure` != 'Freehold') %&gt;%\n  ggplot(aes(x = `Transacted Price ($)`, \n           y = `Purchaser Address Indicator`, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.05, 0.95)\n    ) +\n  scale_fill_manual(\n    name = \"Percentage of Total\",\n    values = c(\"#FF00FFA0\", \"#FFE4E1A0\", \"#0000ffA0\"),\n    labels = c(\"Bottom 5%\", \"5-95%\", \"Top 5%\")\n  ) +\n  theme_ridges()+\n  theme_economist() +\n  theme(plot.title = element_text(size=12)) +\n  scale_x_continuous(name = \"Transacted Price in million ($)\",\n    labels = c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15), \n    breaks = c(0,1000000,2000000,3000000,4000000,5000000,6000000,7000000,8000000,9000000,10000000,11000000, 12000000, 13000000, 14000000, 15000000)) + \n  ggtitle(\"Central Region Condominium: Purchaser Address Indicator 24Q1\")\n\n\n\n\n\n\n\n\n1. T.S. Kam, R for Visual Analytics Chapter 9 for visualisation of Ridgeline plots with quantile lines.\n2. Claus O. Wilke, Fundamentals of Data Visualization Chapter 2 for understanding and classification of variables.\n3. Stack Overflow, “Filter rows which contain a certain string” for removing en bloc sales from the data frame."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#overview",
    "href": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#overview",
    "title": "Take-home Ex 01",
    "section": "",
    "text": "There are two major residential property market in Singapore, namely public and private housing. Public housing aims to meet the basic need of the general public with monthly household income less than or equal to S$14,000. For families with monthly household income more than S$14,000, they need to turn to the private residential market.\n\n\n\nAssuming the role of a graphical editor of a media company, you are requested to prepare minimum two and maximum three data visualisation to reveal the private residential market and sub-markets of Singapore for the 1st quarter of 2024."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#set-up",
    "href": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#set-up",
    "title": "Take-home Ex 01",
    "section": "",
    "text": "T he pac::p_load() function to load the required R packages in the working environment. The following packages are used in this set up:\n\nggthemes: Extra themes, geoms, and scales for ggplot2.\ntidyverse: A collection of core packages designed for data science, used extensively for data preparation and wrangling.\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\ncolorspace:\nggiraph: for making ‘ggplot’ graphics interactive.\nplotly: R library for plotting interactive statistical graphs.\npatchwork: specially designed for combining separate ggplot2 graphs into a single figure.\nlubridate: for easy and fast parsing of Date / Time\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggdist: a ggplot2 extension specially design for visualising distribution and uncertainty\n\n\n# load tidyverse from pacman\npacman::p_load(tidyverse,dplyr,ggridges,\n               ggthemes,colorspace,ggiraph,\n               plotly,patchwork,lubridate, \n               ggrepel,ggdist)\n\n\n\n\nFive sets of data are provided for this exercise:\n\n2023 Quarter 1 Residential Transaction: ResidentialTransaction20240308160536.csv\n2023 Quarter 2 Residential Transaction: ResidentialTransaction20240308160736.csv\n2023 Quarter 3 Residential Transaction: ResidentialTransaction20240308161009.csv\n2023 Quarter 4 Residential Transaction: ResidentialTransaction20240308161109.csv\n2024 Quarter 1 Residential Transaction: ResidentialTransaction20240414220633.csv\n\nAll five sets of data consists of past transactions between Q1 of 2023 to Q1 of 2024, with 21 variables that will be explored in detail after the data sets are imported.\n\n# load transaction data using readr, part of tidyverse package\ndata.23Q1 &lt;- read_csv(\"data/ResidentialTransaction20240308160536.csv\")\ndata.23Q2 &lt;- read_csv(\"data/ResidentialTransaction20240308160736.csv\")\ndata.23Q3 &lt;- read_csv(\"data/ResidentialTransaction20240308161009.csv\")\ndata.23Q4 &lt;- read_csv(\"data/ResidentialTransaction20240308161109.csv\")\ndata.24Q1 &lt;- read_csv(\"data/ResidentialTransaction20240414220633.csv\")\n\nTo add new column Quarter into each of the csv file.\n\ndata.23Q1$Quarter &lt;- rep('23Q1',nrow(data.23Q1))\ndata.23Q2$Quarter &lt;- rep('23Q2',nrow(data.23Q2))\ndata.23Q3$Quarter &lt;- rep('23Q3',nrow(data.23Q3))\ndata.23Q4$Quarter &lt;- rep('23Q4',nrow(data.23Q4))\ndata.24Q1$Quarter &lt;- rep('24Q1',nrow(data.24Q1))\n\nCombining all the data set into one\n\ndf &lt;- rbind(data.23Q1,data.23Q2,data.23Q3,data.23Q4,data.24Q1)\n\nTo have a basic understanding of all 22 variables, glimpse() is used with 2024 Q1 data. We can see that there are 21 columns, with the following data types:\n\ndoubles: Transacted Price ($), Area (SQFT), Unit Price ($ PSF), Area (SQM), Unit Price ($ PSM), Number of Units\ncharacters: the rest of the 16 variables, including Quarter\n\nInterestingly, Nett Price is classified as characters, as it contains - character, although the rest of the columns should return doubles like Transacted Price ($)\n\nThe CodeThe Output\n\n\n\n# have a basic understanding for all 22 variables\nglimpse(df)\n\n\n\n\n\nRows: 26,806\nColumns: 22\n$ `Project Name`                &lt;chr&gt; \"THE REEF AT KING'S DOCK\", \"URBAN TREASU…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2317000, 1823500, 1421112, 1258112, 1280…\n$ `Area (SQFT)`                 &lt;dbl&gt; 882.65, 882.65, 1076.40, 1033.34, 871.88…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2625, 2066, 1320, 1218, 1468, 1767, 1095…\n$ `Sale Date`                   &lt;chr&gt; \"01 Jan 2023\", \"02 Jan 2023\", \"02 Jan 20…\n$ Address                       &lt;chr&gt; \"12 HARBOURFRONT AVENUE #05-32\", \"205 JA…\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Area (SQM)`                  &lt;dbl&gt; 82.0, 82.0, 100.0, 96.0, 81.0, 308.7, 42…\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 28256, 22238, 14211, 13105, 15802, 19015…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Condominium\", \"Executive…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 12/01/2021\", \"Freehold\", \"9…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"HDB\", \"Private\", \"HDB\", \"HDB\", \"HDB\", \"…\n$ `Postal Code`                 &lt;chr&gt; \"097996\", \"419535\", \"269343\", \"269294\", …\n$ `Postal District`             &lt;chr&gt; \"04\", \"14\", \"27\", \"27\", \"28\", \"19\", \"10\"…\n$ `Postal Sector`               &lt;chr&gt; \"09\", \"41\", \"26\", \"26\", \"79\", \"54\", \"27\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"East Region\", \"North …\n$ `Planning Area`               &lt;chr&gt; \"Bukit Merah\", \"Bedok\", \"Yishun\", \"Yishu…\n$ Quarter                       &lt;chr&gt; \"23Q1\", \"23Q1\", \"23Q1\", \"23Q1\", \"23Q1\", …\n\n\n\n\n\n\n\n\nOn top of the data types that is automatically assigned by R to help the program know how to process it, we can split the data into 4 types of data category to help ourselves for future analytics application. You can refer to the FAQ, Data Dictionary and Methodology in URA website.\n\n\n\n\n\n\n\n\n\nVariable\nType\nAppropriate Scale\nRemarks\n\n\n\n\nProject Name\nQualitative / Nominal\nDiscrete\n\n\n\nTransacted Price ($)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nArea (SQFT)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nUnit Price ($ PSF)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nSales Date\nDate / Ordinal\nDiscrete\n\n\n\nAddress\nQualitative / Nominal\nDiscrete\n\n\n\nType of Sale\nQualitative / Nominal\nDiscrete\n\n\n\nType of Area\nQualitative / Nominal\nDiscrete\n\n\n\nArea (SQM)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nUnit Price ($ PSF)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nNett Price($)\nQuantitative/ Numerical Continuous\nContinuous\n\n\n\nProperty Type\nQualitative/ Nominal\nDiscrete\n\n\n\nNumber of Units\nQuantitative/ Numerical Discrete\nDiscrete\n\n\n\nTenure\nNominal + Date / Ordinal\nNone + Discrete\nMixture of remaining lease date and tenure type and duration\n\n\nCompletion Date\nNominal + Date / Ordinal\nNone + Discrete\nMixture of “Uncomplete” status and completion date.\n\n\nPurchaser Address Indicator\nQualitative / Nominal\nDiscrete\n\n\n\nPostal Code\nQualitative / Nominal\nDiscrete\n\n\n\nPostal District\nQualitative / Nominal\nDiscrete\n\n\n\nPostal Sector\nQualitative / Nominal\nDiscrete\n\n\n\nPlanning Region\nQualitative / Nominal\nDiscrete\n\n\n\nPlanning Area\nQualitative / Nominal\nDiscrete\n\n\n\nQuarter\nDate / Ordinal\nDiscrete\nCreated for ease of data analysis\n\n\n\nNotes\n\nContextually, some of the variables could be classified under ordinal. For example, I can choose to fill in the order for Purchaser Address Indicator as Private category is higher order than HDB category. The context is purchaser that address is Private is likely to be a person with higher Social Economic Status (SES) compared to a purchaser with HDB as address. Same could be applied to Postal Code, Postal District, Postal Sector, Planning Region and Planning Area.\nSpecial Case: Tenure is still a character data type as of now. If we choose to use this column for analysis, we do need to do data preparation on this. Tenure date could also be classified under continuous data for more granularity. This variable will be a mixture of nominal data type with categories like “Freehold”, and also quantitative data type using its Tenure date.\nNett Price($) is still a character data type as of now. If we choose to use this column for analysis, we do need to do data preparation on this.\nSpecial Case: Completion Date is classified as character, but a quick look at the data df, we will realise that this variable is a mix of quantitative and qualitative data type of Uncomplete and actual date of completion, which could be converted and analyse as doubles.\n\nFor simplicity of data analysis\n1. Transacted Price ($) and Nett Price($) are similar. In this analysis, we will use Transacted Price ($) for simplicity, as Nett Price($) has fair amount of missing data, and it is prices after deducting the value of the indirect discounts or benefits, and not representative for all transactions.\n\nArea (SQFT) and Area (SQM), and Unit Price ($ PSF) and Unit Price ($ PSM) are similar measures in different unit. We will use Area (SQFT) and Unit Price ($ PSF)\n\nNext we look at the basic statistics of the current data using summary()\n\nThe OutputThe Code\n\n\n\n\n Project Name       Transacted Price ($)  Area (SQFT)       Unit Price ($ PSF)\n Length:26806       Min.   :   440000    Min.   :   322.9   Min.   : 138      \n Class :character   1st Qu.:  1280000    1st Qu.:   721.2   1st Qu.:1384      \n Mode  :character   Median :  1660000    Median :   990.3   Median :1762      \n                    Mean   :  2143286    Mean   :  1191.6   Mean   :1852      \n                    3rd Qu.:  2320000    3rd Qu.:  1302.4   3rd Qu.:2260      \n                    Max.   :392180000    Max.   :144883.4   Max.   :5756      \n                                                                              \n  Sale Date           Address          Type of Sale       Type of Area      \n Length:26806       Length:26806       Length:26806       Length:26806      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Area (SQM)      Unit Price ($ PSM) Nett Price($)      Property Type     \n Min.   :   30.0   Min.   : 1484      Length:26806       Length:26806      \n 1st Qu.:   67.0   1st Qu.:14893      Class :character   Class :character  \n Median :   92.0   Median :18966      Mode  :character   Mode  :character  \n Mean   :  110.5   Mean   :19930                                           \n 3rd Qu.:  121.0   3rd Qu.:24327                                           \n Max.   :13460.0   Max.   :61962                                           \n NA's   :6                                                                 \n Number of Units     Tenure          Completion Date   \n Min.   : 1.000   Length:26806       Length:26806      \n 1st Qu.: 1.000   Class :character   Class :character  \n Median : 1.000   Mode  :character   Mode  :character  \n Mean   : 1.005                                        \n 3rd Qu.: 1.000                                        \n Max.   :60.000                                        \n                                                       \n Purchaser Address Indicator Postal Code        Postal District   \n Length:26806                Length:26806       Length:26806      \n Class :character            Class :character   Class :character  \n Mode  :character            Mode  :character   Mode  :character  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n Postal Sector      Planning Region    Planning Area        Quarter         \n Length:26806       Length:26806       Length:26806       Length:26806      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n\n\n\n\n\n# data summary\nsummary(df)\n\n\n\n\nFrom the continuous data above, we can conclude that there is a possibility of extreme outliers. The Number of Units has a max of 60, while the 3rd Quartile is 1.000. Let’s examine the data by sorting it in descending order using arrange(desc()) and take a look at data with Number of Units, as it greatly affect the analysis of Transaction Price ($).\n\ndf %&gt;%\n  filter(`Number of Units` &gt; 1) %&gt;%\n  arrange(desc(`Number of Units`)) \n\n# A tibble: 12 × 22\n   `Project Name`      `Transacted Price ($)` `Area (SQFT)` `Unit Price ($ PSF)`\n   &lt;chr&gt;                                &lt;dbl&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n 1 MEYER PARK                       392180000       144883.                 2707\n 2 BAGNALL COURT                    115280000        68491.                 1683\n 3 KEW LODGE                         66800000        25177                  2653\n 4 KARTAR APARTMENTS                 18000000         6964.                 2585\n 5 MONDO MANSION BUIL…                6280000         5490.                 1144\n 6 N.A.                              10600000         6747.                 1571\n 7 N.A.                              61080008        32149.                 1900\n 8 N.A.                              32200000        14123.                 2280\n 9 N.A.                               6150000         4342.                 1416\n10 EAST VIEW GARDEN                   6100000         8338.                  732\n11 N.A.                               8000000         3659.                 2187\n12 CLAYMORE PLAZA                     7000000         4209.                 1663\n# ℹ 18 more variables: `Sale Date` &lt;chr&gt;, Address &lt;chr&gt;, `Type of Sale` &lt;chr&gt;,\n#   `Type of Area` &lt;chr&gt;, `Area (SQM)` &lt;dbl&gt;, `Unit Price ($ PSM)` &lt;dbl&gt;,\n#   `Nett Price($)` &lt;chr&gt;, `Property Type` &lt;chr&gt;, `Number of Units` &lt;dbl&gt;,\n#   Tenure &lt;chr&gt;, `Completion Date` &lt;chr&gt;, `Purchaser Address Indicator` &lt;chr&gt;,\n#   `Postal Code` &lt;chr&gt;, `Postal District` &lt;chr&gt;, `Postal Sector` &lt;chr&gt;,\n#   `Planning Region` &lt;chr&gt;, `Planning Area` &lt;chr&gt;, Quarter &lt;chr&gt;\n\n\nWe have 12 rows that have transactions containing more than 1 unit sold, and a check on the Address shows that there are 5 sales that are en bloc cases which should be remove from the analysis. These en bloc cases are likely sales to companies for purpose of redevelopment rather than individual purchasers. See this link on the Meyer Park en bloc transaction. Otherwise, those 7 cases of transactions with more than one unit sold should be included as are likely the cases of purchase of multiple units or the purchase of multiple adjacent plots of land.\n\n\n\nFor the follow code chunk, we aim to remove all the enbloc sales, including the 5 transactions that we have found in section 2.3. While we have no idea if there are more than 5 cases of en bloc sales, the follow code chunk will effectively remove all the rows with the string “ENBLOC” in Address column.\n\ndf.c &lt;- df %&gt;%\n  filter(!grepl('ENBLOC', df$Address))\ndf.c\n\n# A tibble: 26,801 × 22\n   `Project Name`      `Transacted Price ($)` `Area (SQFT)` `Unit Price ($ PSF)`\n   &lt;chr&gt;                                &lt;dbl&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n 1 THE REEF AT KING'S…                2317000          883.                 2625\n 2 URBAN TREASURES                    1823500          883.                 2066\n 3 NORTH GAIA                         1421112         1076.                 1320\n 4 NORTH GAIA                         1258112         1033.                 1218\n 5 PARC BOTANNIA                      1280000          872.                 1468\n 6 NANYANG PARK                       5870000         3323.                 1767\n 7 PALMS @ SIXTH AVEN…                4950000         4521.                 1095\n 8 N.A.                               3260000         1555.                 2096\n 9 WHISTLER GRAND                      850000          441.                 1926\n10 NORTHOAKS                          1268000         1604.                  791\n# ℹ 26,791 more rows\n# ℹ 18 more variables: `Sale Date` &lt;chr&gt;, Address &lt;chr&gt;, `Type of Sale` &lt;chr&gt;,\n#   `Type of Area` &lt;chr&gt;, `Area (SQM)` &lt;dbl&gt;, `Unit Price ($ PSM)` &lt;dbl&gt;,\n#   `Nett Price($)` &lt;chr&gt;, `Property Type` &lt;chr&gt;, `Number of Units` &lt;dbl&gt;,\n#   Tenure &lt;chr&gt;, `Completion Date` &lt;chr&gt;, `Purchaser Address Indicator` &lt;chr&gt;,\n#   `Postal Code` &lt;chr&gt;, `Postal District` &lt;chr&gt;, `Postal Sector` &lt;chr&gt;,\n#   `Planning Region` &lt;chr&gt;, `Planning Area` &lt;chr&gt;, Quarter &lt;chr&gt;\n\n\nIn the df.c tibble, it showed that 5 rows had been removed. Now there is only 26,801 rows instead of the original 26,806 rows in the initial df. This confirm that there are only 5 en bloc sales in the data frame.\n\n\n\nAfter removing the 5 en bloc sales, we are still left with 7 transactions that has between 2 to 4 units sold per transaction. This has caused the Transacted Price ($) to be greatly inflated. In dealing with this, we can (1) choose to divide the Transacted Price ($) by the number of units sold, or (2) remove them from the analysis. In this analysis, we will be using method (2), to remove these transactions from the data set for further analysis. The reason to do so is that we cannot determine at this stage that each unit in the sales are equally priced, or whether each unit has the same Area size.\n\ndf.c &lt;- df.c %&gt;%\n  filter(`Number of Units` &lt;= 1)\n\n\n\n\nLand vs. Strata\nTo lend some background context, you can refer to this link: Landed vs. Strata Landed Basically, Strata means you only owns the property build on this land, but do not own the land itself. Land means you own the land and have more freedom and responsibility in terms of the constructions etc. The application of this knowledge for analysis is that Strata and Land classification can be identify as the sub-markets as owning a piece of land is likely to be more expensive for the purchaser compared to Strata title.\nThe plot below shows the 10 sub-markets in Singapore private housing market.\nLand\n\nApartment\nDetached House\nSemi-Detached House\nTerrace House\n\nStrata\n\nApartment\nCondominium\nDetached House\nExecutive Condominium\nSemi-Detached House\nTerrace House\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.c %&gt;%\n  select(`Transacted Price ($)`, `Type of Area`, `Property Type`) %&gt;%\n  group_by(`Property Type`, `Type of Area`) %&gt;%\n  summarise(`Median Transaction Price ($)` = median(`Transacted Price ($)`, .groups = 'drop')) %&gt;%\n  ggplot(aes(x = `Property Type`, \n             y = `Median Transaction Price ($)`, \n             color = `Type of Area`)) +  # Adding color aesthetic based on Type of Area\n  geom_point() +\n  theme_economist() +\n  theme(axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust=1)) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(color = \"Type of Area\")\n\n\n\n\nConclusion\nFrom the Median Transacted Price plot above, we observe that the 10 sub-markets are distinct in pricing, with Land Type of Area generally priced a lot higher than Strata, especially in the Apartment and Detached House categories."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#data-visualisation",
    "href": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#data-visualisation",
    "title": "Take-home Ex 01",
    "section": "",
    "text": "In this section, we would want to focus on the Apartment(Strata) Condominium and Executive Condominium sub-market in Singapore, out of the 10 sub-markets identified in section 2.3.3. As stated in section 1.1 Setting the scene, for families with monthly household income more than S$14,000, they need to turn to the private residential market. However, due limited number of plots to be shown, we will take Condominium as example for the illustrations\nAssumptions and considerations for the selecting the sub-markets to visualise:\n\nAffordability - According to the this article, apartment(Strata), Executive Condominium and Condominium are the next most affordable type of private housing if the family is not eligible to purchase public housing. This corroborate with the Median Transaction Price plot above.\nTarget Audience - The target audience for this graphics will be the public who would like to consider housing option in the private residential market. The target audience are those who’s family household are above the limits and would like to look for relatively affordable housing option in the private residential market as mentioned above in point 1.\nInformation Required - Pricing, Location, Types of Sales, Unit Price, Size of Property"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#the-plots",
    "href": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#the-plots",
    "title": "Take-home Ex 01",
    "section": "",
    "text": "Plot 1: Pricing for non-freehold Condominium\nThis first plot focus on Condominium that are not of Freehold Tenure. Compared to freehold property, these non-freehold Condominiums are relatively lower price. The plot below shows the distribution of Transacted Price ($) in the five Planning Region in Singapore, and reveal the median price of each region varies from 1 to 2 million, with North Region with a tighter spread at a lower price than the other four regions. The Central Region has the highest median at 2 million, with a widest spread. The 4th quantile of the Central Region consists of Transacted Price ($) between 2.5 to just after 9 million. This shows that the Condominiums with Lease are more affordable in the North and East Region in Singapore, with North East and West Region next, and finally the most expensive Condominium would be the ones found in Central Region.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.c %&gt;%\n  filter(`Type of Area` == 'Strata',`Property Type` == 'Condominium', `Quarter` == '24Q1', `Tenure`!= 'Freehold') %&gt;%\n  ggplot(aes(x = `Transacted Price ($)`,\n           y = `Planning Region`,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE,\n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\",\n                       option = \"A\") +\n  scale_x_continuous(name = \"Transacted Price in million ($)\",\n    labels = c(0,1,2,3,4,5,6,7,8,9,10), \n    breaks = c(0,1000000,2000000,3000000,4000000,5000000,6000000,7000000,8000000,9000000,10000000)) +\n  theme_economist() + \n  ggtitle(\"24Q1 Non-Freehold Condo Transacted Price by Planning Region\") +\n  theme(plot.title = element_text(size=12))\n\n\n\n\n\n\n\nPlot 2: Mean and Median Unit Price for non-freehold Condominium\nPlot 2 focus on the mean and median Unit Price ($ PSF) for the non-freehold Condominium in the Central Region. The Mean Unit Price ($ PSF) is represented by the red dot in each of the violin plot and boxplot. While we see a slight rise in the mean and median Unit Price ($ PSF) in 23Q2, where there are more transactions with Unit Price ($ PSF) in the $2,500 range, represented by the bulge at the top of the violin plot. For the rest of the quarters, the mean and median Unit Price ($ PSF) hovers around the range of $2,000. Despite the mean and median are at $2,000 for the 24Q1, there is also a sizable transaction of Unit Price ($ PSF) of $3,000 happening in 24Q1, as represented by the slight bulge at the top of the violin plot.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nquarter_means &lt;- df.c %&gt;%\n  filter(`Property Type` == 'Condominium',`Planning Region` == \"Central Region\", `Tenure`!= 'Freehold') %&gt;%\n  group_by(Quarter) %&gt;%\n  summarise(mean_uprice = mean(`Unit Price ($ PSF)`))\n\ndf.c %&gt;%\n  filter(`Property Type` == 'Condominium', `Planning Region` == \"Central Region\",`Tenure`!= 'Freehold') %&gt;%\n  ggplot(aes(y=`Unit Price ($ PSF)`,\n           x=`Quarter`)\n       )+\n  geom_violin() +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  geom_point(data = quarter_means, \n             aes(y = mean_uprice), \n             color = \"red\", \n             size = 3) +\n    scale_y_continuous(labels = scales::comma) +\n  theme_economist() +\n  ggtitle(\"Condominium Unit Price ($ PSF) for Central Region\") \n\n\n\n\n\n\n\nPlot 3: Purchaser Address Indicator for non-freehold Condominium\nPlot 3 focus on the top and bottom 5% of the different types of purchasers for non-freehold Condominium in the Central Region. We assume that the N.A type are purchasers who do not own any property in Singapore. The top 5% of the Private category purchase units priced at $4.5-$9 million. For HDB and N.A categories, the top 5% are purchasing units that is priced between $3 to 4 million. Both bottom 5% Private and HDB categories purchase unit of $1 million, while the bottom 5% N.A category are buying unit just above $1 million. The plausible reason is that purchaser in N.A. category may want to buy a unit for their own stay since they do not own any other properties. Hence, they are paying slightly more for a larger unit relative to other two types of purchasers, who might be buying studio units for rental income.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.c %&gt;%\n  filter(`Property Type` == 'Condominium', `Quarter` == '24Q1', `Planning Region` == \"Central Region\", `Tenure` != 'Freehold') %&gt;%\n  ggplot(aes(x = `Transacted Price ($)`, \n           y = `Purchaser Address Indicator`, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.05, 0.95)\n    ) +\n  scale_fill_manual(\n    name = \"Percentage of Total\",\n    values = c(\"#FF00FFA0\", \"#FFE4E1A0\", \"#0000ffA0\"),\n    labels = c(\"Bottom 5%\", \"5-95%\", \"Top 5%\")\n  ) +\n  theme_ridges()+\n  theme_economist() +\n  theme(plot.title = element_text(size=12)) +\n  scale_x_continuous(name = \"Transacted Price in million ($)\",\n    labels = c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15), \n    breaks = c(0,1000000,2000000,3000000,4000000,5000000,6000000,7000000,8000000,9000000,10000000,11000000, 12000000, 13000000, 14000000, 15000000)) + \n  ggtitle(\"Central Region Condominium: Purchaser Address Indicator 24Q1\")"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#references",
    "href": "Take-home Exercise/Take-home_Ex01/Take-home_Ex01.html#references",
    "title": "Take-home Ex 01",
    "section": "",
    "text": "1. T.S. Kam, R for Visual Analytics Chapter 9 for visualisation of Ridgeline plots with quantile lines.\n2. Claus O. Wilke, Fundamentals of Data Visualization Chapter 2 for understanding and classification of variables.\n3. Stack Overflow, “Filter rows which contain a certain string” for removing en bloc sales from the data frame."
  },
  {
    "objectID": "Self-Practice/Self-Practice_03/Self-Practice_03.html",
    "href": "Self-Practice/Self-Practice_03/Self-Practice_03.html",
    "title": "Self Practice 3: R for Data Science",
    "section": "",
    "text": "library(tidyverse)\n\n\n\n\nThe principles of tidy data might seem so obvious that you wonder if you’ll ever encounter a dataset that isn’t tidy. Unfortunately, however, most real data is untidy. There are two main reasons:\n\nData is often organized to facilitate some goal other than analysis. For example, it’s common for data to be structured to make data entry, not analysis, easy.\nMost people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.\n\nThis means that most real analyses will require at least a little tidying. You’ll begin by figuring out what the underlying variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. Next, you’ll pivot your data into a tidy form, with variables in the columns and observations in the rows.\ntidyr provides two functions for pivoting data: pivot_longer() and pivot_wider(). We’ll first start with pivot_longer() because it’s the most common case. Let’s dive into some examples.\n\n\nThe billboard dataset records the billboard rank of songs in the year 2000:\n\nbillboard\n\n# A tibble: 317 × 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D… Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give… 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc… 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do… 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try … 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo… Open… 2000-08-26      76    76    74    69    68    67    61    58\n# ℹ 307 more rows\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, …\n\n\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each week1. Here, the column names are one variable (the week) and the cell values are another (the rank).\nTo tidy this data, we’ll use pivot_longer():\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n# A tibble: 24,092 × 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ℹ 24,082 more rows\n\n\nAfter the data, there are three key arguments:\n\ncols specifies which columns need to be pivoted, i.e. which columns aren’t variables. This argument uses the same syntax as select() so here we could use !c(artist, track, date.entered) or starts_with(\"wk\").\nnames_to names the variable stored in the column names, we named that variable week.\nvalues_to names the variable stored in the cell values, we named that variable rank.\n\nNote that in the code \"week\" and \"rank\" are quoted because those are new variables we’re creating, they don’t yet exist in the data when we run the pivot_longer() call.\nNow let’s turn our attention to the resulting, longer data frame. What happens if a song is in the top 100 for less than 76 weeks? Take 2 Pac’s “Baby Don’t Cry”, for example. The above output suggests that it was only in the top 100 for 7 weeks, and all the remaining weeks are filled in with missing values. These NAs don’t really represent unknown observations; they were forced to exist by the structure of the dataset2, so we can ask pivot_longer() to get rid of them by setting values_drop_na = TRUE:\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n\nThe number of rows is now much lower, indicating that many rows with NAs were dropped.\nYou might also wonder what happens if a song is in the top 100 for more than 76 weeks? We can’t tell from this data, but you might guess that additional columns wk77, wk78, … would be added to the dataset.\nThis data is now tidy, but we could make future computation a bit easier by converting values of week from character strings to numbers using mutate() and readr::parse_number(). parse_number() is a handy function that will extract the first number from a string, ignoring all other text.\n\nbillboard_longer &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  mutate(\n    week = parse_number(week)\n  )\n\nNow that we have all the week numbers in one variable and all the rank values in another, we’re in a good position to visualize how song ranks vary over time. The code is shown below and the result is in Figure 5.2. We can see that very few songs stay in the top 100 for more than 20 weeks.\n\nbillboard_longer |&gt; \n  ggplot(aes(x = week, y = rank, group = track)) + \n  geom_line(alpha = 0.25) + \n  scale_y_reverse()\n\n\n\n\n\n\n\n\n\n\n\nNow that you’ve seen how we can use pivoting to reshape our data, let’s take a little time to gain some intuition about what pivoting does to the data. Let’s start with a very simple dataset to make it easier to see what’s happening. Suppose we have three patients with ids A, B, and C, and we take two blood pressure measurements on each patient. We’ll create the data with tribble(), a handy function for constructing small tibbles by hand:\n\ndf &lt;- tribble(\n  ~id,  ~bp1, ~bp2,\n   \"A\",  100,  120,\n   \"B\",  140,  115,\n   \"C\",  120,  125\n)\n\nWe want our new dataset to have three variables: id (already exists), measurement (the column names), and value (the cell values). To achieve this, we need to pivot df longer:\n\ndf_longer &lt;- df |&gt; \n  pivot_longer(\n    cols = starts_with(\"bp\"),\n    names_to = \"measurement\",\n    values_to = \"value\"\n  )\n\nHow does the reshaping work? It’s easier to see if we think about it column by column. As shown in Figure 5.3, the values in a column that was already a variable in the original dataset (id) need to be repeated, once for each column that is pivoted.\n\nThe column names become values in a new variable, whose name is defined by names_to, as shown in Figure 5.4. They need to be repeated once for each row in the original dataset.\n\nThe cell values also become values in a new variable, with a name defined by values_to. They are unwound row by row. Figure 5.5 illustrates the process.\n\n\n\n\nA more challenging situation occurs when you have multiple pieces of information crammed into the column names, and you would like to store these in separate new variables. For example, take the who2 dataset, the source of table1 and friends that you saw above:\n\nwho2\n\n# A tibble: 7,240 × 58\n   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554 sp_m_5564\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan  1980       NA        NA        NA        NA        NA        NA\n 2 Afghanistan  1981       NA        NA        NA        NA        NA        NA\n 3 Afghanistan  1982       NA        NA        NA        NA        NA        NA\n 4 Afghanistan  1983       NA        NA        NA        NA        NA        NA\n 5 Afghanistan  1984       NA        NA        NA        NA        NA        NA\n 6 Afghanistan  1985       NA        NA        NA        NA        NA        NA\n 7 Afghanistan  1986       NA        NA        NA        NA        NA        NA\n 8 Afghanistan  1987       NA        NA        NA        NA        NA        NA\n 9 Afghanistan  1988       NA        NA        NA        NA        NA        NA\n10 Afghanistan  1989       NA        NA        NA        NA        NA        NA\n# ℹ 7,230 more rows\n# ℹ 50 more variables: sp_m_65 &lt;dbl&gt;, sp_f_014 &lt;dbl&gt;, sp_f_1524 &lt;dbl&gt;,\n#   sp_f_2534 &lt;dbl&gt;, sp_f_3544 &lt;dbl&gt;, sp_f_4554 &lt;dbl&gt;, sp_f_5564 &lt;dbl&gt;,\n#   sp_f_65 &lt;dbl&gt;, sn_m_014 &lt;dbl&gt;, sn_m_1524 &lt;dbl&gt;, sn_m_2534 &lt;dbl&gt;,\n#   sn_m_3544 &lt;dbl&gt;, sn_m_4554 &lt;dbl&gt;, sn_m_5564 &lt;dbl&gt;, sn_m_65 &lt;dbl&gt;,\n#   sn_f_014 &lt;dbl&gt;, sn_f_1524 &lt;dbl&gt;, sn_f_2534 &lt;dbl&gt;, sn_f_3544 &lt;dbl&gt;,\n#   sn_f_4554 &lt;dbl&gt;, sn_f_5564 &lt;dbl&gt;, sn_f_65 &lt;dbl&gt;, ep_m_014 &lt;dbl&gt;, …\n\n\nThis dataset, collected by the World Health Organisation, records information about tuberculosis diagnoses. There are two columns that are already variables and are easy to interpret: country and year. They are followed by 56 columns like sp_m_014, ep_m_4554, and rel_m_3544. If you stare at these columns for long enough, you’ll notice there’s a pattern. Each column name is made up of three pieces separated by _. The first piece, sp/rel/ep, describes the method used for the diagnosis, the second piece, m/f is the gender (coded as a binary variable in this dataset), and the third piece, 014/1524/2534/3544/4554/5564/65 is the age range (014 represents 0-14, for example).\nSo in this case we have six pieces of information recorded in who2: the country and the year (already columns); the method of diagnosis, the gender category, and the age range category (contained in the other column names); and the count of patients in that category (cell values). To organize these six pieces of information in six separate columns, we use pivot_longer() with a vector of column names for names_to and instructors for splitting the original variable names into pieces for names_sep as well as a column name for values_to:\n\nwho2 |&gt; \n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n    names_sep = \"_\",\n    values_to = \"count\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 76,046 × 6\n   country      year diagnosis gender age   count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan  1997 sp        m      014       0\n 2 Afghanistan  1997 sp        m      1524     10\n 3 Afghanistan  1997 sp        m      2534      6\n 4 Afghanistan  1997 sp        m      3544      3\n 5 Afghanistan  1997 sp        m      4554      5\n 6 Afghanistan  1997 sp        m      5564      2\n 7 Afghanistan  1997 sp        m      65        0\n 8 Afghanistan  1997 sp        f      014       5\n 9 Afghanistan  1997 sp        f      1524     38\n10 Afghanistan  1997 sp        f      2534     36\n# ℹ 76,036 more rows\n\n\n\n\n\nThe next step up in complexity is when the column names include a mix of variable values and variable names. For example, take the household dataset:\n\nhousehold\n\n# A tibble: 5 × 5\n  family dob_child1 dob_child2 name_child1 name_child2\n   &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;      \n1      1 1998-11-26 2000-01-29 Susan       Jose       \n2      2 1996-06-22 NA         Mark        &lt;NA&gt;       \n3      3 2002-07-11 2004-04-05 Sam         Seth       \n4      4 2004-10-10 2009-08-27 Craig       Khai       \n5      5 2000-12-05 2005-02-28 Parker      Gracie     \n\n\nThis dataset contains data about five families, with the names and dates of birth of up to two children. The new challenge in this dataset is that the column names contain the names of two variables (dob, name) and the values of another (child, with values 1 or 2). To solve this problem we again need to supply a vector to names_to but this time we use the special \".value\" sentinel; this isn’t the name of a variable but a unique value that tells pivot_longer() to do something different. This overrides the usual values_to argument to use the first component of the pivoted column name as a variable name in the output.\n\nhousehold |&gt; \n  pivot_longer(\n    cols = !family, \n    names_to = c(\".value\", \"child\"), \n    names_sep = \"_\", \n    values_drop_na = TRUE\n  )\n\n# A tibble: 9 × 4\n  family child  dob        name  \n   &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt; \n1      1 child1 1998-11-26 Susan \n2      1 child2 2000-01-29 Jose  \n3      2 child1 1996-06-22 Mark  \n4      3 child1 2002-07-11 Sam   \n5      3 child2 2004-04-05 Seth  \n6      4 child1 2004-10-10 Craig \n7      4 child2 2009-08-27 Khai  \n8      5 child1 2000-12-05 Parker\n9      5 child2 2005-02-28 Gracie\n\n\nWe again use values_drop_na = TRUE, since the shape of the input forces the creation of explicit missing variables (e.g., for families with only one child).\nFigure 5.7 illustrates the basic idea with a simpler example. When you use \".value\" in names_to, the column names in the input contribute to both values and variable names in the output.\n\n\n\n\n\nSo far we’ve used pivot_longer() to solve the common class of problems where values have ended up in column names. Next we’ll pivot (HA HA) to pivot_wider(), which makes datasets wider by increasing columns and reducing rows and helps when one observation is spread across multiple rows. This seems to arise less commonly in the wild, but it does seem to crop up a lot when dealing with governmental data.\nWe’ll start by looking at cms_patient_experience, a dataset from the Centers of Medicare and Medicaid services that collects data about patient experiences:\n\ncms_patient_experience\n\n# A tibble: 500 × 5\n   org_pac_id org_nm                           measure_cd measure_title prf_rate\n   &lt;chr&gt;      &lt;chr&gt;                            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       63\n 2 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       87\n 3 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       86\n 4 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       57\n 5 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       85\n 6 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       24\n 7 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       59\n 8 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       85\n 9 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       83\n10 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       63\n# ℹ 490 more rows\n\n\n\ncms_patient_experience |&gt; \n  distinct(measure_cd, measure_title)\n\n# A tibble: 6 × 2\n  measure_cd   measure_title                                                    \n  &lt;chr&gt;        &lt;chr&gt;                                                            \n1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and Infor…\n2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate               \n3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider                 \n4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education               \n5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff           \n6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources             \n\n\nNeither of these columns will make particularly great variable names: measure_cd doesn’t hint at the meaning of the variable and measure_title is a long sentence containing spaces. We’ll use measure_cd as the source for our new column names for now, but in a real analysis you might want to create your own variable names that are both short and meaningful.\npivot_wider() has the opposite interface to pivot_longer(): instead of choosing new column names, we need to provide the existing columns that define the values (values_from) and the column name (names_from):\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 500 × 9\n   org_pac_id org_nm           measure_title CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICA… CAHPS for MI…          63          NA          NA\n 2 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          87          NA\n 3 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          86\n 4 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 5 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 6 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 7 0446162697 ASSOCIATION OF … CAHPS for MI…          59          NA          NA\n 8 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          85          NA\n 9 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          83\n10 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          NA\n# ℹ 490 more rows\n# ℹ 3 more variables: CAHPS_GRP_5 &lt;dbl&gt;, CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\n\nThe output doesn’t look quite right; we still seem to have multiple rows for each organization. That’s because, we also need to tell pivot_wider() which column or columns have values that uniquely identify each row; in this case those are the variables starting with \"org\":\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 95 × 8\n   org_pac_id org_nm CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC C…          63          87          86          57          85\n 2 0446162697 ASSOC…          59          85          83          63          88\n 3 0547164295 BEAVE…          49          NA          75          44          73\n 4 0749333730 CAPE …          67          84          85          65          82\n 5 0840104360 ALLIA…          66          87          87          64          87\n 6 0840109864 REX H…          73          87          84          67          91\n 7 0840513552 SCL H…          58          83          76          58          78\n 8 0941545784 GRITM…          46          86          81          54          NA\n 9 1052612785 COMMU…          65          84          80          58          87\n10 1254237779 OUR L…          61          NA          NA          65          NA\n# ℹ 85 more rows\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;\n\n\n\n\nTo understand how pivot_wider() works, let’s again start with a very simple dataset. This time we have two patients with ids A and B, we have three blood pressure measurements on patient A and two on patient B:\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"B\",        \"bp1\",    140,\n  \"B\",        \"bp2\",    115, \n  \"A\",        \"bp2\",    120,\n  \"A\",        \"bp3\",    105\n)\n\nWe’ll take the values from the value column and the names from the measurement column:\n\ndf |&gt; \n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n\n# A tibble: 2 × 4\n  id      bp1   bp2   bp3\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A       100   120   105\n2 B       140   115    NA\n\n\nTo begin the process pivot_wider() needs to first figure out what will go in the rows and columns. The new column names will be the unique values of measurement.\n\ndf |&gt; \n  distinct(measurement) |&gt; \n  pull()\n\n[1] \"bp1\" \"bp2\" \"bp3\""
  },
  {
    "objectID": "Self-Practice/Self-Practice_03/Self-Practice_03.html#getting-started",
    "href": "Self-Practice/Self-Practice_03/Self-Practice_03.html#getting-started",
    "title": "Self Practice 3: R for Data Science",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "Self-Practice/Self-Practice_03/Self-Practice_03.html#lengthening-data",
    "href": "Self-Practice/Self-Practice_03/Self-Practice_03.html#lengthening-data",
    "title": "Self Practice 3: R for Data Science",
    "section": "",
    "text": "The principles of tidy data might seem so obvious that you wonder if you’ll ever encounter a dataset that isn’t tidy. Unfortunately, however, most real data is untidy. There are two main reasons:\n\nData is often organized to facilitate some goal other than analysis. For example, it’s common for data to be structured to make data entry, not analysis, easy.\nMost people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.\n\nThis means that most real analyses will require at least a little tidying. You’ll begin by figuring out what the underlying variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. Next, you’ll pivot your data into a tidy form, with variables in the columns and observations in the rows.\ntidyr provides two functions for pivoting data: pivot_longer() and pivot_wider(). We’ll first start with pivot_longer() because it’s the most common case. Let’s dive into some examples.\n\n\nThe billboard dataset records the billboard rank of songs in the year 2000:\n\nbillboard\n\n# A tibble: 317 × 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D… Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give… 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc… 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do… 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try … 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo… Open… 2000-08-26      76    76    74    69    68    67    61    58\n# ℹ 307 more rows\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, …\n\n\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each week1. Here, the column names are one variable (the week) and the cell values are another (the rank).\nTo tidy this data, we’ll use pivot_longer():\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n# A tibble: 24,092 × 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ℹ 24,082 more rows\n\n\nAfter the data, there are three key arguments:\n\ncols specifies which columns need to be pivoted, i.e. which columns aren’t variables. This argument uses the same syntax as select() so here we could use !c(artist, track, date.entered) or starts_with(\"wk\").\nnames_to names the variable stored in the column names, we named that variable week.\nvalues_to names the variable stored in the cell values, we named that variable rank.\n\nNote that in the code \"week\" and \"rank\" are quoted because those are new variables we’re creating, they don’t yet exist in the data when we run the pivot_longer() call.\nNow let’s turn our attention to the resulting, longer data frame. What happens if a song is in the top 100 for less than 76 weeks? Take 2 Pac’s “Baby Don’t Cry”, for example. The above output suggests that it was only in the top 100 for 7 weeks, and all the remaining weeks are filled in with missing values. These NAs don’t really represent unknown observations; they were forced to exist by the structure of the dataset2, so we can ask pivot_longer() to get rid of them by setting values_drop_na = TRUE:\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n\nThe number of rows is now much lower, indicating that many rows with NAs were dropped.\nYou might also wonder what happens if a song is in the top 100 for more than 76 weeks? We can’t tell from this data, but you might guess that additional columns wk77, wk78, … would be added to the dataset.\nThis data is now tidy, but we could make future computation a bit easier by converting values of week from character strings to numbers using mutate() and readr::parse_number(). parse_number() is a handy function that will extract the first number from a string, ignoring all other text.\n\nbillboard_longer &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  mutate(\n    week = parse_number(week)\n  )\n\nNow that we have all the week numbers in one variable and all the rank values in another, we’re in a good position to visualize how song ranks vary over time. The code is shown below and the result is in Figure 5.2. We can see that very few songs stay in the top 100 for more than 20 weeks.\n\nbillboard_longer |&gt; \n  ggplot(aes(x = week, y = rank, group = track)) + \n  geom_line(alpha = 0.25) + \n  scale_y_reverse()\n\n\n\n\n\n\n\n\n\n\n\nNow that you’ve seen how we can use pivoting to reshape our data, let’s take a little time to gain some intuition about what pivoting does to the data. Let’s start with a very simple dataset to make it easier to see what’s happening. Suppose we have three patients with ids A, B, and C, and we take two blood pressure measurements on each patient. We’ll create the data with tribble(), a handy function for constructing small tibbles by hand:\n\ndf &lt;- tribble(\n  ~id,  ~bp1, ~bp2,\n   \"A\",  100,  120,\n   \"B\",  140,  115,\n   \"C\",  120,  125\n)\n\nWe want our new dataset to have three variables: id (already exists), measurement (the column names), and value (the cell values). To achieve this, we need to pivot df longer:\n\ndf_longer &lt;- df |&gt; \n  pivot_longer(\n    cols = starts_with(\"bp\"),\n    names_to = \"measurement\",\n    values_to = \"value\"\n  )\n\nHow does the reshaping work? It’s easier to see if we think about it column by column. As shown in Figure 5.3, the values in a column that was already a variable in the original dataset (id) need to be repeated, once for each column that is pivoted.\n\nThe column names become values in a new variable, whose name is defined by names_to, as shown in Figure 5.4. They need to be repeated once for each row in the original dataset.\n\nThe cell values also become values in a new variable, with a name defined by values_to. They are unwound row by row. Figure 5.5 illustrates the process.\n\n\n\n\nA more challenging situation occurs when you have multiple pieces of information crammed into the column names, and you would like to store these in separate new variables. For example, take the who2 dataset, the source of table1 and friends that you saw above:\n\nwho2\n\n# A tibble: 7,240 × 58\n   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554 sp_m_5564\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan  1980       NA        NA        NA        NA        NA        NA\n 2 Afghanistan  1981       NA        NA        NA        NA        NA        NA\n 3 Afghanistan  1982       NA        NA        NA        NA        NA        NA\n 4 Afghanistan  1983       NA        NA        NA        NA        NA        NA\n 5 Afghanistan  1984       NA        NA        NA        NA        NA        NA\n 6 Afghanistan  1985       NA        NA        NA        NA        NA        NA\n 7 Afghanistan  1986       NA        NA        NA        NA        NA        NA\n 8 Afghanistan  1987       NA        NA        NA        NA        NA        NA\n 9 Afghanistan  1988       NA        NA        NA        NA        NA        NA\n10 Afghanistan  1989       NA        NA        NA        NA        NA        NA\n# ℹ 7,230 more rows\n# ℹ 50 more variables: sp_m_65 &lt;dbl&gt;, sp_f_014 &lt;dbl&gt;, sp_f_1524 &lt;dbl&gt;,\n#   sp_f_2534 &lt;dbl&gt;, sp_f_3544 &lt;dbl&gt;, sp_f_4554 &lt;dbl&gt;, sp_f_5564 &lt;dbl&gt;,\n#   sp_f_65 &lt;dbl&gt;, sn_m_014 &lt;dbl&gt;, sn_m_1524 &lt;dbl&gt;, sn_m_2534 &lt;dbl&gt;,\n#   sn_m_3544 &lt;dbl&gt;, sn_m_4554 &lt;dbl&gt;, sn_m_5564 &lt;dbl&gt;, sn_m_65 &lt;dbl&gt;,\n#   sn_f_014 &lt;dbl&gt;, sn_f_1524 &lt;dbl&gt;, sn_f_2534 &lt;dbl&gt;, sn_f_3544 &lt;dbl&gt;,\n#   sn_f_4554 &lt;dbl&gt;, sn_f_5564 &lt;dbl&gt;, sn_f_65 &lt;dbl&gt;, ep_m_014 &lt;dbl&gt;, …\n\n\nThis dataset, collected by the World Health Organisation, records information about tuberculosis diagnoses. There are two columns that are already variables and are easy to interpret: country and year. They are followed by 56 columns like sp_m_014, ep_m_4554, and rel_m_3544. If you stare at these columns for long enough, you’ll notice there’s a pattern. Each column name is made up of three pieces separated by _. The first piece, sp/rel/ep, describes the method used for the diagnosis, the second piece, m/f is the gender (coded as a binary variable in this dataset), and the third piece, 014/1524/2534/3544/4554/5564/65 is the age range (014 represents 0-14, for example).\nSo in this case we have six pieces of information recorded in who2: the country and the year (already columns); the method of diagnosis, the gender category, and the age range category (contained in the other column names); and the count of patients in that category (cell values). To organize these six pieces of information in six separate columns, we use pivot_longer() with a vector of column names for names_to and instructors for splitting the original variable names into pieces for names_sep as well as a column name for values_to:\n\nwho2 |&gt; \n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n    names_sep = \"_\",\n    values_to = \"count\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 76,046 × 6\n   country      year diagnosis gender age   count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan  1997 sp        m      014       0\n 2 Afghanistan  1997 sp        m      1524     10\n 3 Afghanistan  1997 sp        m      2534      6\n 4 Afghanistan  1997 sp        m      3544      3\n 5 Afghanistan  1997 sp        m      4554      5\n 6 Afghanistan  1997 sp        m      5564      2\n 7 Afghanistan  1997 sp        m      65        0\n 8 Afghanistan  1997 sp        f      014       5\n 9 Afghanistan  1997 sp        f      1524     38\n10 Afghanistan  1997 sp        f      2534     36\n# ℹ 76,036 more rows\n\n\n\n\n\nThe next step up in complexity is when the column names include a mix of variable values and variable names. For example, take the household dataset:\n\nhousehold\n\n# A tibble: 5 × 5\n  family dob_child1 dob_child2 name_child1 name_child2\n   &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;      \n1      1 1998-11-26 2000-01-29 Susan       Jose       \n2      2 1996-06-22 NA         Mark        &lt;NA&gt;       \n3      3 2002-07-11 2004-04-05 Sam         Seth       \n4      4 2004-10-10 2009-08-27 Craig       Khai       \n5      5 2000-12-05 2005-02-28 Parker      Gracie     \n\n\nThis dataset contains data about five families, with the names and dates of birth of up to two children. The new challenge in this dataset is that the column names contain the names of two variables (dob, name) and the values of another (child, with values 1 or 2). To solve this problem we again need to supply a vector to names_to but this time we use the special \".value\" sentinel; this isn’t the name of a variable but a unique value that tells pivot_longer() to do something different. This overrides the usual values_to argument to use the first component of the pivoted column name as a variable name in the output.\n\nhousehold |&gt; \n  pivot_longer(\n    cols = !family, \n    names_to = c(\".value\", \"child\"), \n    names_sep = \"_\", \n    values_drop_na = TRUE\n  )\n\n# A tibble: 9 × 4\n  family child  dob        name  \n   &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt; \n1      1 child1 1998-11-26 Susan \n2      1 child2 2000-01-29 Jose  \n3      2 child1 1996-06-22 Mark  \n4      3 child1 2002-07-11 Sam   \n5      3 child2 2004-04-05 Seth  \n6      4 child1 2004-10-10 Craig \n7      4 child2 2009-08-27 Khai  \n8      5 child1 2000-12-05 Parker\n9      5 child2 2005-02-28 Gracie\n\n\nWe again use values_drop_na = TRUE, since the shape of the input forces the creation of explicit missing variables (e.g., for families with only one child).\nFigure 5.7 illustrates the basic idea with a simpler example. When you use \".value\" in names_to, the column names in the input contribute to both values and variable names in the output."
  },
  {
    "objectID": "Self-Practice/Self-Practice_03/Self-Practice_03.html#widening-data",
    "href": "Self-Practice/Self-Practice_03/Self-Practice_03.html#widening-data",
    "title": "Self Practice 3: R for Data Science",
    "section": "",
    "text": "So far we’ve used pivot_longer() to solve the common class of problems where values have ended up in column names. Next we’ll pivot (HA HA) to pivot_wider(), which makes datasets wider by increasing columns and reducing rows and helps when one observation is spread across multiple rows. This seems to arise less commonly in the wild, but it does seem to crop up a lot when dealing with governmental data.\nWe’ll start by looking at cms_patient_experience, a dataset from the Centers of Medicare and Medicaid services that collects data about patient experiences:\n\ncms_patient_experience\n\n# A tibble: 500 × 5\n   org_pac_id org_nm                           measure_cd measure_title prf_rate\n   &lt;chr&gt;      &lt;chr&gt;                            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       63\n 2 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       87\n 3 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       86\n 4 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       57\n 5 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       85\n 6 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       24\n 7 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       59\n 8 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       85\n 9 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       83\n10 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       63\n# ℹ 490 more rows\n\n\n\ncms_patient_experience |&gt; \n  distinct(measure_cd, measure_title)\n\n# A tibble: 6 × 2\n  measure_cd   measure_title                                                    \n  &lt;chr&gt;        &lt;chr&gt;                                                            \n1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and Infor…\n2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate               \n3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider                 \n4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education               \n5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff           \n6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources             \n\n\nNeither of these columns will make particularly great variable names: measure_cd doesn’t hint at the meaning of the variable and measure_title is a long sentence containing spaces. We’ll use measure_cd as the source for our new column names for now, but in a real analysis you might want to create your own variable names that are both short and meaningful.\npivot_wider() has the opposite interface to pivot_longer(): instead of choosing new column names, we need to provide the existing columns that define the values (values_from) and the column name (names_from):\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 500 × 9\n   org_pac_id org_nm           measure_title CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICA… CAHPS for MI…          63          NA          NA\n 2 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          87          NA\n 3 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          86\n 4 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 5 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 6 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 7 0446162697 ASSOCIATION OF … CAHPS for MI…          59          NA          NA\n 8 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          85          NA\n 9 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          83\n10 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          NA\n# ℹ 490 more rows\n# ℹ 3 more variables: CAHPS_GRP_5 &lt;dbl&gt;, CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\n\nThe output doesn’t look quite right; we still seem to have multiple rows for each organization. That’s because, we also need to tell pivot_wider() which column or columns have values that uniquely identify each row; in this case those are the variables starting with \"org\":\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 95 × 8\n   org_pac_id org_nm CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC C…          63          87          86          57          85\n 2 0446162697 ASSOC…          59          85          83          63          88\n 3 0547164295 BEAVE…          49          NA          75          44          73\n 4 0749333730 CAPE …          67          84          85          65          82\n 5 0840104360 ALLIA…          66          87          87          64          87\n 6 0840109864 REX H…          73          87          84          67          91\n 7 0840513552 SCL H…          58          83          76          58          78\n 8 0941545784 GRITM…          46          86          81          54          NA\n 9 1052612785 COMMU…          65          84          80          58          87\n10 1254237779 OUR L…          61          NA          NA          65          NA\n# ℹ 85 more rows\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;\n\n\n\n\nTo understand how pivot_wider() works, let’s again start with a very simple dataset. This time we have two patients with ids A and B, we have three blood pressure measurements on patient A and two on patient B:\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"B\",        \"bp1\",    140,\n  \"B\",        \"bp2\",    115, \n  \"A\",        \"bp2\",    120,\n  \"A\",        \"bp3\",    105\n)\n\nWe’ll take the values from the value column and the names from the measurement column:\n\ndf |&gt; \n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n\n# A tibble: 2 × 4\n  id      bp1   bp2   bp3\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A       100   120   105\n2 B       140   115    NA\n\n\nTo begin the process pivot_wider() needs to first figure out what will go in the rows and columns. The new column names will be the unique values of measurement.\n\ndf |&gt; \n  distinct(measurement) |&gt; \n  pull()\n\n[1] \"bp1\" \"bp2\" \"bp3\""
  },
  {
    "objectID": "Self-Practice/Self-Practice_01/Self-Practice_01.html",
    "href": "Self-Practice/Self-Practice_01/Self-Practice_01.html",
    "title": "Self Practice 1: R for Data Science",
    "section": "",
    "text": "pacman::p_load(tidyverse, patchwork)\n\n\nlibrary(styler)\n\n\nlibrary(palmerpenguins)\nlibrary(ggthemes)\n\n\npalmerpenguins::penguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n?penguins\n\nggplot(penguins, \n       aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method=lm) +\n  labs(title = \"Body mass and flipper length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Flipper length (mm)\", y = \"Body masss (g)\",\n       color = \"Species\", shape = \"Species\") +\n  scale_color_colorblind()\n\n\n\n\n\n\n\n\n\n\n\nHow many rows are in penguins? How many columns?\n\n344 rows, 8 columns\n\nWhat does the bill_depth_mm variable in the penguins data frame describe? Read the help for ?penguins to find out.\n\na number denoting bill depth (millimeters)\n\nMake a scatterplot of bill_depth_mm vs. bill_length_mm. That is, make a scatterplot with bill_depth_mm on the y-axis and bill_length_mm on the x-axis. Describe the relationship between these two variables.\n\n\nggplot(penguins,\n       aes(x = bill_length_mm, y = bill_depth_mm,\n          color = species, shape = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n-   bill depth increases as bill length increases for all threee species. \n\nWhat happens if you make a scatterplot of species vs. bill_depth_mm? What might be a better choice of geom?\n\nBoxplot would be a better choice for a categorical variable vs a continuous variable.\n\n\n\nggplot(penguins,\n       aes(x = species, y = bill_depth_mm)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWhy does the following give an error and how would you fix it?\n\nTo add aes argument for first layer (ggplot)\n\n\n\nggplot(data = penguins, \n       aes(x = bill_depth_mm, y = body_mass_g,\n           color = species, shape = species)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nWhat does the na.rm argument do in geom_point()? What is the default value of the argument? Create a scatterplot where you successfully use this argument set to TRUE.\n\nit means remove the data points that are “N.A”. No more error message\n\n\n\nggplot(data = penguins, \n       aes(x = bill_depth_mm, y = body_mass_g,\n           color = species, shape = species)) + \n  geom_point(na.rm = TRUE) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nAdd the following caption to the plot you made in the previous exercise: “Data come from the palmerpenguins package.” Hint: Take a look at the documentation for labs().\n\n\nggplot(data = penguins, \n       aes(x = bill_depth_mm, y = body_mass_g,\n           color = species, shape = species)) + \n  geom_point(na.rm = TRUE) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Bill depth and Body mass\",\n       subtitle = \"Data come from the palmerpenguins package\")\n\n\n\n\n\n\n\n\n\nRecreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?\n\n\nggplot(penguins,\n       aes(x = flipper_length_mm, y = body_mass_g,\n           color = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nCheck this output.\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = island)\n) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\nWill these two graphs look different? Why/why not?\n\nSame. Mapping is the same.\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )"
  },
  {
    "objectID": "Self-Practice/Self-Practice_01/Self-Practice_01.html#data-visualization",
    "href": "Self-Practice/Self-Practice_01/Self-Practice_01.html#data-visualization",
    "title": "Self Practice 1: R for Data Science",
    "section": "",
    "text": "pacman::p_load(tidyverse, patchwork)\n\n\nlibrary(styler)\n\n\nlibrary(palmerpenguins)\nlibrary(ggthemes)\n\n\npalmerpenguins::penguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n?penguins\n\nggplot(penguins, \n       aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method=lm) +\n  labs(title = \"Body mass and flipper length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Flipper length (mm)\", y = \"Body masss (g)\",\n       color = \"Species\", shape = \"Species\") +\n  scale_color_colorblind()\n\n\n\n\n\n\n\n\n\n\n\nHow many rows are in penguins? How many columns?\n\n344 rows, 8 columns\n\nWhat does the bill_depth_mm variable in the penguins data frame describe? Read the help for ?penguins to find out.\n\na number denoting bill depth (millimeters)\n\nMake a scatterplot of bill_depth_mm vs. bill_length_mm. That is, make a scatterplot with bill_depth_mm on the y-axis and bill_length_mm on the x-axis. Describe the relationship between these two variables.\n\n\nggplot(penguins,\n       aes(x = bill_length_mm, y = bill_depth_mm,\n          color = species, shape = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n-   bill depth increases as bill length increases for all threee species. \n\nWhat happens if you make a scatterplot of species vs. bill_depth_mm? What might be a better choice of geom?\n\nBoxplot would be a better choice for a categorical variable vs a continuous variable.\n\n\n\nggplot(penguins,\n       aes(x = species, y = bill_depth_mm)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWhy does the following give an error and how would you fix it?\n\nTo add aes argument for first layer (ggplot)\n\n\n\nggplot(data = penguins, \n       aes(x = bill_depth_mm, y = body_mass_g,\n           color = species, shape = species)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nWhat does the na.rm argument do in geom_point()? What is the default value of the argument? Create a scatterplot where you successfully use this argument set to TRUE.\n\nit means remove the data points that are “N.A”. No more error message\n\n\n\nggplot(data = penguins, \n       aes(x = bill_depth_mm, y = body_mass_g,\n           color = species, shape = species)) + \n  geom_point(na.rm = TRUE) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nAdd the following caption to the plot you made in the previous exercise: “Data come from the palmerpenguins package.” Hint: Take a look at the documentation for labs().\n\n\nggplot(data = penguins, \n       aes(x = bill_depth_mm, y = body_mass_g,\n           color = species, shape = species)) + \n  geom_point(na.rm = TRUE) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Bill depth and Body mass\",\n       subtitle = \"Data come from the palmerpenguins package\")\n\n\n\n\n\n\n\n\n\nRecreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?\n\n\nggplot(penguins,\n       aes(x = flipper_length_mm, y = body_mass_g,\n           color = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nCheck this output.\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = island)\n) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\nWill these two graphs look different? Why/why not?\n\nSame. Mapping is the same.\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )"
  },
  {
    "objectID": "Self-Practice/Self-Practice_01/Self-Practice_01.html#ggplot2-calls-use-of-pipe",
    "href": "Self-Practice/Self-Practice_01/Self-Practice_01.html#ggplot2-calls-use-of-pipe",
    "title": "Self Practice 1: R for Data Science",
    "section": "1.3 ggplot2 calls (use of pipe |>)",
    "text": "1.3 ggplot2 calls (use of pipe |&gt;)\n\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "Self-Practice/Self-Practice_01/Self-Practice_01.html#visualizing-distributions",
    "href": "Self-Practice/Self-Practice_01/Self-Practice_01.html#visualizing-distributions",
    "title": "Self Practice 1: R for Data Science",
    "section": "1.4 Visualizing distributions",
    "text": "1.4 Visualizing distributions\n\nggplot(penguins, \n       aes(x = fct_infreq(species))) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nfct_infreq() - by number of observations with each level, largest first\nfct_inorder() - by order of first appearance\nfct_inseq() - by numberical value of level\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\n1.4.3 Exercises\n\nMake a bar plot of species of penguins, where you assign species to the y aesthetic. How is this plot different?\n\n\nggplot(penguins, \naes(y = fct_infreq(species))) +\ngeom_bar()\n\n\n\n\n\n\n\n\n\nHow are the following two plots different? Which aesthetic, color or fill, is more useful for changing the color of bars?\n\ncolor is applied as the outline of the bar, while fill is applied to the whole bar.\n\n\n\nggplot(penguins, aes(x = species)) +\n  geom_bar(color = \"red\")\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = species)) +\n  geom_bar(fill = \"red\")\n\n\n\n\n\n\n\n\n\nWhat does the bins argument in geom_histogram() do?\n\nit sets the size/range of the bar of a category.\n\nMake a histogram of the carat variable in the diamonds dataset that is available when you load the tidyverse package. Experiment with different binwidths. What binwidth reveals the most interesting patterns?\n\n\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n\np1 &lt;- ggplot(diamonds,\naes(x = carat)) +\ngeom_histogram(binwidth = 0.01) +\nlabs(title = \"carat with 0.01 binwidth\")\n\n\np2 &lt;- ggplot(diamonds,\naes(x = carat)) +\ngeom_histogram(binwidth = 0.05) +\nlabs(title = \"carat with 0.05 binwidth\")\n\n\np3 &lt;- ggplot(diamonds,\naes(x = carat)) +\ngeom_histogram(binwidth = 0.1) +\nlabs(title = \"carat with 0.1 binwidth\")\n\n\np1/p2/p3"
  },
  {
    "objectID": "Self-Practice/Self-Practice_01/Self-Practice_01.html#visualising-relationship",
    "href": "Self-Practice/Self-Practice_01/Self-Practice_01.html#visualising-relationship",
    "title": "Self Practice 1: R for Data Science",
    "section": "1.5 Visualising relationship",
    "text": "1.5 Visualising relationship\n\n1.5.1 A numerical and a categorical variable\nTo visualize the relationship between a numerical and a categorical variable we can use side-by-side box plots. A boxplot is a type of visual shorthand for measures of position (percentiles) that describe a distribution. It is also useful for identifying potential outliers\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(linewidth = 0.75, alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n1.5.2 Two categorical variables\nWe can use stacked bar plots to visualize the relationship between two categorical variables. For example, the following two stacked bar plots both display the relationship between island and species, or specifically, visualizing the distribution of species within each island.\nThe first plot shows the frequencies of each species of penguins on each island. The plot of frequencies shows that there are equal numbers of Adelies on each island. But we don’t have a good sense of the percentage balance within each island.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nThe second plot, a relative frequency plot created by setting position = “fill” in the geom, is more useful for comparing species distributions across islands since it’s not affected by the unequal numbers of penguins across the islands. Using this plot we can see that Gentoo penguins all live on Biscoe island and make up roughly 75% of the penguins on that island, Chinstrap all live on Dream island and make up roughly 50% of the penguins on that island, and Adelie live on all three islands and make up all of the penguins on Torgersen.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"Percentage of Species on same Island\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse position = \"fill\" argument in geom_ for a relative frequency plot.\n\n\n\n\n1.5.3 Two numerical variables\nSo far you’ve learned about scatterplots (created with geom_point()) and smooth curves (created with geom_smooth()) for visualizing the relationship between two numerical variables. A scatterplot is probably the most commonly used plot for visualizing the relationship between two numerical variables.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = island))\n\n\n\n\n\n\n\n\n\n\n1.5.4 Three or more variables\nAs we saw in Section 1.2.4, we can incorporate more variables into a plot by mapping them to additional aesthetics. For example, in the following scatterplot the colors of points represent species and the shapes of points represent islands.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = island))\n\n\n\n\n\n\n\n\nHowever adding too many aesthetic mappings to a plot makes it cluttered and difficult to make sense of. Another way, which is particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.\nTo facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() is a formula, which you create with ~ followed by a variable name. The variable that you pass to facet_wrap() should be categorical.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\n\n\n\n\n\n\n\n\n1.5.5 Exercises\n\nThe mpg data frame that is bundled with the ggplot2 package contains 234 observations collected by the US Environmental Protection Agency on 38 car models. Which variables in mpg are categorical? Which variables are numerical? (Hint: Type ?mpg to read the documentation for the dataset.) How can you see this information when you run mpg?\n\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\nMake a scatterplot of hwy vs. displ using the mpg data frame. Next, map a third, numerical variable to color, then size, then both color and size, then shape. How do these aesthetics behave differently for categorical vs. numerical variables?\n\n\nggplot(mpg,aes(x = hwy, y = displ)) +\ngeom_point()\n\n\n\n\n\n\n\n\n\nggplot(mpg,aes(x = hwy, y = displ, color = cyl)) +\ngeom_point()\n\n\n\n\n\n\n\n\n\nggplot(mpg,\n      aes(x = hwy, y = displ, \n          color = cyl, size = cty,\n          )) +\ngeom_point() \n\n\n\n\n\n\n\n\n\nggplot(mpg,\n      aes(x = hwy, y = displ, \n          color = cyl, size = cty, shape = trans,\n          )) +\ngeom_point() \n\n\n\n\n\n\n\n\n\nIn the scatterplot of hwy vs. displ, what happens if you map a third variable to linewidth?\n\n\nggplot(mpg,aes(x = hwy, y = displ, size = cyl)) +\ngeom_point()\n\n\n\n\n\n\n\n\n\nWhat happens if you map the same variable to multiple aesthetics?\n\nToo confusing.\n\nMake a scatterplot of bill_depth_mm vs. bill_length_mm and color the points by species. What does adding coloring by species reveal about the relationship between these two variables? What about faceting by species?\n\n\nggplot(penguins, aes(x = bill_depth_mm, \n                    y = bill_length_mm,\n                    color = species)) +\n  geom_point() +\n  facet_wrap(~species)\n\n\n\n\n\n\n\n\nWhy does the following yield two separate legends? How would you fix it to combine the two legends?\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm, y = bill_depth_mm, \n    color = species, shape = species\n  )\n) +\n  geom_point() +\n  labs(color = \"Species\")\n\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm, y = bill_depth_mm, \n    color = species, shape = species)) +\n  geom_point() +\n  labs()\n\n\n\n\n\n\n\n\n\nggplot(penguins)\n\n\n\n\n\n\n\n\n\nCreate the two following stacked bar plots. Which question can you answer with the first one? Which question can you answer with the second one?\n\nCan answer, what species are found in each island.\n\n\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n-   second plot can answer, what percentage of each species is found in each island?\n\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex05/In-class_Ex05.html#getting-started",
    "href": "In-class Exercise/In-class_Ex05/In-class_Ex05.html#getting-started",
    "title": "In-class Ex 5: Visualising Text",
    "section": "1.0 Getting Started",
    "text": "1.0 Getting Started\n\npacman::p_load(tidyverse,readtext,\n               quanteda,tidytext,\n               jsonlite,igraph)\n\n\nRead about quanteda\n\n\n1.1 Loading the all the articles into a single file\nUsing readtext() to convert many articles into a data table, consisting of article name in first column and article:\n\ntext_data &lt;- readtext(paste0(\"data/articles\",\n                             \"/*\"))\n\n\nusenet_words &lt;- text_data %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n\n\nusenet_words |&gt; \n  count (word, sort = TRUE)\n\nreadtext object consisting of 3260 documents and 0 docvars.\n# A data frame: 3,260 × 3\n  word             n text     \n  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ℹ 3,254 more rows\n\n\n\ntext_data_splitted &lt;- text_data |&gt; \n  separate_wider_delim(\"doc_id\",\n                       delim = \"__0__\",\n                       names = c(\"X\",\"Y\"),\n                       too_few = \"align_end\")"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex05/In-class_Ex05.html#using-jsonlite-package",
    "href": "In-class Exercise/In-class_Ex05/In-class_Ex05.html#using-jsonlite-package",
    "title": "In-class Ex 5: Visualising Text",
    "section": "1.2 Using jsonlite package",
    "text": "1.2 Using jsonlite package\n\neasy to glean the insight from the data through looking at the details of the data. - see time series difference\n\n\nmc1_data &lt;- fromJSON(\"data/mc1.json\")\n\n\nmc2_data &lt;- fromJSON(\"data/mc2.json\")\n\nFor creating graph model: - igraph"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class Exercise/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Ex02 - Visualising Distribution",
    "section": "",
    "text": "This In-class Exercise is a walk-through of Chapter 9 - Visualising Distribution in R4VA. The learning outcome of this chapter is learning the following two distribution by using ggplot and its extension:\n\nRidgeline plot\nRaincloud plot"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#learning-outcome",
    "href": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#learning-outcome",
    "title": "In-class Ex02 - Visualising Distribution",
    "section": "",
    "text": "This In-class Exercise is a walk-through of Chapter 9 - Visualising Distribution in R4VA. The learning outcome of this chapter is learning the following two distribution by using ggplot and its extension:\n\nRidgeline plot\nRaincloud plot"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#getting-started",
    "href": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#getting-started",
    "title": "In-class Ex02 - Visualising Distribution",
    "section": "2.0 Getting Started",
    "text": "2.0 Getting Started\n\n2.1 Installing and loading the packages\nFor the purpose of this exercise, the following R packages will be used, they are:\n\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\nggdist: a ggplot2 extension specially design for visualising distribution and uncertainty\ntidyverse: a family of R packages to meet the modern data science and visual communication needs\nggthemes: a ggplot extension that provides the user additional themes, scales, and geoms for the ggplots package\ncolorspace: an R package provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\n\nThe code chunk below will be used load these R packages into RStudio environment.\n\npacman::p_load(tidyverse,ggdist,ggthemes,\n               colorspace,ggridges)\n\n\n\n2.2 Data Import\nFor the purpose of this exercise, Exam_data.csv will be used.\nIn the code chunk below, read_csv() of readr package is used to import Exam_data.csv into R and saved it into a tibble data.frame.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#visualising-distribution-with-ridgeline-plot",
    "href": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#visualising-distribution-with-ridgeline-plot",
    "title": "In-class Ex02 - Visualising Distribution",
    "section": "3.0 Visualising Distribution with Ridgeline Plot",
    "text": "3.0 Visualising Distribution with Ridgeline Plot\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nFigure below is a ridgelines plot showing the distribution of English score by class.\n\n\n\n\n\n\n\n\n\nNote - Ridgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\n\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.\n\n\n3.1 Plotting ridgeline graph: ggridges method\nThere are several ways to plot ridgeline plot with R. In this section, you will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = MATHS,\n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 2,\n    fill = lighten(\"#CC99FF\", 0.5),\n    color = \"black\"\n  ) +\n  scale_x_continuous(\n    name = \"Maths grades\",\n    expand = c(0,0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(3,3))) +\n  theme_ridges()\n\n\n\n\n\n\n3.2 Varying fill colors along the x axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = MATHS,\n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_continuous(name = \"Scores\") +\n  scale_x_continuous(\n    name = \"Maths grades\",\n    expand = c(0,0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n3.3 Mapping the probabilities directly onto colour\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = MATHS, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\nNote It is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n3.4 Ridgeline plots with quantile lines\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nThe Code\n\n\n\nggplot(exam,\n       aes(x = MATHS, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 10,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 5% and 95% tails to colour the ridgeline plot as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = MATHS, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.05, 0.95)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF00FFA0\", \"#FFE4E1A0\", \"#0000ffA0\"),\n    labels = c(\"(0, 0.05]\", \"(0.05, 0.95]\", \"(0.95, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#visualising-distribution-with-raincloud-plot",
    "href": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#visualising-distribution-with-raincloud-plot",
    "title": "In-class Ex02 - Visualising Distribution",
    "section": "4.0 Visualising Distribution with Raincloud Plot",
    "text": "4.0 Visualising Distribution with Raincloud Plot\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of Maths score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n4.1 Plotting a Half Eye graph\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualisation, which is contains a half-density and a slab-interval.\n\nThe PlotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = MATHS)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n4.2 Adding the boxplot with geom_boxplot()\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = MATHS)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n4.3 Adding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = MATHS)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n4.4 Finishing Touch\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_wsj() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = MATHS)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2) +\n  coord_flip() +\n  theme_wsj()\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = MATHS)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2) +\n  coord_flip() +\n  theme_wsj()"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#archival-of-in-class-ex_02-done-in-class",
    "href": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#archival-of-in-class-ex_02-done-in-class",
    "title": "In-class Ex02 - Visualising Distribution",
    "section": "5.0 Archival of In-class Ex_02 done in class",
    "text": "5.0 Archival of In-class Ex_02 done in class"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#histogram",
    "href": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#histogram",
    "title": "In-class Ex02 - Visualising Distribution",
    "section": "Histogram",
    "text": "Histogram\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\nggplot(exam,\n       aes(x = ENGLISH)) +\n  geom_density(\n    color = \"#1696d2\",\n    adjust = .65,\n    alpha = .6\n  )"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#the-alternative-design",
    "href": "In-class Exercise/In-class_Ex02/In-class_Ex02.html#the-alternative-design",
    "title": "In-class Ex02 - Visualising Distribution",
    "section": "The alternative design",
    "text": "The alternative design\n\nmedian_eng &lt;- median(exam$ENGLISH)\nmean_eng &lt;- mean(exam$ENGLISH)\nstd_eng &lt;- sd(exam$ENGLISH)\n\nggplot(exam,\n       aes(x= ENGLISH)) +\n  geom_density(\n    color = \"#1696d2\",\n    adjust = .65,\n    alpha = .6) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean_eng,\n                sd = std_eng),\n    col = \"grey30\",\n    linewidth = .8) +\n  geom_vline(\n    aes(xintercept = mean_eng),\n    color = \"#4d5887\",\n    linewidth = .6,\n    linetype = \"dashed\") +\n  annotate(geom = \"text\",\n           x = mean_eng - 8,\n           y = 0.04,\n           label = paste0(\"Mean ENGLISH: \",\n                          round((mean_eng), 2)),\n           color = \"#4d5887\") +\n  geom_vline(\n    aes(xintercept = median_eng)\n  )\n\n\n\n\n\n\n\n\nReference\n\nT.S Kam, R for Visual Analytics Chapter 9"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Ex 6: 27 Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, you will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "href": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "title": "Hands-on Ex 6: 27 Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, you will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "href": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "title": "Hands-on Ex 6: 27 Modelling, Visualising and Analysing Network Data with R",
    "section": "27.2 Getting Started",
    "text": "27.2 Getting Started\n\n27.2.1 Installing and launching R packages\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#the-data",
    "href": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#the-data",
    "title": "Hands-on Ex 6: 27 Modelling, Visualising and Analysing Network Data with R",
    "section": "27.3 The Data",
    "text": "27.3 The Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n27.3.1 The edges data\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\n27.3.2 The nodes data\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\n27.3.3 Importing network data from files\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n \n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n27.3.4 Reviewing the imported data\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n27.3.5 Wrangling time\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nTip\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the day spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n27.3.6 Reviewing the revised date fields\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n27.3.7 Wrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nTip\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n27.3.8 Reviewing the revised edges file\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Ex 6: 27 Modelling, Visualising and Analysing Network Data with R",
    "section": "27.4 Creating network objects using tidygraph",
    "text": "27.4 Creating network objects using tidygraph\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n27.4.1 The tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n27.4.2 The dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n27.4.3 Using tbl_graph() to build tidygraph data model.\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n27.4.5 Reviewing the output tidygraph’s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 1372 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n27.4.6 Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Ex 6: 27 Modelling, Visualising and Analysing Network Data with R",
    "section": "27.5 Plotting Static Network Graphs with ggraph package",
    "text": "27.5 Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n27.5.1 Plotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\n27.5.2 Changing the default network graph theme\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n27.5.3 Changing the coloring of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\n27.5.4 Working with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\n27.5.5 Fruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nlayout argument is used to define the layout to be used.\n\n\n\n\n\n27.5.6 Modifying network nodes\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\ngeom_node_point is equivalent in functionality to geom_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuk above colour and size are used.\n\n\n\n\n\n27.5.7 Modifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#creating-facet-graphs",
    "href": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#creating-facet-graphs",
    "title": "Hands-on Ex 6: 27 Modelling, Visualising and Analysing Network Data with R",
    "section": "27.6 Creating facet graphs",
    "text": "27.6 Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in all panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n27.6.1 Working with facet_edges()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n27.6.2 Working with facet_edges()\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\nA framed facet graph\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom',\n        legend.justification = 'center',\n        legend.box = 'horizontal',\n        legend.key.size = unit(0.3, 'cm'), \n        legend.text = element_text(size = 7), \n        legend.title = element_text(size = 7))\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) \n\n\n\n\n\n\n\n\n\n\n27.6.4 Working with facet_nodes()\nIn the code chunk below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom',\n        legend.justification = 'center',\n        legend.box = 'horizontal',\n        legend.key.size = unit(0.3, 'cm'), \n        legend.text = element_text(size = 7), \n        legend.title = element_text(size = 7))"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#network-metrics-analysis",
    "href": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#network-metrics-analysis",
    "title": "Hands-on Ex 6: 27 Modelling, Visualising and Analysing Network Data with R",
    "section": "27.7 Network Metrics Analysis",
    "text": "27.7 Network Metrics Analysis\n\n27.7.1 Computing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph() +\n  theme(legend.position = 'right',\n        legend.justification = 'center',\n        legend.box = 'vertical',\n        legend.key.size = unit(0.3, 'cm'), \n        legend.text = element_text(size = 7), \n        legend.title = element_text(size = 7))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\n\n27.7.2 Visualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph() +\n  theme(legend.position = 'right',\n        legend.justification = 'center',\n        legend.box = 'vertical',\n        legend.key.size = unit(0.3, 'cm'), \n        legend.text = element_text(size = 7), \n        legend.title = element_text(size = 7))\n\n\n\n\n\n\n\n\n\n\n27.7.3 Visualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on Exercise/Hands-on_Ex06/Hands-on_Ex06.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Ex 6: 27 Modelling, Visualising and Analysing Network Data with R",
    "section": "27.8 Building Interactive Network Graph with visNetwork",
    "text": "27.8 Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n27.8.1 Data preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n27.8.2 Plotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n27.8.3 Working with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n27.8.4 Working with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n27.8.5 Working with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\",\n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\n27.8.6 Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "",
    "text": "This hands-on exercise consists of Chapter 10 to 12 of R4VA."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#lesson-outline",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#lesson-outline",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "Lesson Outline",
    "text": "Lesson Outline\n\nVisual Analytics for Knowledge Discovery\nVisual Analytics Approach for Statistical Testing\nVisual Analytics for Building Better Models\nVisualising Uncertainty\n\nWhy Visualising Uncertainty?\nBasic Statistical Concepts Related to Uncertainty (Chapter 10 of R4VA)\nUnivariate Graphical Methods for Visualising Uncertainty\n\nError bars (Chapter 11 of R4VA)\nConfidence strips (Chapter 11 of R4VA)\nRidge plot (Chpater 9 of R4VA)\n\nBivariate Graphical Methods for Visualising Uncertainty\n\nFunnel plot (Chapter 12 of R4VA)\n\n\nVariation and Its Discontents"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "10.1 Learning Outcome",
    "text": "10.1 Learning Outcome\nIn this hands-on exercise, you will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "10.2 Visual Statistical Analysis with ggstatsplot",
    "text": "10.2 Visual Statistical Analysis with ggstatsplot\nggstatsplot  is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "10.3 Getting Started",
    "text": "10.3 Getting Started\n\n10.3.1 Installing and launching R packages\nIn this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse, FunnelPlotR,rstantools)\n\n\n\n10.3.2 Importing data\nFor the purpose of this exercise, Exam_data.csv will be used.\nIn the code chunk below, read_csv() of readr package is used to import Exam_data.csv into R and saved it into a tibble data.frame.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n10.3.3 One-sample test: gghistostats() method\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary.\n\n\n10.3.4 Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n10.3.5 How to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n10.3.6 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n10.3.7 Oneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nlibrary(rstantools)\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n10.3.7.1 ggbetweenstats - Summary of tests\n\n\n\n\n\n\n10.3.8 Significant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n10.3.9 Significant Test of Association (Depedence) : ggbarstats() methods\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association.\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visualising-models",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visualising-models",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "10.4 Visualising Models",
    "text": "10.4 Visualising Models\nIn this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-1",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-1",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "10.5 Getting Started",
    "text": "10.5 Getting Started"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "10.6 Installing and loading the required libraries",
    "text": "10.6 Installing and loading the required libraries\n\npacman:::p_load(readxl, performance, parameters,see)\n\n\n10.6.1 Importing Excel file: readxl methods\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\") #import only worksheet named \"data\"\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\n10.6.2 Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n10.6.3 Model Diagnostic: checking for multicolinearity:\nIn the code chunk, check_collinearity() of performance package.\n\nt &lt;- check_collinearity(model)\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\n10.6.4 Model Diagnostic: checking normality assumption\nIn the code chunk, check_normality() of performance package.\nThe year is is excluded\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n &lt;- check_normality(model1)\n\n\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\n10.6.5 Model Diagnostic: Check model for homogeneity of variances\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n10.6.6 Model Diagnostic: Complete check\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\n10.6.7 Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\nt1 &lt;- parameters(model1)\n\n\n\n10.6.8 Visualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome-1",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome-1",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "11.1 Learning Outcome",
    "text": "11.1 Learning Outcome\nVisualising uncertainty is relatively new in statistical graphics. In this chapter, you will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this chapter you will be able:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-2",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-2",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "11.2 Getting Started",
    "text": "11.2 Getting Started\n\n11.2.1 Installing and loading the packages\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n11.2.2 Data import\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "11.3 Visualizing the uncertainty of point estimates: ggplot2 methods",
    "text": "11.3 Visualizing the uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n\n\n\n\n\nImportant\n\n\n\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\n\nIn this section, you will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\nThings to learn\n\n\n\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the mathematical explanation, please refer to Slide 20 of Lesson 4.\n\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nThe CodeThe Table\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\n\n\n11.3.1 Plotting standard error bars of point estimates\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.75, \n    linewidth=2) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 2,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\nThings to learn\n\n\n\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”.\n\n\n\n\n\n11.3.2 Plotting confidence interval of point estimates\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n11.3.3 Visualizing the uncertainty of point estimates with interactive error bars\nIn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-ggdist-package",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-ggdist-package",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "11.4 Visualising Uncertainty: ggdist package",
    "text": "11.4 Visualising Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\n11.4.1 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\n?stat_pointinterval()\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n11.4.2 Visualizing the uncertainty of point estimates: ggdist methods\nMakeover the plot on previous slide by showing 95% and 99% confidence intervals is shown below:\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(.width = c(0.95,0.99),\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n11.4.3 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "11.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "11.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "12.1 Overview",
    "text": "12.1 Overview\nFunnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-launching-r-packages-1",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-launching-r-packages-1",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "12.2 Installing and Launching R Packages",
    "text": "12.2 Installing and Launching R Packages\nIn this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(readr,tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#importing-data-1",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#importing-data-1",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "12.3 Importing Data",
    "text": "12.3 Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr-methods",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr-methods",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "12.4 FunnelPlotR methods",
    "text": "12.4 FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n12.4.1 FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n\nTip\n\n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n\n\n12.4.2 FunnelPlotR methods: Makeover 1\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  x_range = c(0, 6500),  #&lt;&lt;\n  y_range = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n\nTip\n\n\n\nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n\n\n12.4.3 FunnelPlotR methods: Makeover 2\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  x_range = c(0, 6500),  \n  y_range = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles\n\n\n\n12.5.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n12.5.2 Calculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n12.5.3 Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\n12.5.4 Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#references",
    "href": "Hands-on Exercise/Hands-on_Ex04/Hands-on_Ex04.html#references",
    "title": "Hands-on Ex 4: Fundamentals of Visual Analytics",
    "section": "12.6 References",
    "text": "12.6 References\n\nfunnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "This chapter / Hands-on exercise is an introduction to several ggplot2 extension. Key idea is to create more elegant and effective statistical graphics, pleasing to the eyes of the reader, and reducing the cognitive load of the reader.\nKey learning:\n\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package\ncreate professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages\nplot composite figure by combining ggplot2 graphs by using patchwork package."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#overview",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#overview",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "This chapter / Hands-on exercise is an introduction to several ggplot2 extension. Key idea is to create more elegant and effective statistical graphics, pleasing to the eyes of the reader, and reducing the cognitive load of the reader.\nKey learning:\n\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package\ncreate professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages\nplot composite figure by combining ggplot2 graphs by using patchwork package."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "2 Getting started",
    "text": "2 Getting started\n\nInstalling and loading the required librariesImporting dataglimpse() the Data\n\n\nIn this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\nCode chunk below will be used to check if these packages have been installed and also will load them onto your working R environment.\n\npacman::p_load(ggrepel,patchwork,\n               ggthemes,hrbrthemes,\n               tidyverse)\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package. readr is one of the tidyverse package.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nglimpse(exam_data)\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\nThere are a total of seven attributes in the exam_data tibble data frame. Four of them are categorical data type and the other three are in continuous data type.\n\nCategorical attributes: ID, CLASS, GENDER and RACE.\nContinuous attributes: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "3 Beyond ggplot2 Annotation: ggrepel",
    "text": "3 Beyond ggplot2 Annotation: ggrepel\nAnnotation for large number of data points is potentially challenging, as the annotations may overwhelm the plot. In this example, geom_label() is used to show the labels on all the data points. The resulting plot below shows the annotation covering much of the data points.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = 0.5, \n             vjust = 0.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\nggrepel is an extension of ggplot2 package which provides geoms for ggplot2 to repel overlapping text as in our example above.\nWe simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data,\n       aes(x= MATHS,\n       y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm,\n              linewidth = 0.5) +\n  geom_label_repel(aes(label = ID),\n                   hjust = 0.5,\n                   vjust = 0.5) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "4 Beyond ggplot2 Themes",
    "text": "4 Beyond ggplot2 Themes\nThere are eight built-in themes in ggplot2. You can browse through the plots of all the themes below. Visit this link to learn more about ggplot2 Themes.\n\n1. theme_gray() - Default2. theme_bw()3. theme_classic()4. theme_dark()5. theme_light()6. theme_linedraw()7. theme_minimal()8. theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoints to note\n\nNote 1\nNote 2"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#working-with-ggthemes-package",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#working-with-ggthemes-package",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "4.1 Working with ggthemes package",
    "text": "4.1 Working with ggthemes package\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nIn the example below, wsj theme is used.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data,\n       aes(x = MATHS,\n       y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm,\n              linewidth = 0.5) +\n  ggtitle(\"English versus Maths scores\") +\n  geom_label_repel(aes(label = ID), \n                   max.overlaps = 5) +  \n  theme_wsj()\n\n\n\n\n\n\n\n\n\n\nAdditional tip on ggthemes\n\n\n\n\nVisit this link for the overview of the applications of each theme."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#working-with-hrbrthemes-package",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#working-with-hrbrthemes-package",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "4.2 Working with hrbrthemes package",
    "text": "4.2 Working with hrbrthemes package\nThe hrbrthemes is a collection of custom themes and utility functions for creating visually appealing and consistent plots using ggplot2. This package is developed by Bob Rudis (hrbrmstr) and offers an assortment of themes and utilities that can enhance the aesthetics and readability of your ggplot2 visualisations.\n\nOriginalWith FT Theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "5 Beyond Single Graph",
    "text": "5 Beyond Single Graph\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose figure with multiple graphs. In this section, you will learn how to create composite plot by combining multiple graphs. First, let us create three statistical graphics by using the code chunk below.\n\nThe plotThe code\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\nNext, we create two more graphs.\n\nThe plotThe code\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\nThe plotThe code"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#creating-composite-graphics-patchwork-methods",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#creating-composite-graphics-patchwork-methods",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "5.1 Creating Composite Graphics: patchwork methods",
    "text": "5.1 Creating Composite Graphics: patchwork methods\nThere are several ggplot2 extension’s functions support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package. In this section, I am going to shared with you an ggplot2 extension called patchwork which is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily.\nGeneral syntax that combines: - Two-Column Layout using the Plus Sign +. - Parenthesis () to create a subplot group. - Two-Row Layout using the Division Sign /\n\n5.2 Combining the ggplot2 graphs\n\nThe plot using +The code using +The plot using all 3 syntaxThe code using all 3 syntax\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(p1 + p2) / p3\n\n\n\n\nRefer to Plot Assembly to learn more about nesting and wrapping of plots.\n\n\n5.3 Creating a composite figure with tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(p1 + p2) / p3 + \n  plot_annotation(tag_levels = 'I')"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#creating-figure-with-insert",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#creating-figure-with-insert",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "5.4 Creating figure with insert",
    "text": "5.4 Creating figure with insert\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-by-using-patchwork-and-ggtheme",
    "href": "Hands-on Exercise/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-by-using-patchwork-and-ggtheme",
    "title": "Hands-on Ex 2: Beyond ggplot2 Fundamentals",
    "section": "Creating a composite figure by using patchwork and ggtheme",
    "text": "Creating a composite figure by using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_ft_rc() of ggthemes package discussed earlier.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 + p2) / p3\npatchwork & theme_ft_rc() +\n  theme(plot.title = element_text(size=10))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site",
    "section": "",
    "text": "This site is created as part my course work in Visual Analytics taught by Professor Kam TS. Through this module, I hope to improve my data visualisation skill.\nSkills of a Data Scientists and my progress thus far:"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on Exercise/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Ex 1",
    "section": "",
    "text": "# load tidyverse from pacman\npacman::p_load(tidyverse)\n\n# load exam data using readr, part of tidyverse package\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n# R base Histogram\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\n# ggplot histogram\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\n# If the dataset is not already a data.frame, it will be converted to one by fortify().\nggplot(data=exam_data, \n       aes(x= MATHS))\n\n\n\n\n\n\n\n# Bar Chart\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n# Dotplot / scale_y_continuous to turn off y-axis\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)  \n\n\n\n\n\n\n\n# Histogram, changing geom() \nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill='light blue') \n\n\n\n\n\n\n\n# Histogram, changing aes()\n# This approach can be used to colour, fill and alpha of the geometric.\nggplot(data=exam_data, \n       aes(x = MATHS,\n           fill = GENDER)) +\n  geom_histogram(bins = 20,\n                 color = \"grey30\") \n\n\n\n\n\n\n\n# Kernel Density Estimate (KDE), changing aes()\nggplot(data=exam_data, \n       aes(x = MATHS,\n           color = GENDER)) +\n  geom_density()   \n\n\n\n\n\n\n\n# Boxplot, notches to determine if median differs\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot(notch=T) \n\n\n\n\n\n\n\n# Violin plot - comparing multiple data distributions\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n\n# geom_point - scatterplot\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()            \n\n\n\n\n\n\n\n# Combining geom objects\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)        \n\n\n\n\n\n\n\n# Using stat_summary(), use fun=\"mean\", fun.y has deprecated\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun=\"mean\",         \n               colour =\"red\",        \n               size=4)  \n\n\n\n\n\n\n\n# Using stat in geom_point(), use fun instead of fun.y\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun=\"mean\",           \n             colour =\"red\",          \n             size=4) \n\n\n\n\n\n\n\n# geom_smooth() for best fit curve on scatterplot, default method = loess\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(linewidth=0.5)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "",
    "text": "In this hands on exercise 3, we will be learning Chapter 3 and 4 of R for Visual Analytics. For Part I, we will focus on creating interactive data visualisation by using functions provided by ggiraph and plotly packages. For Part II, we will focus on learning how to create animated data visualisation by using gganimate and plotly r packages. At the same time, we will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#overview",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#overview",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "",
    "text": "In this hands on exercise 3, we will be learning Chapter 3 and 4 of R for Visual Analytics. For Part I, we will focus on creating interactive data visualisation by using functions provided by ggiraph and plotly packages. For Part II, we will focus on learning how to create animated data visualisation by using gganimate and plotly r packages. At the same time, we will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "2.1 Getting Started",
    "text": "2.1 Getting Started\nFirst, write a code chunk to check, install and launch the following R packages:\n\nDT: provides an R interface to the JavaScript library DataTables that create interactive table on html page.\nggiraph: for making ‘ggplot’ graphics interactive.\npatchwork: for combining multiple ggplot2 graphs into one figure.\nplotly: R library for plotting interactive statistical graphs.\ntidyverse: a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\nThe code chunk below will be used to accomplish the task.\n\npacman::p_load(DT, ggiraph, \n               patchwork, plotly, \n               tidyverse)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#importing-data",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#importing-data",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "2.2 Importing Data",
    "text": "2.2 Importing Data\nIn this section, Exam_data.csv provided will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "2.3 Interactive Data Visualisation - ggiraph methods",
    "text": "2.3 Interactive Data Visualisation - ggiraph methods\n\n2.3.1 Tooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np &lt;- ggplot(exam_data,\n            aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,\n                     breaks = NULL)\n\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\nNotice that two steps are involved. First, an interactive version of ggplot2 geom i.e. geom_dotplot_interactive() will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactivity",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactivity",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "2.4 Interactivity",
    "text": "2.4 Interactivity\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\n\n\n\n\n\n\n2.4.1 Displaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactivity-1",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactivity-1",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "2.5 Interactivity",
    "text": "2.5 Interactivity\nBy hovering the mouse pointer on an data point of interest, the student’s ID, Class, and Gender will be displayed.\n\n\n\n\n\n\n\n2.5.1 Customising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)\n\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\n\n\n\n\n\n\n\n\n2.5.2 Displaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n2.5.2 Hover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = GENDER),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)  \n\nInteractivity: Elements associated with a data_id (i.e Gender) will be highlighted upon mouse over.\n\n\n\n\n\n\nNote that the default value of the hover css is hover_css = “fill:orange;”.\n\n\n2.5.4 Styling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.5;\") \n  )                                        \n)    \n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\nNote Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\n2.5.5 Combining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np &lt;- ggplot(exam_data,\n            aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = CLASS,\n        data_id = CLASS),\n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,\n                     breaks = NULL)\n\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618,\n  options = list(\n    opts_hover(css = \"fill: #202020;\"),\n    opts_hover_inv(css = \"opacity:0.2;\")\n  )\n)\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\n\n\n\n\n\n\n2.5.6 Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                 \n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\n\n\n\n\n\n\n\n\n2.5.7 Coordinated Multiple Views with ggiraph\nCoordinated multiple views methods has been implemented in the data visualisation below.\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\n*patchwork* function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\n\n\n\n\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---plotly-methods",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "2.6 Interactive Data Visualisation - plotly methods!",
    "text": "2.6 Interactive Data Visualisation - plotly methods!\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n2.6.1 Creating an interactive scatter plot: plot_ly() method\nHere’s an example a basic interactive plot created by using plot_ly().\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n2.6.2 Working with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\n::: panel-tabset ## The Plot"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#the-code-1",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#the-code-1",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "The Code",
    "text": "The Code\n\nplot_ly(exam_data, \n        x = ~MATHS, \n        y = ~ENGLISH,\n        color = ~RACE)\n\n\n2.6.3 Creating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH,\n                color = RACE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n\n\n\n2.6.4 Coordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\n\n\n\n\n\n\n\n\n\nThing to learn from the code:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\nVisit this link to learn more about crosstalk"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "2.7 Interactive Data Visualisation - crosstalk methods!",
    "text": "2.7 Interactive Data Visualisation - crosstalk methods!\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n2.7.1 Interactive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n2.7.2 Linked brushing: crosstalk method\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5) \n\nThings to learn from the code chunk:\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#overview-1",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#overview-1",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nWhen telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, you will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, you will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#getting-started-1",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#getting-started-1",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "3.2 Getting Started",
    "text": "3.2 Getting Started\n\n3.2.1 Loading the R Packages\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n3.2.2 Importing data\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\nThings to learn from the code chunk above\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_at() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "3.3 Animated Data Visualisation: gganimate methods",
    "text": "3.3 Animated Data Visualisation: gganimate methods\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n3.3.1 Building a static population bubble plot\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young,                        \n                      size = Population,                        \n                      colour = Country)) +   \n  geom_point(alpha = 0.7,               \n             show.legend = FALSE) +   \n  scale_colour_manual(values = country_colors) +   \n  scale_size(range = c(2, 12)) +   \n  labs(title = 'Year: {frame_time}',        \n       x = '% Aged',         \n       y = '% Young')\n\n\n\n\n\n\n\n\n\n\n3.3.2 Building the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')          \n\n\n\n\n\n\n\n\nThe animated bubble chart"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-plotly",
    "href": "Hands-on Exercise/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-plotly",
    "title": "Hands-on Ex 03: Interactivity in Visual Analytics: Principles and Methods",
    "section": "3.4 Animated Data Visualisation: plotly",
    "text": "3.4 Animated Data Visualisation: plotly\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n3.4.1 Building an animated bubble plot: ggplotly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using ggplotly() method.\n\nThe Plot\n\n\n\n\n\n\n\n\nThe animated bubble plot above includes a play/pause button and a slider component for controlling the animation ## The Code\n\nanimate &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') \n\nggplotly(animate)\n\nThings to learn from the code chunk above\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called animate.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\n\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position='none') should be used as shown in the plot and code chunk belo\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\nanimate &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') +\n  theme(legend.position='none')\n\nggplotly(animate)\n\n\n\n\n\n\n3.4.2 Building an animated bubble plot: plot_ly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nThe Bubble PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\nbubble &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbubble"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Ex 5: 29 Visualising and Analysing Text Data with R: tidytext methods",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to visualise and analyse text data using R.\nBy the end of this hands-on exercise, you will be able to:\n\nunderstand tidytext framework for processing, analysing and visualising text data,\nwrite function for importing multiple files into R,\ncombine multiple files into a single data frame,\nclean and wrangle text data by using tidyverse approach,\nvisualise words with Word Cloud,\ncompute term frequency–inverse document frequency (TF-IDF) using tidytext method, and\nvisualising texts and terms relationship."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#learning-outcome",
    "href": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#learning-outcome",
    "title": "Hands-on Ex 5: 29 Visualising and Analysing Text Data with R: tidytext methods",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to visualise and analyse text data using R.\nBy the end of this hands-on exercise, you will be able to:\n\nunderstand tidytext framework for processing, analysing and visualising text data,\nwrite function for importing multiple files into R,\ncombine multiple files into a single data frame,\nclean and wrangle text data by using tidyverse approach,\nvisualise words with Word Cloud,\ncompute term frequency–inverse document frequency (TF-IDF) using tidytext method, and\nvisualising texts and terms relationship."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "href": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "title": "Hands-on Ex 5: 29 Visualising and Analysing Text Data with R: tidytext methods",
    "section": "29.2 Getting Started",
    "text": "29.2 Getting Started\n\n29.2.1 Installing and launching R packages\nIn this hands-on exercise, the following R packages for handling, processing, wrangling, analysing and visualising text data will be used:\n\ntidytext, tidyverse (mainly readr, purrr, stringr, ggplot2)\nwidyr,\nwordcloud and ggwordcloud,\ntextplot (required igraph, tidygraph and ggraph, )\nDT,\nlubridate and hms.\n\nThe code chunk:\n\npacman::p_load(tidytext, widyr, wordcloud, DT, ggwordcloud, textplot, lubridate, hms,\ntidyverse, tidygraph, ggraph, igraph)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders",
    "href": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders",
    "title": "Hands-on Ex 5: 29 Visualising and Analysing Text Data with R: tidytext methods",
    "section": "29.3 Importing Multiple Text Files from Multiple Folders",
    "text": "29.3 Importing Multiple Text Files from Multiple Folders\n\n29.3.1 Creating a folder list\n\nnews20 &lt;- \"data/20news/\"\n\n\n\n29.3.2 Define a function to read all files from a folder into a data frame\n\nread_folder &lt;- function(infolder) {\n  tibble(file = dir(infolder, \n                    full.names = TRUE)) %&gt;%\n    mutate(text = map(file, \n                      read_lines)) %&gt;%\n    transmute(id = basename(file), \n              text) %&gt;%\n    unnest(text)\n}"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders-1",
    "href": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders-1",
    "title": "Hands-on Ex 5: 29 Visualising and Analysing Text Data with R: tidytext methods",
    "section": "29.4 Importing Multiple Text Files from Multiple Folders",
    "text": "29.4 Importing Multiple Text Files from Multiple Folders\n\n29.4.1 Reading in all the messages from the 20news folder\n\nraw_text &lt;- tibble(folder = \n                     dir(news20, \n                         full.names = TRUE)) %&gt;%\n  mutate(folder_out = map(folder, \n                          read_folder)) %&gt;%\n  unnest(cols = c(folder_out)) %&gt;%\n  transmute(newsgroup = basename(folder), \n            id, text)\nwrite_rds(raw_text, \"data/rds/news20.rds\")\n\n\n\n\n\n\n\nThings to learn from the code:\n\n\n\n\nread_lines() of readr package is used to read up to n_max lines from a file.\nmap() of purrr package is used to transform their input by applying a function to each element of a list and returning an object of the same length as the input.\nunnest() of dplyr package is used to flatten a list-column of data frames back out into regular columns.\nmutate() of dplyr is used to add new variables and preserves existing ones;\ntransmute() of dplyr is used to add new variables and drops existing ones.\nread_rds() is used to save the extracted and combined data frame as rds file for future use."
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#initial-eda",
    "href": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#initial-eda",
    "title": "Hands-on Ex 5: 29 Visualising and Analysing Text Data with R: tidytext methods",
    "section": "29.5 Initial EDA",
    "text": "29.5 Initial EDA\nFigure below shows the frequency of messages by newsgroup.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews20 &lt;- read_rds(\"data/rds/news20.rds\")\nraw_text &lt;- news20\nraw_text %&gt;%\n  group_by(newsgroup) %&gt;%\n  summarize(messages = n_distinct(id)) %&gt;%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col(fill = \"lightblue\") +\n  labs(y = NULL)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#introducing-tidytext",
    "href": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#introducing-tidytext",
    "title": "Hands-on Ex 5: 29 Visualising and Analysing Text Data with R: tidytext methods",
    "section": "29.6 Introducing tidytext",
    "text": "29.6 Introducing tidytext\n\nUsing tidy data principles in processing, analysing and visualising text data.\nMuch of the infrastructure needed for text mining with tidy data frames already exists in packages like ‘dplyr’, ‘broom’, ‘tidyr’, and ‘ggplot2’.\n\nFigure below shows the workflow using tidytext approach for processing and visualising text data.\n\n\n29.6.1 Removing header and automated email signitures\nNotice that each message has some structure and extra text that we don’t want to include in our analysis. For example, every message has a header, containing field such as “from:” or “in_reply_to:” that describe the message. Some also have automated email signatures, which occur after a line like “–”.\n\ncleaned_text &lt;- raw_text %&gt;%\n  group_by(newsgroup, id) %&gt;%\n  filter(cumsum(text == \"\") &gt; 0,\n         cumsum(str_detect(\n           text, \"^--\")) == 0) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nThings to learn:\n\n\n\n\ncumsum() of base R is used to return a vector whose elements are the cumulative sums of the elements of the argument.\nstr_detect() from stringr is used to detect the presence or absence of a pattern in a string.\n\n\n\n\n\n29.6.2 Removing lines with nested text representing quotes from other users.\nIn this code chunk below, regular expressions are used to remove with nested text representing quotes from other users.\n\ncleaned_text &lt;- cleaned_text %&gt;%\n  filter(str_detect(text, \"^[^&gt;]+[A-Za-z\\\\d]\")\n         | text == \"\",\n         !str_detect(text, \n                     \"writes(:|\\\\.\\\\.\\\\.)$\"),\n         !str_detect(text, \n                     \"^In article &lt;\")\n  )\n\n\n\n\n\n\n\nThings to learn\n\n\n\n\nstr_detect() from stringr is used to detect the presence or absence of a pattern in a string.\nfilter() of dplyr package is used to subset a data frame, retaining all rows that satisfy the specified conditions.\n\n\n\n\n\n29.6.3 Text Data Processing\nIn this code chunk below, unnest_tokens() of tidytext package is used to split the dataset into tokens, while stop_words() is used to remove stop-words.\n\nusenet_words &lt;- cleaned_text %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n\nNow that we’ve removed the headers, signatures, and formatting, we can start exploring common words. For starters, we could find the most common words in the entire dataset, or within particular newsgroups.\n\nusenet_words %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 5,542 × 2\n   word           n\n   &lt;chr&gt;      &lt;int&gt;\n 1 people        57\n 2 time          50\n 3 jesus         47\n 4 god           44\n 5 message       40\n 6 br            27\n 7 bible         23\n 8 drive         23\n 9 homosexual    23\n10 read          22\n# ℹ 5,532 more rows\n\n\nInstead of counting individual word, you can also count words within by newsgroup by using the code chunk below.\n\nwords_by_newsgroup &lt;- usenet_words %&gt;%\n  count(newsgroup, word, sort = TRUE) %&gt;%\n  ungroup()\n\n\n\n29.6.4 Visualising Words in newsgroups\nIn this code chunk below, wordcloud() of wordcloud package is used to plot a static wordcloud.\n\nwordcloud(words_by_newsgroup$word,\n          words_by_newsgroup$n,\n          max.words = 300)\n\n\n\n\n\n\n\n\nA DT table can be used to complement the visual discovery.\n\nset.seed(1234)\n\nwords_by_newsgroup %&gt;%\n  filter(n &gt; 0) %&gt;%\nggplot(aes(label = word,\n           size = n)) +\n  geom_text_wordcloud() +\n  theme_minimal() +\n  facet_wrap(~newsgroup)"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#basic-concept-of-tf-idf",
    "href": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#basic-concept-of-tf-idf",
    "title": "Hands-on Ex 5: 29 Visualising and Analysing Text Data with R: tidytext methods",
    "section": "29.7 Basic Concept of TF-IDF",
    "text": "29.7 Basic Concept of TF-IDF\n\ntf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection of corpus.\n\n\n\n29.7.1 Computing tf-idf within newsgroups\nThe code chunk below uses bind_tf_idf() of tidytext to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.\n\ntf_idf &lt;- words_by_newsgroup %&gt;%\n  bind_tf_idf(word, newsgroup, n) %&gt;%\n  arrange(desc(tf_idf))\n\n\n\n29.7.2 Visualising tf-idf as interactive table\nTable below is an interactive table created by using datatable().\n\nDT::datatable(tf_idf, filter = 'top') %&gt;% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %&gt;%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\n\n\n\n\n\nThings to learn\n\n\n\n\nfilter() argument is used to turn control the filter UI.\nformatRound() is used to customise the values format. The argument digits define the number of decimal places.\nformatStyle() is used to customise the output table.\n\n\n\nTo learn more about customising DT’s table, visit this link.\n\n\n29.7.4 Visualising tf-idf within newsgroups\nFacet bar charts technique is used to visualise the tf-idf values of science related newsgroup.\n\ntf_idf %&gt;%\n  filter(str_detect(newsgroup, \"^sci\\\\.\")) %&gt;%\n  group_by(newsgroup) %&gt;%\n  slice_max(tf_idf, \n            n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, \n                        tf_idf)) %&gt;%\n  ggplot(aes(tf_idf, \n             word, \n             fill = newsgroup)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ newsgroup, \n             scales = \"free\") +\n  labs(x = \"tf-idf\", \n       y = NULL)\n\n\n\n\n\n\n\n\n\n\n29.7.5 Counting and correlating pairs of words with the widyr package\n\nTo count the number of times that two words appear within the same document, or to see how correlated they are.\nMost operations for finding pairwise counts or correlations need to turn the data into a wide matrix first.\nwidyr package first ‘casts’ a tidy dataset into a wide matrix, performs an operation such as a correlation on it, then re-tidies the result.\n\n\nIn this code chunk below, pairwise_cor() of widyr package is used to compute the correlation between newsgroup based on the common words found.\n\nnewsgroup_cors &lt;- words_by_newsgroup %&gt;%\n  pairwise_cor(newsgroup, \n               word, \n               n, \n               sort = TRUE)\n\n\n\n29.7.6 Visualising correlation as a network\nNow, we can visualise the relationship between newgroups in network graph as shown below.\n\nset.seed(2017)\n\nnewsgroup_cors %&gt;%\n  filter(correlation &gt; .025) %&gt;%\n  graph_from_data_frame() %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha = correlation, \n                     width = correlation)) +\n  geom_node_point(size = 6, \n                  color = \"lightblue\") +\n  geom_node_text(aes(label = name),\n                 color = \"red\",\n                 repel = TRUE) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n29.7.7 Bigram\nIn this code chunk below, a bigram data frame is created by using unnest_tokens() of tidytext.\n\nbigrams &lt;- cleaned_text %&gt;%\n  unnest_tokens(bigram, \n                text, \n                token = \"ngrams\", \n                n = 2)\nbigrams \n\n# A tibble: 28,827 × 3\n   newsgroup   id    bigram    \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;     \n 1 alt.atheism 54256 &lt;NA&gt;      \n 2 alt.atheism 54256 &lt;NA&gt;      \n 3 alt.atheism 54256 as i      \n 4 alt.atheism 54256 i don't   \n 5 alt.atheism 54256 don't know\n 6 alt.atheism 54256 know this \n 7 alt.atheism 54256 this book \n 8 alt.atheism 54256 book i    \n 9 alt.atheism 54256 i will    \n10 alt.atheism 54256 will use  \n# ℹ 28,817 more rows\n\n\n\n\n29.7.8 Counting bigrams\nThe code chunk is used to count and sort the bigram data frame ascendingly.\n\nbigrams_count &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  count(bigram, sort = TRUE)\nbigrams_count\n\n# A tibble: 19,888 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 of the     169\n 2 in the     113\n 3 to the      74\n 4 to be       59\n 5 for the     52\n 6 i have      48\n 7 that the    47\n 8 if you      40\n 9 on the      39\n10 it is       38\n# ℹ 19,878 more rows\n\n\n\n\n29.7.9 Cleaning bigram\nThe code chunk below is used to seperate the bigram into two words.\n\nbigrams_separated &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), \n           sep = \" \")\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\nbigrams_filtered\n\n# A tibble: 4,607 × 4\n   newsgroup   id    word1        word2        \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        \n 1 alt.atheism 54256 defines      god          \n 2 alt.atheism 54256 term         preclues     \n 3 alt.atheism 54256 science      ideas        \n 4 alt.atheism 54256 ideas        drawn        \n 5 alt.atheism 54256 supernatural precludes    \n 6 alt.atheism 54256 scientific   assertions   \n 7 alt.atheism 54256 religious    dogma        \n 8 alt.atheism 54256 religion     involves     \n 9 alt.atheism 54256 involves     circumventing\n10 alt.atheism 54256 gain         absolute     \n# ℹ 4,597 more rows\n\n\n\n\n29.7.10 Counting the bigram again\n\nbigram_counts &lt;- bigrams_filtered %&gt;% \n  count(word1, word2, sort = TRUE)\n\n\n\n29.7.11 Create a network graph from bigram data frame\nIn the code chunk below, a network graph is created by using graph_from_data_frame() of igraph package.\n\nbigram_graph &lt;- bigram_counts %&gt;%\n  filter(n &gt; 3) %&gt;%\n  graph_from_data_frame()\nbigram_graph\n\nIGRAPH 95f7ccf DN-- 40 24 -- \n+ attr: name (v/c), n (e/n)\n+ edges from 95f7ccf (vertex names):\n [1] 1          -&gt;2           1          -&gt;3           static     -&gt;void       \n [4] time       -&gt;pad         1          -&gt;4           infield    -&gt;fly        \n [7] mat        -&gt;28          vv         -&gt;vv          1          -&gt;5          \n[10] cock       -&gt;crow        noticeshell-&gt;widget      27         -&gt;1993       \n[13] 3          -&gt;4           child      -&gt;molestation cock       -&gt;crew       \n[16] gun        -&gt;violence    heat       -&gt;sink        homosexual -&gt;male       \n[19] homosexual -&gt;women       include    -&gt;xol         mary       -&gt;magdalene  \n[22] read       -&gt;write       rev        -&gt;20          tt         -&gt;ee         \n\n\n\n\n29.7.12 Visualizing a network of bigrams with ggraph\nIn this code chunk below, ggraph package is used to plot the bigram.\n\nset.seed(1234)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1)\n\n\n\n\n\n\n\n\n\n\n29.7.13 Revised version\n\nset.seed(1234)\n\na &lt;- grid::arrow(type = \"closed\", \n                 length = unit(.15,\n                               \"inches\"))\n\nggraph(bigram_graph, \n       layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), \n                 show.legend = FALSE,\n                 arrow = a, \n                 end_cap = circle(.07,\n                                  'inches')) +\n  geom_node_point(color = \"lightblue\", \n                  size = 5) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1) +\n  theme_void()"
  },
  {
    "objectID": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#references",
    "href": "Hands-on Exercise/Hands-on_Ex05/Hands-on_Ex05.html#references",
    "title": "Hands-on Ex 5: 29 Visualising and Analysing Text Data with R: tidytext methods",
    "section": "29.8 References",
    "text": "29.8 References\n\n29.8.0.1 widyr\n\nReference guide\n\nwidyr: Widen, process, and re-tidy a dataset\nUnited Nations Voting Correlations"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class Exercise/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "In the code chunk below, p_load() of pacman package is used to load tidyverse family of packages\n\npacman::p_load(tidyverse)\n\n\nrealis.csv &lt;- read_csv(\"data/realis2019.csv\")\n\n\nggplot(data = realis.csv,\n       aes(x = `Unit Price ($ psm)`)) + \n  geom_histogram()"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "href": "In-class Exercise/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "In the code chunk below, p_load() of pacman package is used to load tidyverse family of packages\n\npacman::p_load(tidyverse)\n\n\nrealis.csv &lt;- read_csv(\"data/realis2019.csv\")\n\n\nggplot(data = realis.csv,\n       aes(x = `Unit Price ($ psm)`)) + \n  geom_histogram()"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class Exercise/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class Ex04: Visualising Statistical Analysis",
    "section": "",
    "text": "pacman::p_load(ggstatsplot, tidyverse)\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nThe parametric test (student’s t):\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"parametric\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nThe non-parametric(Wilcoxon) test will have median line plotted instead of mean.\n\nset.seed(1234)\n\np_n &lt;- gghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"non-parametric\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\nHow to save the tibble after plotting: - save the plot as an object “p_n”\n\nextract_stats(p_n)\n\n$subtitle_data\n# A tibble: 1 × 12\n  statistic  p.value method                    alternative effectsize       \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;       &lt;chr&gt;            \n1     38743 3.43e-16 Wilcoxon signed rank test two.sided   r (rank biserial)\n  estimate conf.level conf.low conf.high conf.method n.obs expression\n     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt; &lt;list&gt;    \n1    0.528       0.95    0.430     0.613 normal        322 &lt;language&gt;\n\n$caption_data\nNULL\n\n$pairwise_comparisons_data\nNULL\n\n$descriptive_data\nNULL\n\n$one_sample_data\nNULL\n\n$tidy_data\nNULL\n\n$glance_data\nNULL\n\n\nThe parametric test (student’s t):\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"parametric\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nThe robust(bootstrapped method) test will have trimmed mean line plotted instead of mean.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"robust\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nThe parametric test (student’s t):\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"parametric\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nThe bayes test will have not have the Bayes statistics below. mean of MAP used instead.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nAdjusting normal.curve TRUE / FALSE Adjusting normal.curve.args as list\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = TRUE,\n  normal.curve.args = list(linewidth = 0.5),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nNotes on reading documentation: - Logical value: TRUE or FALSE - normal.curve.arg: can use a list of ggplot’s aesthetic arguments\nDot plot: - does sorting from highest to lowest and present in percentile.\n\nggdotplotstats(\n  data = exam,\n  x = ENGLISH,\n  y = CLASS,\n  title = \"\",\n  xlab = \"\"\n)\n\n\n\n\n\n\n\n\nLooking at the exam tibble:\n\nWe need to have a subject column and a scores column instead of the current form with Subjects as the header with all the scores as data.\nDo a pivot table to combine ENGLISH, MATHS, SCIENCE into a subject column.\n\n\nexam_long &lt;- exam %&gt;%\n  pivot_longer(\n    cols = ENGLISH:SCIENCE,\n    names_to = \"SUBJECT\",\n    values_to = \"SCORES\") %&gt;%\n  filter(CLASS == \"3A\")\n\nNote:Data can be filtered using tidyverse commands\n\nggwithinstats(\n  data = filter(exam_long,\n                SUBJECT %in%\n                  c(\"MATHS\", \"SCIENCE\")),\n  x = SUBJECT,\n  y = SCORES,\n  type = \"p\"\n)\n\n\n\n\n\n\n\n\n\n\n\nggscatterstats() wrapped scatter plot into the function.\n\nmarginal set to TRUE for marginal distribution.\n\nlabel.expression allow us to label/highlight the things we want to focus on.\n\n\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = TRUE,\n  label.var = ID,\n  label.expression = ENGLISH &gt; 90 & MATHS &gt; 90,\n  smooth.line.args = list(linewidth = 1, color = \"red\", method = \"lm\", formula = y ~\n    x)\n  )"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex04/In-class_Ex04.html#section",
    "href": "In-class Exercise/In-class_Ex04/In-class_Ex04.html#section",
    "title": "In-class Ex04: Visualising Statistical Analysis",
    "section": "",
    "text": "pacman::p_load(ggstatsplot, tidyverse)\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nThe parametric test (student’s t):\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"parametric\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nThe non-parametric(Wilcoxon) test will have median line plotted instead of mean.\n\nset.seed(1234)\n\np_n &lt;- gghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"non-parametric\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\nHow to save the tibble after plotting: - save the plot as an object “p_n”\n\nextract_stats(p_n)\n\n$subtitle_data\n# A tibble: 1 × 12\n  statistic  p.value method                    alternative effectsize       \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;       &lt;chr&gt;            \n1     38743 3.43e-16 Wilcoxon signed rank test two.sided   r (rank biserial)\n  estimate conf.level conf.low conf.high conf.method n.obs expression\n     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt; &lt;list&gt;    \n1    0.528       0.95    0.430     0.613 normal        322 &lt;language&gt;\n\n$caption_data\nNULL\n\n$pairwise_comparisons_data\nNULL\n\n$descriptive_data\nNULL\n\n$one_sample_data\nNULL\n\n$tidy_data\nNULL\n\n$glance_data\nNULL\n\n\nThe parametric test (student’s t):\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"parametric\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nThe robust(bootstrapped method) test will have trimmed mean line plotted instead of mean.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"robust\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nThe parametric test (student’s t):\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"parametric\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nThe bayes test will have not have the Bayes statistics below. mean of MAP used instead.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = FALSE,\n  normal.curve.args = list(linewidth = 2),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nAdjusting normal.curve TRUE / FALSE Adjusting normal.curve.args as list\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n  normal.curve = TRUE,\n  normal.curve.args = list(linewidth = 0.5),\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nNotes on reading documentation: - Logical value: TRUE or FALSE - normal.curve.arg: can use a list of ggplot’s aesthetic arguments\nDot plot: - does sorting from highest to lowest and present in percentile.\n\nggdotplotstats(\n  data = exam,\n  x = ENGLISH,\n  y = CLASS,\n  title = \"\",\n  xlab = \"\"\n)\n\n\n\n\n\n\n\n\nLooking at the exam tibble:\n\nWe need to have a subject column and a scores column instead of the current form with Subjects as the header with all the scores as data.\nDo a pivot table to combine ENGLISH, MATHS, SCIENCE into a subject column.\n\n\nexam_long &lt;- exam %&gt;%\n  pivot_longer(\n    cols = ENGLISH:SCIENCE,\n    names_to = \"SUBJECT\",\n    values_to = \"SCORES\") %&gt;%\n  filter(CLASS == \"3A\")\n\nNote:Data can be filtered using tidyverse commands\n\nggwithinstats(\n  data = filter(exam_long,\n                SUBJECT %in%\n                  c(\"MATHS\", \"SCIENCE\")),\n  x = SUBJECT,\n  y = SCORES,\n  type = \"p\"\n)\n\n\n\n\n\n\n\n\n\n\n\nggscatterstats() wrapped scatter plot into the function.\n\nmarginal set to TRUE for marginal distribution.\n\nlabel.expression allow us to label/highlight the things we want to focus on.\n\n\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = TRUE,\n  label.var = ID,\n  label.expression = ENGLISH &gt; 90 & MATHS &gt; 90,\n  smooth.line.args = list(linewidth = 1, color = \"red\", method = \"lm\", formula = y ~\n    x)\n  )"
  },
  {
    "objectID": "In-class Exercise/In-class_Ex04/In-class_Ex04.html#visualing-models",
    "href": "In-class Exercise/In-class_Ex04/In-class_Ex04.html#visualing-models",
    "title": "In-class Ex04: Visualising Statistical Analysis",
    "section": "10.4 Visualing Models",
    "text": "10.4 Visualing Models\n\nDiagnostics Test on Models"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my first Quarto website",
    "section": "",
    "text": "This is my first Quarto website.\nThe picture below shows my brain working at 100% learning about visual analytics.\n\nSome recommended readings for this course:\n\nR for Data Science, 2nd Edition\nR for Visual Analytics\nText Mining with R\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "Self-Practice/Self-Practice_02/Self-Practice_02.html",
    "href": "Self-Practice/Self-Practice_02/Self-Practice_02.html",
    "title": "Self Practice 2: R for Data Science",
    "section": "",
    "text": "library(nycflights13)\nlibrary(tidyverse)\n\n\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThe most important difference between tibbles and data frames is the way tibbles print; they are designed for large datasets, so they only show the first few rows and only the columns that fit on one screen. There are a few options to see everything. If you’re using RStudio, the most convenient is probably View(flights), which will open an interactive scrollable and filterable view. Otherwise you can use print(flights, width = Inf) to show all columns, or use glimpse():\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\nIn both views, the variables names are followed by abbreviations that tell you the type of each variable: &lt;int&gt; is short for integer, &lt;dbl&gt; is short for double (aka real numbers), &lt;chr&gt; for character (aka strings), and &lt;dttm&gt; for date-time. These are important because the operations you can perform on a column depend so much on its “type”.\n\n\nYou’re about to learn the primary dplyr verbs (functions) which will allow you to solve the vast majority of your data manipulation challenges. But before we discuss their individual differences, it’s worth stating what they have in common:\n\nThe first argument is always a data frame.\nThe subsequent arguments typically describe which columns to operate on, using the variable names (without quotes).\nThe output is always a new data frame.\n\n\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     1     17.8 \n 2  2013     1     2      7   \n 3  2013     1     3     18.3 \n 4  2013     1     4     -3.2 \n 5  2013     1     5     20.2 \n 6  2013     1     6      9.28\n 7  2013     1     7     -7.74\n 8  2013     1     8      7.79\n 9  2013     1     9     18.1 \n10  2013     1    10      6.68\n# ℹ 355 more rows\n\n\n\n\n\nThe most important verbs that operate on rows of a dataset are filter(), which changes which rows are present without changing their order, and arrange(), which changes the order of the rows without changing which are present. Both functions only affect the rows, and the columns are left unchanged. We’ll also discuss distinct() which finds rows with unique values but unlike arrange() and filter() it can also optionally modify the columns.\n\n\nfilter() allows you to keep rows based on the values of the columns1. The first argument is the data frame. The second and subsequent arguments are the conditions that must be true to keep the row. For example, we could find all flights that departed more than 120 minutes (two hours) late:\n\nflights |&gt;\n  filter(dep_delay &gt; 120)\n\n# A tibble: 9,723 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      848           1835       853     1001           1950\n 2  2013     1     1      957            733       144     1056            853\n 3  2013     1     1     1114            900       134     1447           1222\n 4  2013     1     1     1540           1338       122     2020           1825\n 5  2013     1     1     1815           1325       290     2120           1542\n 6  2013     1     1     1842           1422       260     1958           1535\n 7  2013     1     1     1856           1645       131     2212           2005\n 8  2013     1     1     1934           1725       129     2126           1855\n 9  2013     1     1     1938           1703       155     2109           1823\n10  2013     1     1     1942           1705       157     2124           1830\n# ℹ 9,713 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Flights that departed on January 1\nflights |&gt; \n  filter(month == 1 & day == 1)\n\n# A tibble: 842 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 832 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Flights that departed in January or February\nflights |&gt; \n  filter(month == 1 | month == 2)\n\n# A tibble: 51,955 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 51,945 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# A shorter way to select flights that departed in January or February\nflights |&gt; \n  filter(month %in% c(1, 2))\n\n# A tibble: 51,955 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 51,945 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\narrange() changes the order of the rows based on the value of the columns. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns. For example, the following code sorts by the departure time, which is spread over four columns. We get the earliest years first, then within a year the earliest months, etc.\n\nflights |&gt; \n  arrange(year, month, day, dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nYou can use desc() on a column inside of arrange() to re-order the data frame based on that column in descending (big-to-small) order. For example, this code orders flights from most to least delayed:\n\nflights |&gt;\n  arrange(desc(dep_delay))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\ndistinct() finds all the unique rows in a dataset, so in a technical sense, it primarily operates on the rows. Most of the time, however, you’ll want the distinct combination of some variables, so you can also optionally supply column names:\n\n# Remove duplicate rows, if any\nflights |&gt; \n  distinct()\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Find all unique origin and destination pairs\nflights |&gt; \n  distinct(origin, dest)\n\n# A tibble: 224 × 2\n   origin dest \n   &lt;chr&gt;  &lt;chr&gt;\n 1 EWR    IAH  \n 2 LGA    IAH  \n 3 JFK    MIA  \n 4 JFK    BQN  \n 5 LGA    ATL  \n 6 EWR    ORD  \n 7 EWR    FLL  \n 8 LGA    IAD  \n 9 JFK    MCO  \n10 LGA    ORD  \n# ℹ 214 more rows\n\n\nAlternatively, if you want to the keep other columns when filtering for unique rows, you can use the .keep_all = TRUE option.\n\nflights |&gt; \n  distinct(origin, dest, .keep_all = TRUE)\n\n# A tibble: 224 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 214 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIt’s not a coincidence that all of these distinct flights are on January 1: distinct() will find the first occurrence of a unique row in the dataset and discard the rest.\nIf you want to find the number of occurrences instead, you’re better off swapping distinct() for count(), and with the sort = TRUE argument you can arrange them in descending order of number of occurrences. \n\nflights |&gt;\n  count(origin, dest, sort = TRUE)\n\n# A tibble: 224 × 3\n   origin dest      n\n   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n 1 JFK    LAX   11262\n 2 LGA    ATL   10263\n 3 LGA    ORD    8857\n 4 JFK    SFO    8204\n 5 LGA    CLT    6168\n 6 EWR    ORD    6100\n 7 JFK    BOS    5898\n 8 LGA    MIA    5781\n 9 JFK    MCO    5464\n10 EWR    BOS    5327\n# ℹ 214 more rows\n\n\n\n\n\n\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\n\n\n\nflights |&gt; \n  filter(arr_delay &gt;= 2)\n\n# A tibble: 127,929 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            558        -4      740            728\n 5  2013     1     1      555            600        -5      913            854\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      559            600        -1      941            910\n 9  2013     1     1      600            600         0      837            825\n10  2013     1     1      602            605        -3      821            805\n# ℹ 127,919 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Flew to Houston (`IAH` or `HOU`)\n\nflights |&gt; \n  filter(dest %in% c(\"IAH\",\"HOU\"))\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Were operated by United, American, or Delta\n\nflights |&gt; \n  filter(carrier %in% c(\"UA\",\"DL\"))\n\n# A tibble: 106,775 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      554            600        -6      812            837\n 4  2013     1     1      554            558        -4      740            728\n 5  2013     1     1      558            600        -2      924            917\n 6  2013     1     1      558            600        -2      923            937\n 7  2013     1     1      559            600        -1      854            902\n 8  2013     1     1      602            610        -8      812            820\n 9  2013     1     1      606            610        -4      837            845\n10  2013     1     1      607            607         0      858            915\n# ℹ 106,765 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Departed in summer (July, August, and September)\n\nflights |&gt; \n  filter(month %in% c(\"7\",\"8\", \"9\"))\n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Arrived more than two hours late, but didn’t leave late\n\nflights |&gt; \n  filter(arr_delay &gt; 2 & dep_delay == 0)\n\n# A tibble: 4,368 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      600            600         0      837            825\n 2  2013     1     1      635            635         0     1028            940\n 3  2013     1     1      739            739         0     1104           1038\n 4  2013     1     1      745            745         0     1135           1125\n 5  2013     1     1      800            800         0     1022           1014\n 6  2013     1     1      805            805         0     1015           1005\n 7  2013     1     1      810            810         0     1048           1037\n 8  2013     1     1      823            823         0     1151           1135\n 9  2013     1     1      830            830         0     1018           1015\n10  2013     1     1      835            835         0     1210           1150\n# ℹ 4,358 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Were delayed by at least an hour, but made up over 30 minutes in flight.\n\nflights |&gt; \n  filter(dep_delay &gt; 1 & (dep_delay - arr_delay) &gt; 30 )\n\n# A tibble: 7,474 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      857            851         6     1157           1222\n 2  2013     1     1      909            810        59     1331           1315\n 3  2013     1     1     1025            951        34     1258           1302\n 4  2013     1     1     1625           1550        35     2054           2050\n 5  2013     1     1     1957           1945        12     2307           2329\n 6  2013     1     1     2035           2030         5     2337              5\n 7  2013     1     1     2046           2035        11     2144           2213\n 8  2013     1     1     2107           2040        27     2354           2359\n 9  2013     1     1     2205           1720       285       46           2040\n10  2013     1     1     2326           2130       116      131             18\n# ℹ 7,464 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nSort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\n\n\nflights |&gt; \n  arrange(desc(dep_delay))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nSort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)\n\n\nflights |&gt; \n  arrange(desc(speed = distance/(hour+ minute/60)))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      857            900        -3     1516           1530\n 2  2013     1     2      909            900         9     1525           1530\n 3  2013     1     3      914            900        14     1504           1530\n 4  2013     1     4      900            900         0     1516           1530\n 5  2013     1     5      858            900        -2     1519           1530\n 6  2013     1     6     1019            900        79     1558           1530\n 7  2013     1     7     1042            900       102     1620           1530\n 8  2013     1     8      901            900         1     1504           1530\n 9  2013     1     9      641            900      1301     1242           1530\n10  2013     1    10      859            900        -1     1449           1530\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nWas there a flight on every day of 2013? Yes.\n\n\nflights |&gt; \n  distinct(year, month,day)\n\n# A tibble: 365 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     2\n 3  2013     1     3\n 4  2013     1     4\n 5  2013     1     5\n 6  2013     1     6\n 7  2013     1     7\n 8  2013     1     8\n 9  2013     1     9\n10  2013     1    10\n# ℹ 355 more rows\n\n\n\nWhich flights traveled the farthest distance? Which traveled the least distance?\n\nJFK-HNL = furthest\nEWR-LGA - nearest\n\n\n\nflights |&gt; \n  arrange(distance)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7    27       NA            106        NA       NA            245\n 2  2013     1     3     2127           2129        -2     2222           2224\n 3  2013     1     4     1240           1200        40     1333           1306\n 4  2013     1     4     1829           1615       134     1937           1721\n 5  2013     1     4     2128           2129        -1     2218           2224\n 6  2013     1     5     1155           1200        -5     1241           1306\n 7  2013     1     6     2125           2129        -4     2224           2224\n 8  2013     1     7     2124           2129        -5     2212           2224\n 9  2013     1     8     2127           2130        -3     2304           2225\n10  2013     1     9     2126           2129        -3     2217           2224\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nDoes it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.\n\nfilter() should be used first as it will narrow down the number of rows that arrange() need to sort.\n\n\n\n\n\n\nThere are four important verbs that affect the columns without changing the rows: mutate() creates new columns that are derived from the existing columns, select() changes which columns are present, rename() changes the names of the columns, and relocate() changes the positions of the columns.\n\n\nThe job of mutate() is to add new columns that are calculated from the existing columns. In the transform chapters, you’ll learn a large set of functions that you can use to manipulate different types of variables. For now, we’ll stick with basic algebra, which allows us to compute the gain, how much time a delayed flight made up in the air, and the speed in miles per hour:\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .before = 1\n  )\n\n# A tibble: 336,776 × 21\n    gain speed  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1    -9  370.  2013     1     1      517            515         2      830\n 2   -16  374.  2013     1     1      533            529         4      850\n 3   -31  408.  2013     1     1      542            540         2      923\n 4    17  517.  2013     1     1      544            545        -1     1004\n 5    19  394.  2013     1     1      554            600        -6      812\n 6   -16  288.  2013     1     1      554            558        -4      740\n 7   -24  404.  2013     1     1      555            600        -5      913\n 8    11  259.  2013     1     1      557            600        -3      709\n 9     5  405.  2013     1     1      557            600        -3      838\n10   -10  319.  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThe . is a sign that .before is an argument to the function, not the name of a third new variable we are creating. You can also use .after to add after a variable, and in both .before and .after you can use the variable name instead of a position. For example, we could add the new variables after day:\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .after = day\n  )\n\n# A tibble: 336,776 × 21\n    year month   day  gain speed dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1  2013     1     1    -9  370.      517            515         2      830\n 2  2013     1     1   -16  374.      533            529         4      850\n 3  2013     1     1   -31  408.      542            540         2      923\n 4  2013     1     1    17  517.      544            545        -1     1004\n 5  2013     1     1    19  394.      554            600        -6      812\n 6  2013     1     1   -16  288.      554            558        -4      740\n 7  2013     1     1   -24  404.      555            600        -5      913\n 8  2013     1     1    11  259.      557            600        -3      709\n 9  2013     1     1     5  405.      557            600        -3      838\n10  2013     1     1   -10  319.      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nNote that since we haven’t assigned the result of the above computation back to flights, the new variables gain, hours, and gain_per_hour will only be printed but will not be stored in a data frame. And if we want them to be available in a data frame for future use, we should think carefully about whether we want the result to be assigned back to flights, overwriting the original data frame with many more variables, or to a new object. Often, the right answer is a new object that is named informatively to indicate its contents, e.g., delay_gain, but you might also have good reasons for overwriting flights.\n\n\n\nIt’s not uncommon to get datasets with hundreds or even thousands of variables. In this situation, the first challenge is often just focusing on the variables you’re interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables:\n\nSelect columns by name:\n\nflights |&gt;\n  select(year, month, day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows\n\n\nSelect all columns between year and day (inclusive):\n\nflights |&gt;\n  select(year:day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows\n\n\nSelect all columns except those from year to day (inclusive):\n\nflights |&gt;\n  select(!year:day)\n\n# A tibble: 336,776 × 16\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;  \n 1      517            515         2      830            819        11 UA     \n 2      533            529         4      850            830        20 UA     \n 3      542            540         2      923            850        33 AA     \n 4      544            545        -1     1004           1022       -18 B6     \n 5      554            600        -6      812            837       -25 DL     \n 6      554            558        -4      740            728        12 UA     \n 7      555            600        -5      913            854        19 B6     \n 8      557            600        -3      709            723       -14 EV     \n 9      557            600        -3      838            846        -8 B6     \n10      558            600        -2      753            745         8 AA     \n# ℹ 336,766 more rows\n# ℹ 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;,\n#   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nHistorically this operation was done with - instead of !, so you’re likely to see that in the wild. These two operators serve the same purpose but with subtle differences in behavior. We recommend using ! because it reads as “not” and combines well with & and |.\nSelect all columns that are characters:\n\nflights |&gt;\n  select(where(is.character))\n\n# A tibble: 336,776 × 4\n   carrier tailnum origin dest \n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;\n 1 UA      N14228  EWR    IAH  \n 2 UA      N24211  LGA    IAH  \n 3 AA      N619AA  JFK    MIA  \n 4 B6      N804JB  JFK    BQN  \n 5 DL      N668DN  LGA    ATL  \n 6 UA      N39463  EWR    ORD  \n 7 B6      N516JB  EWR    FLL  \n 8 EV      N829AS  LGA    IAD  \n 9 B6      N593JB  JFK    MCO  \n10 AA      N3ALAA  LGA    ORD  \n# ℹ 336,766 more rows\n\n\n\nThere are a number of helper functions you can use within select():\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\nends_with(\"xyz\"): matches names that end with “xyz”.\ncontains(\"ijk\"): matches names that contain “ijk”.\nnum_range(\"x\", 1:3): matches x1, x2 and x3.\n\nSee ?select for more details. Once you know regular expressions (the topic of Chapter 15) you’ll also be able to use matches() to select variables that match a pattern.\nYou can rename variables as you select() them by using =. The new name appears on the left hand side of the =, and the old variable appears on the right hand side:\n\nflights |&gt; \n  select(tail_num = tailnum)\n\n# A tibble: 336,776 × 1\n   tail_num\n   &lt;chr&gt;   \n 1 N14228  \n 2 N24211  \n 3 N619AA  \n 4 N804JB  \n 5 N668DN  \n 6 N39463  \n 7 N516JB  \n 8 N829AS  \n 9 N593JB  \n10 N3ALAA  \n# ℹ 336,766 more rows\n\n\n\n\n\nIf you want to keep all the existing variables and just want to rename a few, you can use rename() instead of select():\n\nflights |&gt; \n  rename(tail_num = tailnum)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tail_num &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIf you have a bunch of inconsistently named columns and it would be painful to fix them all by hand, check out janitor::clean_names() which provides some useful automated cleaning.\n\n\n\nUse relocate() to move variables around. You might want to collect related variables together or move important variables to the front. By default relocate() moves variables to the front:\n\nflights |&gt; \n  relocate(time_hour, air_time)\n\n# A tibble: 336,776 × 19\n   time_hour           air_time  year month   day dep_time sched_dep_time\n   &lt;dttm&gt;                 &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n 1 2013-01-01 05:00:00      227  2013     1     1      517            515\n 2 2013-01-01 05:00:00      227  2013     1     1      533            529\n 3 2013-01-01 05:00:00      160  2013     1     1      542            540\n 4 2013-01-01 05:00:00      183  2013     1     1      544            545\n 5 2013-01-01 06:00:00      116  2013     1     1      554            600\n 6 2013-01-01 05:00:00      150  2013     1     1      554            558\n 7 2013-01-01 06:00:00      158  2013     1     1      555            600\n 8 2013-01-01 06:00:00       53  2013     1     1      557            600\n 9 2013-01-01 06:00:00      140  2013     1     1      557            600\n10 2013-01-01 06:00:00      138  2013     1     1      558            600\n# ℹ 336,766 more rows\n# ℹ 12 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n#   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;,\n#   dest &lt;chr&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;\n\n\nYou can also specify where to put them using the .before and .after arguments, just like in mutate():\n\nflights |&gt; \n  relocate(year:dep_time, .after = time_hour)\n\n# A tibble: 336,776 × 19\n   sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight\n            &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1            515         2      830            819        11 UA        1545\n 2            529         4      850            830        20 UA        1714\n 3            540         2      923            850        33 AA        1141\n 4            545        -1     1004           1022       -18 B6         725\n 5            600        -6      812            837       -25 DL         461\n 6            558        -4      740            728        12 UA        1696\n 7            600        -5      913            854        19 B6         507\n 8            600        -3      709            723       -14 EV        5708\n 9            600        -3      838            846        -8 B6          79\n10            600        -2      753            745         8 AA         301\n# ℹ 336,766 more rows\n# ℹ 12 more variables: tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, year &lt;int&gt;,\n#   month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;\n\n\n\nflights |&gt; \n  relocate(starts_with(\"arr\"), .before = dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day arr_time arr_delay dep_time sched_dep_time dep_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     1      830        11      517            515         2\n 2  2013     1     1      850        20      533            529         4\n 3  2013     1     1      923        33      542            540         2\n 4  2013     1     1     1004       -18      544            545        -1\n 5  2013     1     1      812       -25      554            600        -6\n 6  2013     1     1      740        12      554            558        -4\n 7  2013     1     1      913        19      555            600        -5\n 8  2013     1     1      709       -14      557            600        -3\n 9  2013     1     1      838        -8      557            600        -3\n10  2013     1     1      753         8      558            600        -2\n# ℹ 336,766 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\ndep_delay = dep_time - sched_dep_time\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n\nflights |&gt;\n  select(contains(c(\"dep\", \"time\", \"arr\", \"delay\")))\n\n# A tibble: 336,776 × 9\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time air_time\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt;\n 1      517            515         2      830            819      227\n 2      533            529         4      850            830      227\n 3      542            540         2      923            850      160\n 4      544            545        -1     1004           1022      183\n 5      554            600        -6      812            837      116\n 6      554            558        -4      740            728      150\n 7      555            600        -5      913            854      158\n 8      557            600        -3      709            723       53\n 9      557            600        -3      838            846      140\n10      558            600        -2      753            745      138\n# ℹ 336,766 more rows\n# ℹ 3 more variables: time_hour &lt;dttm&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;\n\n\n\nWhat happens if you specify the name of the same variable multiple times in a select() call?\n\n\nflights |&gt;\n  select(arr_time,arr_time)\n\n# A tibble: 336,776 × 1\n   arr_time\n      &lt;int&gt;\n 1      830\n 2      850\n 3      923\n 4     1004\n 5      812\n 6      740\n 7      913\n 8      709\n 9      838\n10      753\n# ℹ 336,766 more rows\n\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\n\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\nflights |&gt;\n  select(any_of(variables))\n\n# A tibble: 336,776 × 5\n    year month   day dep_delay arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2013     1     1         2        11\n 2  2013     1     1         4        20\n 3  2013     1     1         2        33\n 4  2013     1     1        -1       -18\n 5  2013     1     1        -6       -25\n 6  2013     1     1        -4        12\n 7  2013     1     1        -5        19\n 8  2013     1     1        -3       -14\n 9  2013     1     1        -3        -8\n10  2013     1     1        -2         8\n# ℹ 336,766 more rows\n\n\n\nflights |&gt; \n  select(contains(\"TIME\"))\n\n# A tibble: 336,776 × 6\n   dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n 1      517            515      830            819      227 2013-01-01 05:00:00\n 2      533            529      850            830      227 2013-01-01 05:00:00\n 3      542            540      923            850      160 2013-01-01 05:00:00\n 4      544            545     1004           1022      183 2013-01-01 05:00:00\n 5      554            600      812            837      116 2013-01-01 06:00:00\n 6      554            558      740            728      150 2013-01-01 05:00:00\n 7      555            600      913            854      158 2013-01-01 06:00:00\n 8      557            600      709            723       53 2013-01-01 06:00:00\n 9      557            600      838            846      140 2013-01-01 06:00:00\n10      558            600      753            745      138 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n\n\nflights |&gt; \n  mutate(air_time_min = air_time,\n         .before = 1)\n\n# A tibble: 336,776 × 20\n   air_time_min  year month   day dep_time sched_dep_time dep_delay arr_time\n          &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1          227  2013     1     1      517            515         2      830\n 2          227  2013     1     1      533            529         4      850\n 3          160  2013     1     1      542            540         2      923\n 4          183  2013     1     1      544            545        -1     1004\n 5          116  2013     1     1      554            600        -6      812\n 6          150  2013     1     1      554            558        -4      740\n 7          158  2013     1     1      555            600        -5      913\n 8           53  2013     1     1      557            600        -3      709\n 9          140  2013     1     1      557            600        -3      838\n10          138  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nWhy doesn’t the following work, and what does the error mean?\n\nafter selecting the column tailnum, there is no more column called arr_delay.\n\n\n\nflights |&gt; \n  select(tailnum) |&gt; \n  arrange(arr_delay)\n#&gt; Error in `arrange()`:\n#&gt; ℹ In argument: `..1 = arr_delay`.\n#&gt; Caused by error:\n#&gt; ! object 'arr_delay' not found\n\n\n\n\n\nWe’ve shown you simple examples of the pipe above, but its real power arises when you start to combine multiple verbs. For example, imagine that you wanted to find the fastest flights to Houston’s IAH airport: you need to combine filter(), mutate(), select(), and arrange():\n\nflights |&gt; \n  filter(dest == \"IAH\", ) |&gt; \n  mutate(speed = distance / air_time*60) |&gt; \n  select(year:day, dep_time, carrier, flight, speed) |&gt; \n  arrange(desc(speed))\n\n# A tibble: 7,198 × 7\n    year month   day dep_time carrier flight speed\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n 1  2013     7     9      707 UA         226  522.\n 2  2013     8    27     1850 UA        1128  521.\n 3  2013     8    28      902 UA        1711  519.\n 4  2013     8    28     2122 UA        1022  519.\n 5  2013     6    11     1628 UA        1178  515.\n 6  2013     8    27     1017 UA         333  515.\n 7  2013     8    27     1205 UA        1421  515.\n 8  2013     8    27     1758 UA         302  515.\n 9  2013     9    27      521 UA         252  515.\n10  2013     8    28      625 UA         559  515.\n# ℹ 7,188 more rows\n\n\n\n\nUse group_by() to divide your dataset into groups meaningful for your analysis:\n\nflights |&gt; \n  group_by(month)\n\n# A tibble: 336,776 × 19\n# Groups:   month [12]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\ngroup_by() doesn’t change the data but, if you look closely at the output, you’ll notice that the output indicates that it is “grouped by” month (Groups: month [12]). This means subsequent operations will now work “by month”. group_by() adds this grouped feature (referred to as class) to the data frame, which changes the behavior of the subsequent verbs applied to the data.\n\n\n\nThe most important grouped operation is a summary, which, if being used to calculate a single summary statistic, reduces the data frame to have a single row for each group. In dplyr, this operation is performed by summarize()3, as shown by the following example, which computes the average departure delay by month:\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarise(\n    avg_delay = mean(dep_delay)\n  )\n\n# A tibble: 12 × 2\n   month avg_delay\n   &lt;int&gt;     &lt;dbl&gt;\n 1     1        NA\n 2     2        NA\n 3     3        NA\n 4     4        NA\n 5     5        NA\n 6     6        NA\n 7     7        NA\n 8     8        NA\n 9     9        NA\n10    10        NA\n11    11        NA\n12    12        NA\n\n\nUhoh! Something has gone wrong and all of our results are NAs (pronounced “N-A”), R’s symbol for missing value. This happened because some of the observed flights had missing data in the delay column, and so when we calculated the mean including those values, we got an NA result. We’ll come back to discuss missing values in detail in Chapter 18, but for now we’ll tell the mean() function to ignore all missing values by setting the argument na.rm to TRUE.\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarise(\n    avg_delay = mean(dep_delay, na.rm = TRUE)\n  )\n\n# A tibble: 12 × 2\n   month avg_delay\n   &lt;int&gt;     &lt;dbl&gt;\n 1     1     10.0 \n 2     2     10.8 \n 3     3     13.2 \n 4     4     13.9 \n 5     5     13.0 \n 6     6     20.8 \n 7     7     21.7 \n 8     8     12.6 \n 9     9      6.72\n10    10      6.24\n11    11      5.44\n12    12     16.6 \n\n\nYou can create any number of summaries in a single call to summarize(). You’ll learn various useful summaries in the upcoming chapters, but one very useful summary is n(), which returns the number of rows in each group:\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarise(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\n# A tibble: 12 × 3\n   month avg_delay     n\n   &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;\n 1     1     10.0  27004\n 2     2     10.8  24951\n 3     3     13.2  28834\n 4     4     13.9  28330\n 5     5     13.0  28796\n 6     6     20.8  28243\n 7     7     21.7  29425\n 8     8     12.6  29327\n 9     9      6.72 27574\n10    10      6.24 28889\n11    11      5.44 27268\n12    12     16.6  28135\n\n\nMeans and counts can get you a surprisingly long way in data science!\n\n\n\nThere are five handy functions that allow you extract specific rows within each group:\n\ndf |&gt; slice_head(n = 1) takes the first row from each group.\ndf |&gt; slice_tail(n = 1) takes the last row in each group.\ndf |&gt; slice_min(x, n = 1) takes the row with the smallest value of column x.\ndf |&gt; slice_max(x, n = 1) takes the row with the largest value of column x.\ndf |&gt; slice_sample(n = 1) takes one random row.\n\nYou can vary n to select more than one row, or instead of n =, you can use prop = 0.1 to select (e.g.) 10% of the rows in each group. For example, the following code finds the flights that are most delayed upon arrival at each destination:\n\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1,with_ties = FALSE) |&gt;\n  relocate(dest) |&gt; \n  arrange(desc(arr_delay))\n\n# A tibble: 105 × 19\n# Groups:   dest [105]\n   dest   year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1 HNL    2013     1     9      641            900      1301     1242\n 2 CMH    2013     6    15     1432           1935      1137     1607\n 3 ORD    2013     1    10     1121           1635      1126     1239\n 4 SFO    2013     9    20     1139           1845      1014     1457\n 5 CVG    2013     7    22      845           1600      1005     1044\n 6 TPA    2013     4    10     1100           1900       960     1342\n 7 MSP    2013     3    17     2321            810       911      135\n 8 ATL    2013     7    22     2257            759       898      121\n 9 MIA    2013    12     5      756           1700       896     1058\n10 LAS    2013     5    19      713           1700       853     1007\n# ℹ 95 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nNote that there are 105 destinations but we get 108 rows here. What’s up? slice_min() and slice_max() keep tied values so n = 1 means give us all rows with the highest value. If you want exactly one row per group you can set with_ties = FALSE.\n\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1) |&gt;\n  relocate(dest) |&gt; \n  arrange(desc(arr_delay))\n\n# A tibble: 108 × 19\n# Groups:   dest [105]\n   dest   year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1 HNL    2013     1     9      641            900      1301     1242\n 2 CMH    2013     6    15     1432           1935      1137     1607\n 3 ORD    2013     1    10     1121           1635      1126     1239\n 4 SFO    2013     9    20     1139           1845      1014     1457\n 5 CVG    2013     7    22      845           1600      1005     1044\n 6 TPA    2013     4    10     1100           1900       960     1342\n 7 MSP    2013     3    17     2321            810       911      135\n 8 ATL    2013     7    22     2257            759       898      121\n 9 MIA    2013    12     5      756           1700       896     1058\n10 LAS    2013     5    19      713           1700       853     1007\n# ℹ 98 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThis is similar to computing the max delay with summarize(), but you get the whole corresponding row (or rows if there’s a tie) instead of the single summary statistic.\n\n\n\nYou can create groups using more than one variable. For example, we could make a group for each date.\n\ndaily &lt;- flights |&gt;  \n  group_by(year, month, day)\ndaily\n\n# A tibble: 336,776 × 19\n# Groups:   year, month, day [365]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIf you’re happy with this behavior, you can explicitly request it in order to suppress the message:\n\ndaily_flights &lt;- daily |&gt; \n  summarize(\n    n = n(), \n    .groups = \"drop_last\"\n  )\ndaily_flights\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1   842\n 2  2013     1     2   943\n 3  2013     1     3   914\n 4  2013     1     4   915\n 5  2013     1     5   720\n 6  2013     1     6   832\n 7  2013     1     7   933\n 8  2013     1     8   899\n 9  2013     1     9   902\n10  2013     1    10   932\n# ℹ 355 more rows\n\n\nAlternatively, change the default behavior by setting a different value, e.g., “drop” to drop all grouping or “keep” to preserve the same groups.\n\ndaily_flights &lt;- daily |&gt; \n  summarize(\n    n = n(), \n    .groups = \"drop\"\n  )\ndaily_flights\n\n# A tibble: 365 × 4\n    year month   day     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1   842\n 2  2013     1     2   943\n 3  2013     1     3   914\n 4  2013     1     4   915\n 5  2013     1     5   720\n 6  2013     1     6   832\n 7  2013     1     7   933\n 8  2013     1     8   899\n 9  2013     1     9   902\n10  2013     1    10   932\n# ℹ 355 more rows\n\n\n\n\n\nYou might also want to remove grouping from a data frame without using summarize(). You can do this with ungroup().\n\ndaily |&gt; \n  ungroup()\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nNow let’s see what happens when you summarize an ungrouped data frame.\n\ndaily |&gt; \n  ungroup() |&gt;\n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    flights = n()\n  )\n\n# A tibble: 1 × 2\n  avg_delay flights\n      &lt;dbl&gt;   &lt;int&gt;\n1      12.6  336776\n\n\nYou get a single row back because dplyr treats all the rows in an ungrouped data frame as belonging to one group.\n\n\n\ndplyr 1.1.0 includes a new, experimental, syntax for per-operation grouping, the .by argument. group_by() and ungroup() aren’t going away, but you can now also use the .by argument to group within a single operation:\n\nflights |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = month\n  ) |&gt; \n  arrange(month)\n\n# A tibble: 12 × 3\n   month delay     n\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1     1 10.0  27004\n 2     2 10.8  24951\n 3     3 13.2  28834\n 4     4 13.9  28330\n 5     5 13.0  28796\n 6     6 20.8  28243\n 7     7 21.7  29425\n 8     8 12.6  29327\n 9     9  6.72 27574\n10    10  6.24 28889\n11    11  5.44 27268\n12    12 16.6  28135\n\n\nOr if you want to group by multiple variables:\n\nflights |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = c(origin, dest)\n  )\n\n# A tibble: 224 × 4\n   origin dest  delay     n\n   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n 1 EWR    IAH   11.8   3973\n 2 LGA    IAH    9.06  2951\n 3 JFK    MIA    9.34  3314\n 4 JFK    BQN    6.67   599\n 5 LGA    ATL   11.4  10263\n 6 EWR    ORD   14.6   6100\n 7 EWR    FLL   13.5   3793\n 8 LGA    IAD   16.7   1803\n 9 JFK    MCO   10.6   5464\n10 LGA    ORD   10.7   8857\n# ℹ 214 more rows\n\n\n\n\n\n\nWhich carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarize(n()))\n\n\nflights |&gt; \n  group_by(carrier, dest) |&gt; \n  summarise(n())\n\n# A tibble: 314 × 3\n# Groups:   carrier [16]\n   carrier dest  `n()`\n   &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n 1 9E      ATL      59\n 2 9E      AUS       2\n 3 9E      AVL      10\n 4 9E      BGR       1\n 5 9E      BNA     474\n 6 9E      BOS     914\n 7 9E      BTV       2\n 8 9E      BUF     833\n 9 9E      BWI     856\n10 9E      CAE       3\n# ℹ 304 more rows\n\n\n\nFind the flights that are most delayed upon departure from each destination.\n\n\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(dep_delay) |&gt; \n  arrange(desc(dep_delay))\n\n# A tibble: 105 × 19\n# Groups:   dest [105]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 95 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nHow do delays vary over the course of the day. Illustrate your answer with a plot.\nWhat happens if you supply a negative n to slice_min() and friends?\nExplain what count() does in terms of the dplyr verbs you just learned. What does the sort argument to count() do?\n\n\ndf &lt;- tibble(\n  x = 1:5,\n  y = c(\"a\", \"b\", \"a\", \"a\", \"b\"),\n  z = c(\"K\", \"K\", \"L\", \"L\", \"K\")\n)\n\n\ndf |&gt;\n  group_by(y)\n\n# A tibble: 5 × 3\n# Groups:   y [2]\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     2 b     K    \n3     3 a     L    \n4     4 a     L    \n5     5 b     K    \n\n\n\ndf |&gt;\n  arrange(y)\n\n# A tibble: 5 × 3\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     3 a     L    \n3     4 a     L    \n4     2 b     K    \n5     5 b     K    \n\n\n\ndf |&gt;\n  group_by(y) |&gt;\n  summarize(mean_x = mean(x))\n\n# A tibble: 2 × 2\n  y     mean_x\n  &lt;chr&gt;  &lt;dbl&gt;\n1 a       2.67\n2 b       3.5 \n\n\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n# A tibble: 3 × 3\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  mutate(mean_x = mean(x))\n\n# A tibble: 5 × 4\n# Groups:   y, z [3]\n      x y     z     mean_x\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 a     K        1  \n2     2 b     K        3.5\n3     3 a     L        3.5\n4     4 a     L        3.5\n5     5 b     K        3.5\n\n\n\n\n\n\nWhenever you do any aggregation, it’s always a good idea to include a count (n()). That way, you can ensure that you’re not drawing conclusions based on very small amounts of data. We’ll demonstrate this with some baseball data from the Lahman package. Specifically, we will compare what proportion of times a player gets a hit (H) vs. the number of times they try to put the ball in play (AB):\n\nbatters &lt;- Lahman::Batting |&gt; \n  group_by(playerID) |&gt; \n  summarize(\n    performance = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),\n    n = sum(AB, na.rm = TRUE)\n  )\nbatters\n\n# A tibble: 20,469 × 3\n   playerID  performance     n\n   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;\n 1 aardsda01      0          4\n 2 aaronha01      0.305  12364\n 3 aaronto01      0.229    944\n 4 aasedo01       0          5\n 5 abadan01       0.0952    21\n 6 abadfe01       0.111      9\n 7 abadijo01      0.224     49\n 8 abbated01      0.254   3044\n 9 abbeybe01      0.169    225\n10 abbeych01      0.281   1756\n# ℹ 20,459 more rows\n\n\n\nbatters |&gt; \n  filter(n &gt; 100) |&gt; \n  ggplot(aes(x = n, y = performance)) +\n  geom_point(alpha = 1 / 10) + \n  geom_smooth(se = FALSE)"
  },
  {
    "objectID": "Self-Practice/Self-Practice_02/Self-Practice_02.html#rows",
    "href": "Self-Practice/Self-Practice_02/Self-Practice_02.html#rows",
    "title": "Self Practice 2: R for Data Science",
    "section": "",
    "text": "The most important verbs that operate on rows of a dataset are filter(), which changes which rows are present without changing their order, and arrange(), which changes the order of the rows without changing which are present. Both functions only affect the rows, and the columns are left unchanged. We’ll also discuss distinct() which finds rows with unique values but unlike arrange() and filter() it can also optionally modify the columns.\n\n\nfilter() allows you to keep rows based on the values of the columns1. The first argument is the data frame. The second and subsequent arguments are the conditions that must be true to keep the row. For example, we could find all flights that departed more than 120 minutes (two hours) late:\n\nflights |&gt;\n  filter(dep_delay &gt; 120)\n\n# A tibble: 9,723 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      848           1835       853     1001           1950\n 2  2013     1     1      957            733       144     1056            853\n 3  2013     1     1     1114            900       134     1447           1222\n 4  2013     1     1     1540           1338       122     2020           1825\n 5  2013     1     1     1815           1325       290     2120           1542\n 6  2013     1     1     1842           1422       260     1958           1535\n 7  2013     1     1     1856           1645       131     2212           2005\n 8  2013     1     1     1934           1725       129     2126           1855\n 9  2013     1     1     1938           1703       155     2109           1823\n10  2013     1     1     1942           1705       157     2124           1830\n# ℹ 9,713 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Flights that departed on January 1\nflights |&gt; \n  filter(month == 1 & day == 1)\n\n# A tibble: 842 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 832 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Flights that departed in January or February\nflights |&gt; \n  filter(month == 1 | month == 2)\n\n# A tibble: 51,955 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 51,945 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# A shorter way to select flights that departed in January or February\nflights |&gt; \n  filter(month %in% c(1, 2))\n\n# A tibble: 51,955 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 51,945 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\narrange() changes the order of the rows based on the value of the columns. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns. For example, the following code sorts by the departure time, which is spread over four columns. We get the earliest years first, then within a year the earliest months, etc.\n\nflights |&gt; \n  arrange(year, month, day, dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nYou can use desc() on a column inside of arrange() to re-order the data frame based on that column in descending (big-to-small) order. For example, this code orders flights from most to least delayed:\n\nflights |&gt;\n  arrange(desc(dep_delay))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\ndistinct() finds all the unique rows in a dataset, so in a technical sense, it primarily operates on the rows. Most of the time, however, you’ll want the distinct combination of some variables, so you can also optionally supply column names:\n\n# Remove duplicate rows, if any\nflights |&gt; \n  distinct()\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Find all unique origin and destination pairs\nflights |&gt; \n  distinct(origin, dest)\n\n# A tibble: 224 × 2\n   origin dest \n   &lt;chr&gt;  &lt;chr&gt;\n 1 EWR    IAH  \n 2 LGA    IAH  \n 3 JFK    MIA  \n 4 JFK    BQN  \n 5 LGA    ATL  \n 6 EWR    ORD  \n 7 EWR    FLL  \n 8 LGA    IAD  \n 9 JFK    MCO  \n10 LGA    ORD  \n# ℹ 214 more rows\n\n\nAlternatively, if you want to the keep other columns when filtering for unique rows, you can use the .keep_all = TRUE option.\n\nflights |&gt; \n  distinct(origin, dest, .keep_all = TRUE)\n\n# A tibble: 224 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 214 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIt’s not a coincidence that all of these distinct flights are on January 1: distinct() will find the first occurrence of a unique row in the dataset and discard the rest.\nIf you want to find the number of occurrences instead, you’re better off swapping distinct() for count(), and with the sort = TRUE argument you can arrange them in descending order of number of occurrences. \n\nflights |&gt;\n  count(origin, dest, sort = TRUE)\n\n# A tibble: 224 × 3\n   origin dest      n\n   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n 1 JFK    LAX   11262\n 2 LGA    ATL   10263\n 3 LGA    ORD    8857\n 4 JFK    SFO    8204\n 5 LGA    CLT    6168\n 6 EWR    ORD    6100\n 7 JFK    BOS    5898\n 8 LGA    MIA    5781\n 9 JFK    MCO    5464\n10 EWR    BOS    5327\n# ℹ 214 more rows\n\n\n\n\n\n\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\n\n\n\nflights |&gt; \n  filter(arr_delay &gt;= 2)\n\n# A tibble: 127,929 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            558        -4      740            728\n 5  2013     1     1      555            600        -5      913            854\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      559            600        -1      941            910\n 9  2013     1     1      600            600         0      837            825\n10  2013     1     1      602            605        -3      821            805\n# ℹ 127,919 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Flew to Houston (`IAH` or `HOU`)\n\nflights |&gt; \n  filter(dest %in% c(\"IAH\",\"HOU\"))\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Were operated by United, American, or Delta\n\nflights |&gt; \n  filter(carrier %in% c(\"UA\",\"DL\"))\n\n# A tibble: 106,775 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      554            600        -6      812            837\n 4  2013     1     1      554            558        -4      740            728\n 5  2013     1     1      558            600        -2      924            917\n 6  2013     1     1      558            600        -2      923            937\n 7  2013     1     1      559            600        -1      854            902\n 8  2013     1     1      602            610        -8      812            820\n 9  2013     1     1      606            610        -4      837            845\n10  2013     1     1      607            607         0      858            915\n# ℹ 106,765 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Departed in summer (July, August, and September)\n\nflights |&gt; \n  filter(month %in% c(\"7\",\"8\", \"9\"))\n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Arrived more than two hours late, but didn’t leave late\n\nflights |&gt; \n  filter(arr_delay &gt; 2 & dep_delay == 0)\n\n# A tibble: 4,368 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      600            600         0      837            825\n 2  2013     1     1      635            635         0     1028            940\n 3  2013     1     1      739            739         0     1104           1038\n 4  2013     1     1      745            745         0     1135           1125\n 5  2013     1     1      800            800         0     1022           1014\n 6  2013     1     1      805            805         0     1015           1005\n 7  2013     1     1      810            810         0     1048           1037\n 8  2013     1     1      823            823         0     1151           1135\n 9  2013     1     1      830            830         0     1018           1015\n10  2013     1     1      835            835         0     1210           1150\n# ℹ 4,358 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n-   Were delayed by at least an hour, but made up over 30 minutes in flight.\n\nflights |&gt; \n  filter(dep_delay &gt; 1 & (dep_delay - arr_delay) &gt; 30 )\n\n# A tibble: 7,474 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      857            851         6     1157           1222\n 2  2013     1     1      909            810        59     1331           1315\n 3  2013     1     1     1025            951        34     1258           1302\n 4  2013     1     1     1625           1550        35     2054           2050\n 5  2013     1     1     1957           1945        12     2307           2329\n 6  2013     1     1     2035           2030         5     2337              5\n 7  2013     1     1     2046           2035        11     2144           2213\n 8  2013     1     1     2107           2040        27     2354           2359\n 9  2013     1     1     2205           1720       285       46           2040\n10  2013     1     1     2326           2130       116      131             18\n# ℹ 7,464 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nSort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\n\n\nflights |&gt; \n  arrange(desc(dep_delay))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nSort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)\n\n\nflights |&gt; \n  arrange(desc(speed = distance/(hour+ minute/60)))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      857            900        -3     1516           1530\n 2  2013     1     2      909            900         9     1525           1530\n 3  2013     1     3      914            900        14     1504           1530\n 4  2013     1     4      900            900         0     1516           1530\n 5  2013     1     5      858            900        -2     1519           1530\n 6  2013     1     6     1019            900        79     1558           1530\n 7  2013     1     7     1042            900       102     1620           1530\n 8  2013     1     8      901            900         1     1504           1530\n 9  2013     1     9      641            900      1301     1242           1530\n10  2013     1    10      859            900        -1     1449           1530\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nWas there a flight on every day of 2013? Yes.\n\n\nflights |&gt; \n  distinct(year, month,day)\n\n# A tibble: 365 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     2\n 3  2013     1     3\n 4  2013     1     4\n 5  2013     1     5\n 6  2013     1     6\n 7  2013     1     7\n 8  2013     1     8\n 9  2013     1     9\n10  2013     1    10\n# ℹ 355 more rows\n\n\n\nWhich flights traveled the farthest distance? Which traveled the least distance?\n\nJFK-HNL = furthest\nEWR-LGA - nearest\n\n\n\nflights |&gt; \n  arrange(distance)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7    27       NA            106        NA       NA            245\n 2  2013     1     3     2127           2129        -2     2222           2224\n 3  2013     1     4     1240           1200        40     1333           1306\n 4  2013     1     4     1829           1615       134     1937           1721\n 5  2013     1     4     2128           2129        -1     2218           2224\n 6  2013     1     5     1155           1200        -5     1241           1306\n 7  2013     1     6     2125           2129        -4     2224           2224\n 8  2013     1     7     2124           2129        -5     2212           2224\n 9  2013     1     8     2127           2130        -3     2304           2225\n10  2013     1     9     2126           2129        -3     2217           2224\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nDoes it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.\n\nfilter() should be used first as it will narrow down the number of rows that arrange() need to sort."
  },
  {
    "objectID": "Self-Practice/Self-Practice_02/Self-Practice_02.html#columns",
    "href": "Self-Practice/Self-Practice_02/Self-Practice_02.html#columns",
    "title": "Self Practice 2: R for Data Science",
    "section": "",
    "text": "There are four important verbs that affect the columns without changing the rows: mutate() creates new columns that are derived from the existing columns, select() changes which columns are present, rename() changes the names of the columns, and relocate() changes the positions of the columns.\n\n\nThe job of mutate() is to add new columns that are calculated from the existing columns. In the transform chapters, you’ll learn a large set of functions that you can use to manipulate different types of variables. For now, we’ll stick with basic algebra, which allows us to compute the gain, how much time a delayed flight made up in the air, and the speed in miles per hour:\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .before = 1\n  )\n\n# A tibble: 336,776 × 21\n    gain speed  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1    -9  370.  2013     1     1      517            515         2      830\n 2   -16  374.  2013     1     1      533            529         4      850\n 3   -31  408.  2013     1     1      542            540         2      923\n 4    17  517.  2013     1     1      544            545        -1     1004\n 5    19  394.  2013     1     1      554            600        -6      812\n 6   -16  288.  2013     1     1      554            558        -4      740\n 7   -24  404.  2013     1     1      555            600        -5      913\n 8    11  259.  2013     1     1      557            600        -3      709\n 9     5  405.  2013     1     1      557            600        -3      838\n10   -10  319.  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThe . is a sign that .before is an argument to the function, not the name of a third new variable we are creating. You can also use .after to add after a variable, and in both .before and .after you can use the variable name instead of a position. For example, we could add the new variables after day:\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .after = day\n  )\n\n# A tibble: 336,776 × 21\n    year month   day  gain speed dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1  2013     1     1    -9  370.      517            515         2      830\n 2  2013     1     1   -16  374.      533            529         4      850\n 3  2013     1     1   -31  408.      542            540         2      923\n 4  2013     1     1    17  517.      544            545        -1     1004\n 5  2013     1     1    19  394.      554            600        -6      812\n 6  2013     1     1   -16  288.      554            558        -4      740\n 7  2013     1     1   -24  404.      555            600        -5      913\n 8  2013     1     1    11  259.      557            600        -3      709\n 9  2013     1     1     5  405.      557            600        -3      838\n10  2013     1     1   -10  319.      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nNote that since we haven’t assigned the result of the above computation back to flights, the new variables gain, hours, and gain_per_hour will only be printed but will not be stored in a data frame. And if we want them to be available in a data frame for future use, we should think carefully about whether we want the result to be assigned back to flights, overwriting the original data frame with many more variables, or to a new object. Often, the right answer is a new object that is named informatively to indicate its contents, e.g., delay_gain, but you might also have good reasons for overwriting flights.\n\n\n\nIt’s not uncommon to get datasets with hundreds or even thousands of variables. In this situation, the first challenge is often just focusing on the variables you’re interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables:\n\nSelect columns by name:\n\nflights |&gt;\n  select(year, month, day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows\n\n\nSelect all columns between year and day (inclusive):\n\nflights |&gt;\n  select(year:day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows\n\n\nSelect all columns except those from year to day (inclusive):\n\nflights |&gt;\n  select(!year:day)\n\n# A tibble: 336,776 × 16\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;  \n 1      517            515         2      830            819        11 UA     \n 2      533            529         4      850            830        20 UA     \n 3      542            540         2      923            850        33 AA     \n 4      544            545        -1     1004           1022       -18 B6     \n 5      554            600        -6      812            837       -25 DL     \n 6      554            558        -4      740            728        12 UA     \n 7      555            600        -5      913            854        19 B6     \n 8      557            600        -3      709            723       -14 EV     \n 9      557            600        -3      838            846        -8 B6     \n10      558            600        -2      753            745         8 AA     \n# ℹ 336,766 more rows\n# ℹ 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;,\n#   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nHistorically this operation was done with - instead of !, so you’re likely to see that in the wild. These two operators serve the same purpose but with subtle differences in behavior. We recommend using ! because it reads as “not” and combines well with & and |.\nSelect all columns that are characters:\n\nflights |&gt;\n  select(where(is.character))\n\n# A tibble: 336,776 × 4\n   carrier tailnum origin dest \n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;\n 1 UA      N14228  EWR    IAH  \n 2 UA      N24211  LGA    IAH  \n 3 AA      N619AA  JFK    MIA  \n 4 B6      N804JB  JFK    BQN  \n 5 DL      N668DN  LGA    ATL  \n 6 UA      N39463  EWR    ORD  \n 7 B6      N516JB  EWR    FLL  \n 8 EV      N829AS  LGA    IAD  \n 9 B6      N593JB  JFK    MCO  \n10 AA      N3ALAA  LGA    ORD  \n# ℹ 336,766 more rows\n\n\n\nThere are a number of helper functions you can use within select():\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\nends_with(\"xyz\"): matches names that end with “xyz”.\ncontains(\"ijk\"): matches names that contain “ijk”.\nnum_range(\"x\", 1:3): matches x1, x2 and x3.\n\nSee ?select for more details. Once you know regular expressions (the topic of Chapter 15) you’ll also be able to use matches() to select variables that match a pattern.\nYou can rename variables as you select() them by using =. The new name appears on the left hand side of the =, and the old variable appears on the right hand side:\n\nflights |&gt; \n  select(tail_num = tailnum)\n\n# A tibble: 336,776 × 1\n   tail_num\n   &lt;chr&gt;   \n 1 N14228  \n 2 N24211  \n 3 N619AA  \n 4 N804JB  \n 5 N668DN  \n 6 N39463  \n 7 N516JB  \n 8 N829AS  \n 9 N593JB  \n10 N3ALAA  \n# ℹ 336,766 more rows\n\n\n\n\n\nIf you want to keep all the existing variables and just want to rename a few, you can use rename() instead of select():\n\nflights |&gt; \n  rename(tail_num = tailnum)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tail_num &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIf you have a bunch of inconsistently named columns and it would be painful to fix them all by hand, check out janitor::clean_names() which provides some useful automated cleaning.\n\n\n\nUse relocate() to move variables around. You might want to collect related variables together or move important variables to the front. By default relocate() moves variables to the front:\n\nflights |&gt; \n  relocate(time_hour, air_time)\n\n# A tibble: 336,776 × 19\n   time_hour           air_time  year month   day dep_time sched_dep_time\n   &lt;dttm&gt;                 &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n 1 2013-01-01 05:00:00      227  2013     1     1      517            515\n 2 2013-01-01 05:00:00      227  2013     1     1      533            529\n 3 2013-01-01 05:00:00      160  2013     1     1      542            540\n 4 2013-01-01 05:00:00      183  2013     1     1      544            545\n 5 2013-01-01 06:00:00      116  2013     1     1      554            600\n 6 2013-01-01 05:00:00      150  2013     1     1      554            558\n 7 2013-01-01 06:00:00      158  2013     1     1      555            600\n 8 2013-01-01 06:00:00       53  2013     1     1      557            600\n 9 2013-01-01 06:00:00      140  2013     1     1      557            600\n10 2013-01-01 06:00:00      138  2013     1     1      558            600\n# ℹ 336,766 more rows\n# ℹ 12 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n#   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;,\n#   dest &lt;chr&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;\n\n\nYou can also specify where to put them using the .before and .after arguments, just like in mutate():\n\nflights |&gt; \n  relocate(year:dep_time, .after = time_hour)\n\n# A tibble: 336,776 × 19\n   sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight\n            &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1            515         2      830            819        11 UA        1545\n 2            529         4      850            830        20 UA        1714\n 3            540         2      923            850        33 AA        1141\n 4            545        -1     1004           1022       -18 B6         725\n 5            600        -6      812            837       -25 DL         461\n 6            558        -4      740            728        12 UA        1696\n 7            600        -5      913            854        19 B6         507\n 8            600        -3      709            723       -14 EV        5708\n 9            600        -3      838            846        -8 B6          79\n10            600        -2      753            745         8 AA         301\n# ℹ 336,766 more rows\n# ℹ 12 more variables: tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, year &lt;int&gt;,\n#   month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;\n\n\n\nflights |&gt; \n  relocate(starts_with(\"arr\"), .before = dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day arr_time arr_delay dep_time sched_dep_time dep_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     1      830        11      517            515         2\n 2  2013     1     1      850        20      533            529         4\n 3  2013     1     1      923        33      542            540         2\n 4  2013     1     1     1004       -18      544            545        -1\n 5  2013     1     1      812       -25      554            600        -6\n 6  2013     1     1      740        12      554            558        -4\n 7  2013     1     1      913        19      555            600        -5\n 8  2013     1     1      709       -14      557            600        -3\n 9  2013     1     1      838        -8      557            600        -3\n10  2013     1     1      753         8      558            600        -2\n# ℹ 336,766 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\ndep_delay = dep_time - sched_dep_time\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n\nflights |&gt;\n  select(contains(c(\"dep\", \"time\", \"arr\", \"delay\")))\n\n# A tibble: 336,776 × 9\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time air_time\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt;\n 1      517            515         2      830            819      227\n 2      533            529         4      850            830      227\n 3      542            540         2      923            850      160\n 4      544            545        -1     1004           1022      183\n 5      554            600        -6      812            837      116\n 6      554            558        -4      740            728      150\n 7      555            600        -5      913            854      158\n 8      557            600        -3      709            723       53\n 9      557            600        -3      838            846      140\n10      558            600        -2      753            745      138\n# ℹ 336,766 more rows\n# ℹ 3 more variables: time_hour &lt;dttm&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;\n\n\n\nWhat happens if you specify the name of the same variable multiple times in a select() call?\n\n\nflights |&gt;\n  select(arr_time,arr_time)\n\n# A tibble: 336,776 × 1\n   arr_time\n      &lt;int&gt;\n 1      830\n 2      850\n 3      923\n 4     1004\n 5      812\n 6      740\n 7      913\n 8      709\n 9      838\n10      753\n# ℹ 336,766 more rows\n\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\n\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\nflights |&gt;\n  select(any_of(variables))\n\n# A tibble: 336,776 × 5\n    year month   day dep_delay arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2013     1     1         2        11\n 2  2013     1     1         4        20\n 3  2013     1     1         2        33\n 4  2013     1     1        -1       -18\n 5  2013     1     1        -6       -25\n 6  2013     1     1        -4        12\n 7  2013     1     1        -5        19\n 8  2013     1     1        -3       -14\n 9  2013     1     1        -3        -8\n10  2013     1     1        -2         8\n# ℹ 336,766 more rows\n\n\n\nflights |&gt; \n  select(contains(\"TIME\"))\n\n# A tibble: 336,776 × 6\n   dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n 1      517            515      830            819      227 2013-01-01 05:00:00\n 2      533            529      850            830      227 2013-01-01 05:00:00\n 3      542            540      923            850      160 2013-01-01 05:00:00\n 4      544            545     1004           1022      183 2013-01-01 05:00:00\n 5      554            600      812            837      116 2013-01-01 06:00:00\n 6      554            558      740            728      150 2013-01-01 05:00:00\n 7      555            600      913            854      158 2013-01-01 06:00:00\n 8      557            600      709            723       53 2013-01-01 06:00:00\n 9      557            600      838            846      140 2013-01-01 06:00:00\n10      558            600      753            745      138 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n\n\nflights |&gt; \n  mutate(air_time_min = air_time,\n         .before = 1)\n\n# A tibble: 336,776 × 20\n   air_time_min  year month   day dep_time sched_dep_time dep_delay arr_time\n          &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1          227  2013     1     1      517            515         2      830\n 2          227  2013     1     1      533            529         4      850\n 3          160  2013     1     1      542            540         2      923\n 4          183  2013     1     1      544            545        -1     1004\n 5          116  2013     1     1      554            600        -6      812\n 6          150  2013     1     1      554            558        -4      740\n 7          158  2013     1     1      555            600        -5      913\n 8           53  2013     1     1      557            600        -3      709\n 9          140  2013     1     1      557            600        -3      838\n10          138  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nWhy doesn’t the following work, and what does the error mean?\n\nafter selecting the column tailnum, there is no more column called arr_delay.\n\n\n\nflights |&gt; \n  select(tailnum) |&gt; \n  arrange(arr_delay)\n#&gt; Error in `arrange()`:\n#&gt; ℹ In argument: `..1 = arr_delay`.\n#&gt; Caused by error:\n#&gt; ! object 'arr_delay' not found"
  },
  {
    "objectID": "Self-Practice/Self-Practice_02/Self-Practice_02.html#the-pipe",
    "href": "Self-Practice/Self-Practice_02/Self-Practice_02.html#the-pipe",
    "title": "Self Practice 2: R for Data Science",
    "section": "",
    "text": "We’ve shown you simple examples of the pipe above, but its real power arises when you start to combine multiple verbs. For example, imagine that you wanted to find the fastest flights to Houston’s IAH airport: you need to combine filter(), mutate(), select(), and arrange():\n\nflights |&gt; \n  filter(dest == \"IAH\", ) |&gt; \n  mutate(speed = distance / air_time*60) |&gt; \n  select(year:day, dep_time, carrier, flight, speed) |&gt; \n  arrange(desc(speed))\n\n# A tibble: 7,198 × 7\n    year month   day dep_time carrier flight speed\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n 1  2013     7     9      707 UA         226  522.\n 2  2013     8    27     1850 UA        1128  521.\n 3  2013     8    28      902 UA        1711  519.\n 4  2013     8    28     2122 UA        1022  519.\n 5  2013     6    11     1628 UA        1178  515.\n 6  2013     8    27     1017 UA         333  515.\n 7  2013     8    27     1205 UA        1421  515.\n 8  2013     8    27     1758 UA         302  515.\n 9  2013     9    27      521 UA         252  515.\n10  2013     8    28      625 UA         559  515.\n# ℹ 7,188 more rows\n\n\n\n\nUse group_by() to divide your dataset into groups meaningful for your analysis:\n\nflights |&gt; \n  group_by(month)\n\n# A tibble: 336,776 × 19\n# Groups:   month [12]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\ngroup_by() doesn’t change the data but, if you look closely at the output, you’ll notice that the output indicates that it is “grouped by” month (Groups: month [12]). This means subsequent operations will now work “by month”. group_by() adds this grouped feature (referred to as class) to the data frame, which changes the behavior of the subsequent verbs applied to the data.\n\n\n\nThe most important grouped operation is a summary, which, if being used to calculate a single summary statistic, reduces the data frame to have a single row for each group. In dplyr, this operation is performed by summarize()3, as shown by the following example, which computes the average departure delay by month:\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarise(\n    avg_delay = mean(dep_delay)\n  )\n\n# A tibble: 12 × 2\n   month avg_delay\n   &lt;int&gt;     &lt;dbl&gt;\n 1     1        NA\n 2     2        NA\n 3     3        NA\n 4     4        NA\n 5     5        NA\n 6     6        NA\n 7     7        NA\n 8     8        NA\n 9     9        NA\n10    10        NA\n11    11        NA\n12    12        NA\n\n\nUhoh! Something has gone wrong and all of our results are NAs (pronounced “N-A”), R’s symbol for missing value. This happened because some of the observed flights had missing data in the delay column, and so when we calculated the mean including those values, we got an NA result. We’ll come back to discuss missing values in detail in Chapter 18, but for now we’ll tell the mean() function to ignore all missing values by setting the argument na.rm to TRUE.\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarise(\n    avg_delay = mean(dep_delay, na.rm = TRUE)\n  )\n\n# A tibble: 12 × 2\n   month avg_delay\n   &lt;int&gt;     &lt;dbl&gt;\n 1     1     10.0 \n 2     2     10.8 \n 3     3     13.2 \n 4     4     13.9 \n 5     5     13.0 \n 6     6     20.8 \n 7     7     21.7 \n 8     8     12.6 \n 9     9      6.72\n10    10      6.24\n11    11      5.44\n12    12     16.6 \n\n\nYou can create any number of summaries in a single call to summarize(). You’ll learn various useful summaries in the upcoming chapters, but one very useful summary is n(), which returns the number of rows in each group:\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarise(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\n# A tibble: 12 × 3\n   month avg_delay     n\n   &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;\n 1     1     10.0  27004\n 2     2     10.8  24951\n 3     3     13.2  28834\n 4     4     13.9  28330\n 5     5     13.0  28796\n 6     6     20.8  28243\n 7     7     21.7  29425\n 8     8     12.6  29327\n 9     9      6.72 27574\n10    10      6.24 28889\n11    11      5.44 27268\n12    12     16.6  28135\n\n\nMeans and counts can get you a surprisingly long way in data science!\n\n\n\nThere are five handy functions that allow you extract specific rows within each group:\n\ndf |&gt; slice_head(n = 1) takes the first row from each group.\ndf |&gt; slice_tail(n = 1) takes the last row in each group.\ndf |&gt; slice_min(x, n = 1) takes the row with the smallest value of column x.\ndf |&gt; slice_max(x, n = 1) takes the row with the largest value of column x.\ndf |&gt; slice_sample(n = 1) takes one random row.\n\nYou can vary n to select more than one row, or instead of n =, you can use prop = 0.1 to select (e.g.) 10% of the rows in each group. For example, the following code finds the flights that are most delayed upon arrival at each destination:\n\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1,with_ties = FALSE) |&gt;\n  relocate(dest) |&gt; \n  arrange(desc(arr_delay))\n\n# A tibble: 105 × 19\n# Groups:   dest [105]\n   dest   year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1 HNL    2013     1     9      641            900      1301     1242\n 2 CMH    2013     6    15     1432           1935      1137     1607\n 3 ORD    2013     1    10     1121           1635      1126     1239\n 4 SFO    2013     9    20     1139           1845      1014     1457\n 5 CVG    2013     7    22      845           1600      1005     1044\n 6 TPA    2013     4    10     1100           1900       960     1342\n 7 MSP    2013     3    17     2321            810       911      135\n 8 ATL    2013     7    22     2257            759       898      121\n 9 MIA    2013    12     5      756           1700       896     1058\n10 LAS    2013     5    19      713           1700       853     1007\n# ℹ 95 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nNote that there are 105 destinations but we get 108 rows here. What’s up? slice_min() and slice_max() keep tied values so n = 1 means give us all rows with the highest value. If you want exactly one row per group you can set with_ties = FALSE.\n\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1) |&gt;\n  relocate(dest) |&gt; \n  arrange(desc(arr_delay))\n\n# A tibble: 108 × 19\n# Groups:   dest [105]\n   dest   year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1 HNL    2013     1     9      641            900      1301     1242\n 2 CMH    2013     6    15     1432           1935      1137     1607\n 3 ORD    2013     1    10     1121           1635      1126     1239\n 4 SFO    2013     9    20     1139           1845      1014     1457\n 5 CVG    2013     7    22      845           1600      1005     1044\n 6 TPA    2013     4    10     1100           1900       960     1342\n 7 MSP    2013     3    17     2321            810       911      135\n 8 ATL    2013     7    22     2257            759       898      121\n 9 MIA    2013    12     5      756           1700       896     1058\n10 LAS    2013     5    19      713           1700       853     1007\n# ℹ 98 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThis is similar to computing the max delay with summarize(), but you get the whole corresponding row (or rows if there’s a tie) instead of the single summary statistic.\n\n\n\nYou can create groups using more than one variable. For example, we could make a group for each date.\n\ndaily &lt;- flights |&gt;  \n  group_by(year, month, day)\ndaily\n\n# A tibble: 336,776 × 19\n# Groups:   year, month, day [365]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIf you’re happy with this behavior, you can explicitly request it in order to suppress the message:\n\ndaily_flights &lt;- daily |&gt; \n  summarize(\n    n = n(), \n    .groups = \"drop_last\"\n  )\ndaily_flights\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1   842\n 2  2013     1     2   943\n 3  2013     1     3   914\n 4  2013     1     4   915\n 5  2013     1     5   720\n 6  2013     1     6   832\n 7  2013     1     7   933\n 8  2013     1     8   899\n 9  2013     1     9   902\n10  2013     1    10   932\n# ℹ 355 more rows\n\n\nAlternatively, change the default behavior by setting a different value, e.g., “drop” to drop all grouping or “keep” to preserve the same groups.\n\ndaily_flights &lt;- daily |&gt; \n  summarize(\n    n = n(), \n    .groups = \"drop\"\n  )\ndaily_flights\n\n# A tibble: 365 × 4\n    year month   day     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1   842\n 2  2013     1     2   943\n 3  2013     1     3   914\n 4  2013     1     4   915\n 5  2013     1     5   720\n 6  2013     1     6   832\n 7  2013     1     7   933\n 8  2013     1     8   899\n 9  2013     1     9   902\n10  2013     1    10   932\n# ℹ 355 more rows\n\n\n\n\n\nYou might also want to remove grouping from a data frame without using summarize(). You can do this with ungroup().\n\ndaily |&gt; \n  ungroup()\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nNow let’s see what happens when you summarize an ungrouped data frame.\n\ndaily |&gt; \n  ungroup() |&gt;\n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    flights = n()\n  )\n\n# A tibble: 1 × 2\n  avg_delay flights\n      &lt;dbl&gt;   &lt;int&gt;\n1      12.6  336776\n\n\nYou get a single row back because dplyr treats all the rows in an ungrouped data frame as belonging to one group.\n\n\n\ndplyr 1.1.0 includes a new, experimental, syntax for per-operation grouping, the .by argument. group_by() and ungroup() aren’t going away, but you can now also use the .by argument to group within a single operation:\n\nflights |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = month\n  ) |&gt; \n  arrange(month)\n\n# A tibble: 12 × 3\n   month delay     n\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1     1 10.0  27004\n 2     2 10.8  24951\n 3     3 13.2  28834\n 4     4 13.9  28330\n 5     5 13.0  28796\n 6     6 20.8  28243\n 7     7 21.7  29425\n 8     8 12.6  29327\n 9     9  6.72 27574\n10    10  6.24 28889\n11    11  5.44 27268\n12    12 16.6  28135\n\n\nOr if you want to group by multiple variables:\n\nflights |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = c(origin, dest)\n  )\n\n# A tibble: 224 × 4\n   origin dest  delay     n\n   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n 1 EWR    IAH   11.8   3973\n 2 LGA    IAH    9.06  2951\n 3 JFK    MIA    9.34  3314\n 4 JFK    BQN    6.67   599\n 5 LGA    ATL   11.4  10263\n 6 EWR    ORD   14.6   6100\n 7 EWR    FLL   13.5   3793\n 8 LGA    IAD   16.7   1803\n 9 JFK    MCO   10.6   5464\n10 LGA    ORD   10.7   8857\n# ℹ 214 more rows\n\n\n\n\n\n\nWhich carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarize(n()))\n\n\nflights |&gt; \n  group_by(carrier, dest) |&gt; \n  summarise(n())\n\n# A tibble: 314 × 3\n# Groups:   carrier [16]\n   carrier dest  `n()`\n   &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n 1 9E      ATL      59\n 2 9E      AUS       2\n 3 9E      AVL      10\n 4 9E      BGR       1\n 5 9E      BNA     474\n 6 9E      BOS     914\n 7 9E      BTV       2\n 8 9E      BUF     833\n 9 9E      BWI     856\n10 9E      CAE       3\n# ℹ 304 more rows\n\n\n\nFind the flights that are most delayed upon departure from each destination.\n\n\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(dep_delay) |&gt; \n  arrange(desc(dep_delay))\n\n# A tibble: 105 × 19\n# Groups:   dest [105]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 95 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nHow do delays vary over the course of the day. Illustrate your answer with a plot.\nWhat happens if you supply a negative n to slice_min() and friends?\nExplain what count() does in terms of the dplyr verbs you just learned. What does the sort argument to count() do?\n\n\ndf &lt;- tibble(\n  x = 1:5,\n  y = c(\"a\", \"b\", \"a\", \"a\", \"b\"),\n  z = c(\"K\", \"K\", \"L\", \"L\", \"K\")\n)\n\n\ndf |&gt;\n  group_by(y)\n\n# A tibble: 5 × 3\n# Groups:   y [2]\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     2 b     K    \n3     3 a     L    \n4     4 a     L    \n5     5 b     K    \n\n\n\ndf |&gt;\n  arrange(y)\n\n# A tibble: 5 × 3\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     3 a     L    \n3     4 a     L    \n4     2 b     K    \n5     5 b     K    \n\n\n\ndf |&gt;\n  group_by(y) |&gt;\n  summarize(mean_x = mean(x))\n\n# A tibble: 2 × 2\n  y     mean_x\n  &lt;chr&gt;  &lt;dbl&gt;\n1 a       2.67\n2 b       3.5 \n\n\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n# A tibble: 3 × 3\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  mutate(mean_x = mean(x))\n\n# A tibble: 5 × 4\n# Groups:   y, z [3]\n      x y     z     mean_x\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 a     K        1  \n2     2 b     K        3.5\n3     3 a     L        3.5\n4     4 a     L        3.5\n5     5 b     K        3.5"
  },
  {
    "objectID": "Self-Practice/Self-Practice_02/Self-Practice_02.html#case-study-aggregates-and-sample-size",
    "href": "Self-Practice/Self-Practice_02/Self-Practice_02.html#case-study-aggregates-and-sample-size",
    "title": "Self Practice 2: R for Data Science",
    "section": "",
    "text": "Whenever you do any aggregation, it’s always a good idea to include a count (n()). That way, you can ensure that you’re not drawing conclusions based on very small amounts of data. We’ll demonstrate this with some baseball data from the Lahman package. Specifically, we will compare what proportion of times a player gets a hit (H) vs. the number of times they try to put the ball in play (AB):\n\nbatters &lt;- Lahman::Batting |&gt; \n  group_by(playerID) |&gt; \n  summarize(\n    performance = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),\n    n = sum(AB, na.rm = TRUE)\n  )\nbatters\n\n# A tibble: 20,469 × 3\n   playerID  performance     n\n   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;\n 1 aardsda01      0          4\n 2 aaronha01      0.305  12364\n 3 aaronto01      0.229    944\n 4 aasedo01       0          5\n 5 abadan01       0.0952    21\n 6 abadfe01       0.111      9\n 7 abadijo01      0.224     49\n 8 abbated01      0.254   3044\n 9 abbeybe01      0.169    225\n10 abbeych01      0.281   1756\n# ℹ 20,459 more rows\n\n\n\nbatters |&gt; \n  filter(n &gt; 100) |&gt; \n  ggplot(aes(x = n, y = performance)) +\n  geom_point(alpha = 1 / 10) + \n  geom_smooth(se = FALSE)"
  },
  {
    "objectID": "Self-Practice/Self-Practice_04/Self-Practice_04.html",
    "href": "Self-Practice/Self-Practice_04/Self-Practice_04.html",
    "title": "Self Practice 4: R for Data Science",
    "section": "",
    "text": "Working with data provided by R packages is a great way to learn data science tools, but you want to apply what you’ve learned to your own data at some point. In this chapter, you’ll learn the basics of reading data files into R.\nSpecifically, this chapter will focus on reading plain-text rectangular files. We’ll start with practical advice for handling features like column names, types, and missing data. You will then learn about reading data from multiple files at once and writing data from R to a file. Finally, you’ll learn how to handcraft data frames in R.\n\n\nIn this chapter, you’ll learn how to load flat files in R with the readr package, which is part of the core tidyverse.\n\nlibrary(tidyverse)\n\n\n\n\n\nTo begin, we’ll focus on the most common rectangular data file type: CSV, which is short for comma-separated values. Here is what a simple CSV file looks like. The first row, commonly called the header row, gives the column names, and the following six rows provide the data. The columns are separated, aka delimited, by commas.\n\nstudents &lt;- read_csv(\"data/students.csv\")\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n\nstudents &lt;- read_csv(\"https://pos.it/r4ds-students-csv\")\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n\n\nOnce you read data in, the first step usually involves transforming it in some way to make it easier to work with in the rest of your analysis. Let’s take another look at the students data with that in mind.\n\nstudents &lt;- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\nAn alternative approach is to use janitor::clean_names() to use some heuristics to turn them all into snake case at once.\n\nstudents |&gt; janitor::clean_names()\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan           age  \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2          2 Barclay Lynn     French fries       Lunch only          5    \n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6          6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\nAnother common task after reading in data is to consider variable types. For example, meal_plan is a categorical variable with a known set of possible values, which in R should be represented as a factor:\n\nstudents |&gt;\n  janitor::clean_names() |&gt;\n  mutate(meal_plan = factor(meal_plan))\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan           age  \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;chr&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2          2 Barclay Lynn     French fries       Lunch only          5    \n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6          6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\nNote that the values in the meal_plan variable have stayed the same, but the type of variable denoted underneath the variable name has changed from character (&lt;chr&gt;) to factor (&lt;fct&gt;). You’ll learn more about factors in Chapter 16.\nBefore you analyze these data, you’ll probably want to fix the age and id columns. Currently, age is a character variable because one of the observations is typed out as five instead of a numeric 5. We discuss the details of fixing this issue in Chapter 20.\n\nstudents &lt;- students |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    meal_plan = factor(meal_plan),\n    age = parse_number(if_else(age == \"five\", \"5\", age))\n  )\n\nstudents\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\n\n\nThere are a couple of other important arguments that we need to mention, and they’ll be easier to demonstrate if we first show you a handy trick: read_csv() can read text strings that you’ve created and formatted like a CSV file:\n\nread_csv(\n  \"a,b,c\n  1,2,3\n  4,5,6\"\n)\n\n# A tibble: 2 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n2     4     5     6\n\n\nUsually, read_csv() uses the first line of the data for the column names, which is a very common convention. But it’s not uncommon for a few lines of metadata to be included at the top of the file. You can use skip = n to skip the first n lines or use comment = \"#\" to drop all lines that start with (e.g.) #:\n\nread_csv(\n  \"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\",\n  skip = 2\n)\n\n# A tibble: 1 × 3\n      x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n\n\n\nread_csv(\n  \"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\"\n)\n\n# A tibble: 1 × 3\n      x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n\n\nIn other cases, the data might not have column names. You can use col_names = FALSE to tell read_csv() not to treat the first row as headings and instead label them sequentially from X1 to Xn:\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = FALSE\n)\n\n# A tibble: 2 × 3\n     X1    X2    X3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n2     4     5     6\n\n\nAlternatively, you can pass col_names a character vector which will be used as the column names:\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = c(\"x\", \"y\", \"z\")\n)\n\n# A tibble: 2 × 3\n      x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n2     4     5     6\n\n\nThese arguments are all you need to know to read the majority of CSV files that you’ll encounter in practice. (For the rest, you’ll need to carefully inspect your .csv file and read the documentation for read_csv()’s many other arguments.)\n\n\n\nOnce you’ve mastered read_csv(), using readr’s other functions is straightforward; it’s just a matter of knowing which function to reach for:\n\nread_csv2() reads semicolon-separated files. These use ; instead of , to separate fields and are common in countries that use , as the decimal marker.\nread_tsv() reads tab-delimited files.\nread_delim() reads in files with any delimiter, attempting to automatically guess the delimiter if you don’t specify it.\nread_fwf() reads fixed-width files. You can specify fields by their widths with fwf_widths() or by their positions with fwf_positions().\nread_table() reads a common variation of fixed-width files where columns are separated by white space.\nread_log() reads Apache-style log files.\n\n\n\n\nreadr provides a total of nine column types for you to use:\n\ncol_logical() and col_double() read logicals and real numbers. They’re relatively rarely needed (except as above), since readr will usually guess them for you.\ncol_integer() reads integers. We seldom distinguish integers and doubles in this book because they’re functionally equivalent, but reading integers explicitly can occasionally be useful because they occupy half the memory of doubles.\ncol_character() reads strings. This can be useful to specify explicitly when you have a column that is a numeric identifier, i.e., long series of digits that identifies an object but doesn’t make sense to apply mathematical operations to. Examples include phone numbers, social security numbers, credit card numbers, etc.\ncol_factor(), col_date(), and col_datetime() create factors, dates, and date-times respectively; you’ll learn more about those when we get to those data types in Chapter 16 and Chapter 17.\ncol_number() is a permissive numeric parser that will ignore non-numeric components, and is particularly useful for currencies. You’ll learn more about it in Chapter 13.\ncol_skip() skips a column so it’s not included in the result, which can be useful for speeding up reading the data if you have a large CSV file and you only want to use some of the columns.\n\n\n\n\n\nSometimes your data is split across multiple files instead of being contained in a single file. For example, you might have sales data for multiple months, with each month’s data in a separate file: 01-sales.csv for January, 02-sales.csv for February, and 03-sales.csv for March. With read_csv() you can read these data in at once and stack them on top of each other in a single data frame.\n\nsales_files &lt;- c(\n  \"https://pos.it/r4ds-01-sales\",\n  \"https://pos.it/r4ds-02-sales\",\n  \"https://pos.it/r4ds-03-sales\"\n)\nread_csv(sales_files, id = \"file\")\n\n# A tibble: 19 × 6\n   file                         month     year brand  item     n\n   &lt;chr&gt;                        &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 https://pos.it/r4ds-01-sales January   2019     1  1234     3\n 2 https://pos.it/r4ds-01-sales January   2019     1  8721     9\n 3 https://pos.it/r4ds-01-sales January   2019     1  1822     2\n 4 https://pos.it/r4ds-01-sales January   2019     2  3333     1\n 5 https://pos.it/r4ds-01-sales January   2019     2  2156     9\n 6 https://pos.it/r4ds-01-sales January   2019     2  3987     6\n 7 https://pos.it/r4ds-01-sales January   2019     2  3827     6\n 8 https://pos.it/r4ds-02-sales February  2019     1  1234     8\n 9 https://pos.it/r4ds-02-sales February  2019     1  8721     2\n10 https://pos.it/r4ds-02-sales February  2019     1  1822     3\n11 https://pos.it/r4ds-02-sales February  2019     2  3333     1\n12 https://pos.it/r4ds-02-sales February  2019     2  2156     3\n13 https://pos.it/r4ds-02-sales February  2019     2  3987     6\n14 https://pos.it/r4ds-03-sales March     2019     1  1234     3\n15 https://pos.it/r4ds-03-sales March     2019     1  3627     1\n16 https://pos.it/r4ds-03-sales March     2019     1  8820     3\n17 https://pos.it/r4ds-03-sales March     2019     2  7253     1\n18 https://pos.it/r4ds-03-sales March     2019     2  8766     3\n19 https://pos.it/r4ds-03-sales March     2019     2  8288     6\n\n\nThe id argument adds a new column called file to the resulting data frame that identifies the file the data come from. This is especially helpful in circumstances where the files you’re reading in do not have an identifying column that can help you trace the observations back to their original sources.\nIf you have many files you want to read in, it can get cumbersome to write out their names as a list. Instead, you can use the base list.files() function to find the files for you by matching a pattern in the file names. You’ll learn more about these patterns in Chapter 15.\n\n\n\nreadr also comes with two useful functions for writing data back to disk: write_csv() and write_tsv(). The most important arguments to these functions are x (the data frame to save) and file (the location to save it). You can also specify how missing values are written with na, and if you want to append to an existing file.\n\nwrite_csv(students, \"data/students1.csv\")\n\nNow let’s read that csv file back in. Note that the variable type information that you just set up is lost when you save to CSV because you’re starting over with reading from a plain text file again:\n\nstudents &lt;- read_csv(\"data/students1.csv\")\nstudents\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\nThis makes CSVs a little unreliable for caching interim results—you need to recreate the column specification every time you load in. There are two main alternatives:\n\nwrite_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS. This means that when you reload the object, you are loading the exact same R object that you stored.\n\n\nstudents &lt;- students |&gt;\n  janitor::clean_names() |&gt;\n  mutate(meal_plan = factor(meal_plan))\n\n\nwrite_rds(students, \"data/students1.rds\")\n\n\nread_rds(\"data/students1.rds\")\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nThe arrow package allows you to read and write parquet files, a fast binary file format that can be shared across programming languages. We’ll return to arrow in more depth in Chapter 22."
  },
  {
    "objectID": "Self-Practice/Self-Practice_04/Self-Practice_04.html#introduction",
    "href": "Self-Practice/Self-Practice_04/Self-Practice_04.html#introduction",
    "title": "Self Practice 4: R for Data Science",
    "section": "",
    "text": "Working with data provided by R packages is a great way to learn data science tools, but you want to apply what you’ve learned to your own data at some point. In this chapter, you’ll learn the basics of reading data files into R.\nSpecifically, this chapter will focus on reading plain-text rectangular files. We’ll start with practical advice for handling features like column names, types, and missing data. You will then learn about reading data from multiple files at once and writing data from R to a file. Finally, you’ll learn how to handcraft data frames in R.\n\n\nIn this chapter, you’ll learn how to load flat files in R with the readr package, which is part of the core tidyverse.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "Self-Practice/Self-Practice_04/Self-Practice_04.html#reading-data-from-a-file",
    "href": "Self-Practice/Self-Practice_04/Self-Practice_04.html#reading-data-from-a-file",
    "title": "Self Practice 4: R for Data Science",
    "section": "",
    "text": "To begin, we’ll focus on the most common rectangular data file type: CSV, which is short for comma-separated values. Here is what a simple CSV file looks like. The first row, commonly called the header row, gives the column names, and the following six rows provide the data. The columns are separated, aka delimited, by commas.\n\nstudents &lt;- read_csv(\"data/students.csv\")\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n\nstudents &lt;- read_csv(\"https://pos.it/r4ds-students-csv\")\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n\n\nOnce you read data in, the first step usually involves transforming it in some way to make it easier to work with in the rest of your analysis. Let’s take another look at the students data with that in mind.\n\nstudents &lt;- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\nAn alternative approach is to use janitor::clean_names() to use some heuristics to turn them all into snake case at once.\n\nstudents |&gt; janitor::clean_names()\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan           age  \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2          2 Barclay Lynn     French fries       Lunch only          5    \n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6          6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\nAnother common task after reading in data is to consider variable types. For example, meal_plan is a categorical variable with a known set of possible values, which in R should be represented as a factor:\n\nstudents |&gt;\n  janitor::clean_names() |&gt;\n  mutate(meal_plan = factor(meal_plan))\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan           age  \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;chr&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2          2 Barclay Lynn     French fries       Lunch only          5    \n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6          6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\nNote that the values in the meal_plan variable have stayed the same, but the type of variable denoted underneath the variable name has changed from character (&lt;chr&gt;) to factor (&lt;fct&gt;). You’ll learn more about factors in Chapter 16.\nBefore you analyze these data, you’ll probably want to fix the age and id columns. Currently, age is a character variable because one of the observations is typed out as five instead of a numeric 5. We discuss the details of fixing this issue in Chapter 20.\n\nstudents &lt;- students |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    meal_plan = factor(meal_plan),\n    age = parse_number(if_else(age == \"five\", \"5\", age))\n  )\n\nstudents\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\n\n\nThere are a couple of other important arguments that we need to mention, and they’ll be easier to demonstrate if we first show you a handy trick: read_csv() can read text strings that you’ve created and formatted like a CSV file:\n\nread_csv(\n  \"a,b,c\n  1,2,3\n  4,5,6\"\n)\n\n# A tibble: 2 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n2     4     5     6\n\n\nUsually, read_csv() uses the first line of the data for the column names, which is a very common convention. But it’s not uncommon for a few lines of metadata to be included at the top of the file. You can use skip = n to skip the first n lines or use comment = \"#\" to drop all lines that start with (e.g.) #:\n\nread_csv(\n  \"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\",\n  skip = 2\n)\n\n# A tibble: 1 × 3\n      x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n\n\n\nread_csv(\n  \"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\"\n)\n\n# A tibble: 1 × 3\n      x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n\n\nIn other cases, the data might not have column names. You can use col_names = FALSE to tell read_csv() not to treat the first row as headings and instead label them sequentially from X1 to Xn:\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = FALSE\n)\n\n# A tibble: 2 × 3\n     X1    X2    X3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n2     4     5     6\n\n\nAlternatively, you can pass col_names a character vector which will be used as the column names:\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = c(\"x\", \"y\", \"z\")\n)\n\n# A tibble: 2 × 3\n      x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n2     4     5     6\n\n\nThese arguments are all you need to know to read the majority of CSV files that you’ll encounter in practice. (For the rest, you’ll need to carefully inspect your .csv file and read the documentation for read_csv()’s many other arguments.)\n\n\n\nOnce you’ve mastered read_csv(), using readr’s other functions is straightforward; it’s just a matter of knowing which function to reach for:\n\nread_csv2() reads semicolon-separated files. These use ; instead of , to separate fields and are common in countries that use , as the decimal marker.\nread_tsv() reads tab-delimited files.\nread_delim() reads in files with any delimiter, attempting to automatically guess the delimiter if you don’t specify it.\nread_fwf() reads fixed-width files. You can specify fields by their widths with fwf_widths() or by their positions with fwf_positions().\nread_table() reads a common variation of fixed-width files where columns are separated by white space.\nread_log() reads Apache-style log files.\n\n\n\n\nreadr provides a total of nine column types for you to use:\n\ncol_logical() and col_double() read logicals and real numbers. They’re relatively rarely needed (except as above), since readr will usually guess them for you.\ncol_integer() reads integers. We seldom distinguish integers and doubles in this book because they’re functionally equivalent, but reading integers explicitly can occasionally be useful because they occupy half the memory of doubles.\ncol_character() reads strings. This can be useful to specify explicitly when you have a column that is a numeric identifier, i.e., long series of digits that identifies an object but doesn’t make sense to apply mathematical operations to. Examples include phone numbers, social security numbers, credit card numbers, etc.\ncol_factor(), col_date(), and col_datetime() create factors, dates, and date-times respectively; you’ll learn more about those when we get to those data types in Chapter 16 and Chapter 17.\ncol_number() is a permissive numeric parser that will ignore non-numeric components, and is particularly useful for currencies. You’ll learn more about it in Chapter 13.\ncol_skip() skips a column so it’s not included in the result, which can be useful for speeding up reading the data if you have a large CSV file and you only want to use some of the columns."
  },
  {
    "objectID": "Self-Practice/Self-Practice_04/Self-Practice_04.html#reading-data-from-multiple-files",
    "href": "Self-Practice/Self-Practice_04/Self-Practice_04.html#reading-data-from-multiple-files",
    "title": "Self Practice 4: R for Data Science",
    "section": "",
    "text": "Sometimes your data is split across multiple files instead of being contained in a single file. For example, you might have sales data for multiple months, with each month’s data in a separate file: 01-sales.csv for January, 02-sales.csv for February, and 03-sales.csv for March. With read_csv() you can read these data in at once and stack them on top of each other in a single data frame.\n\nsales_files &lt;- c(\n  \"https://pos.it/r4ds-01-sales\",\n  \"https://pos.it/r4ds-02-sales\",\n  \"https://pos.it/r4ds-03-sales\"\n)\nread_csv(sales_files, id = \"file\")\n\n# A tibble: 19 × 6\n   file                         month     year brand  item     n\n   &lt;chr&gt;                        &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 https://pos.it/r4ds-01-sales January   2019     1  1234     3\n 2 https://pos.it/r4ds-01-sales January   2019     1  8721     9\n 3 https://pos.it/r4ds-01-sales January   2019     1  1822     2\n 4 https://pos.it/r4ds-01-sales January   2019     2  3333     1\n 5 https://pos.it/r4ds-01-sales January   2019     2  2156     9\n 6 https://pos.it/r4ds-01-sales January   2019     2  3987     6\n 7 https://pos.it/r4ds-01-sales January   2019     2  3827     6\n 8 https://pos.it/r4ds-02-sales February  2019     1  1234     8\n 9 https://pos.it/r4ds-02-sales February  2019     1  8721     2\n10 https://pos.it/r4ds-02-sales February  2019     1  1822     3\n11 https://pos.it/r4ds-02-sales February  2019     2  3333     1\n12 https://pos.it/r4ds-02-sales February  2019     2  2156     3\n13 https://pos.it/r4ds-02-sales February  2019     2  3987     6\n14 https://pos.it/r4ds-03-sales March     2019     1  1234     3\n15 https://pos.it/r4ds-03-sales March     2019     1  3627     1\n16 https://pos.it/r4ds-03-sales March     2019     1  8820     3\n17 https://pos.it/r4ds-03-sales March     2019     2  7253     1\n18 https://pos.it/r4ds-03-sales March     2019     2  8766     3\n19 https://pos.it/r4ds-03-sales March     2019     2  8288     6\n\n\nThe id argument adds a new column called file to the resulting data frame that identifies the file the data come from. This is especially helpful in circumstances where the files you’re reading in do not have an identifying column that can help you trace the observations back to their original sources.\nIf you have many files you want to read in, it can get cumbersome to write out their names as a list. Instead, you can use the base list.files() function to find the files for you by matching a pattern in the file names. You’ll learn more about these patterns in Chapter 15."
  },
  {
    "objectID": "Self-Practice/Self-Practice_04/Self-Practice_04.html#writing-to-a-file",
    "href": "Self-Practice/Self-Practice_04/Self-Practice_04.html#writing-to-a-file",
    "title": "Self Practice 4: R for Data Science",
    "section": "",
    "text": "readr also comes with two useful functions for writing data back to disk: write_csv() and write_tsv(). The most important arguments to these functions are x (the data frame to save) and file (the location to save it). You can also specify how missing values are written with na, and if you want to append to an existing file.\n\nwrite_csv(students, \"data/students1.csv\")\n\nNow let’s read that csv file back in. Note that the variable type information that you just set up is lost when you save to CSV because you’re starting over with reading from a plain text file again:\n\nstudents &lt;- read_csv(\"data/students1.csv\")\nstudents\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\nThis makes CSVs a little unreliable for caching interim results—you need to recreate the column specification every time you load in. There are two main alternatives:\n\nwrite_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS. This means that when you reload the object, you are loading the exact same R object that you stored.\n\n\nstudents &lt;- students |&gt;\n  janitor::clean_names() |&gt;\n  mutate(meal_plan = factor(meal_plan))\n\n\nwrite_rds(students, \"data/students1.rds\")\n\n\nread_rds(\"data/students1.rds\")\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nThe arrow package allows you to read and write parquet files, a fast binary file format that can be shared across programming languages. We’ll return to arrow in more depth in Chapter 22."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Ex 02",
    "section": "",
    "text": "There are two major residential property market in Singapore, namely public and private housing. Public housing aims to meet the basic need of the general public with monthly household income less than or equal to S$14,000. For families with monthly household income more than S$14,000, they need to turn to the private residential market.\n\n\n\nIn this take-home exercise, you are required to:\n\nselect one data visualisation from the Take-home Exercise 1 submission prepared by your classmate,\ncritic the submission in terms of clarity and aesthetics,\nprepare a sketch for the alternative design by using the data visualisation design principles and best practices you had learned in Lesson 1 and 2.\nremake the original design by using ggplot2, ggplot2 extensions and tidyverse packages.\n\n\n\n\nThe original visualisation can be found in this link. I will be using EDA 3 for this take home exercise. Below shows the original plot and the insights by the original creator:\n\nThe Insights - The Central Region’s peak is the most pronounced and shifted towards the high, indicating that there is a high concentration of properties with higher unit prices.\n- The East Region’s pea is lower and more towards the middle of the x-axis compared to the Central Region. This implies that while the East Region has properties with moderate unit prices, it does not reach as high of a price point as frequently as the Central Region.\n- North East and North Regions peaks are less sharp and positioned towards the lower end of the price scale, suggesting a more affordable housing options and a wider distribution of unit prices. North Region appears to have a slightly broader distribution than North East.\n- West Region has the lowest peak among all the regions and is positioned towards the far left of the plot. This indicates that the West Region is the most affordable in Singapore.\n\n\n\n\nWe will now set up according to the original code provided in the link.\n\n\nT he pac::p_load() function to load the required R packages in the working environment. The following packages are used in this set up:\n\nggthemes: Extra themes, geoms, and scales for ggplot2.\ntidyverse: A collection of core packages designed for data science, used extensively for data preparation and wrangling.\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\ncolorspace:\nggiraph: for making ‘ggplot’ graphics interactive.\nplotly: R library for plotting interactive statistical graphs.\npatchwork: specially designed for combining separate ggplot2 graphs into a single figure.\nlubridate: for easy and fast parsing of Date / Time\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggdist: a ggplot2 extension specially design for visualising distribution and uncertainty\n\n\npacman::p_load(tidyverse, haven,\n               ggrepel, ggthemes,\n               ggridges, ggdist,\n               patchwork, scales,\n               viridis, cowplot, \n               dplyr, plotly,\n               tidyr, lubridate, \n               ggplot2, ggExtra)\n\n\n\n\nIdentify the missing values in the data set and removed any missing elements. ‘Type of Sale’ and ‘Property type’ has been converted to factor format. ‘Transacted Price ($)’ and ‘Area (SQFT)’ are converted to numeric data types. ‘Type of Sale’ has been group into three categories. ‘Sale Date’ has been converted to Date format.\nThe process had been repeated for all five data sets.\n\nImport Data SetConverting DataConverting Dates\n\n\n\nds1 &lt;- read_csv(\"data/ResidentialTransaction20240308160536.csv\")\nds2 &lt;- read_csv(\"data/ResidentialTransaction20240308160736.csv\")\nds3 &lt;- read_csv(\"data/ResidentialTransaction20240308161009.csv\")\nds4 &lt;- read_csv(\"data/ResidentialTransaction20240308161109.csv\")\nds5 &lt;- read_csv(\"data/ResidentialTransaction20240414220633.csv\")\n\n\n\n\nprepare_dataset &lt;- function(ds) {\n  colSums(is.na(ds))\n  ds &lt;- na.omit(ds)\n  \n  ds$`Type of Sale` &lt;- tolower(as.character(ds$`Type of Sale`))\n  ds$`Type of Sale` &lt;- ifelse(ds$`Type of Sale` %in% c(\"new sale\", \"resale\"), ds$`Type of Sale`, \"other\")\n  ds$`Type of Sale` &lt;- as.factor(ds$`Type of Sale`)\n  \n  ds$`Property Type` &lt;- as.factor(ds$`Property Type`)\n  \n  ds$`Transacted Price ($)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Transacted Price ($)`, perl = TRUE))\n  ds$`Area (SQFT)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Area (SQFT)`, perl = TRUE))\n  ds$`Unit Price ($ PSF)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Unit Price ($ PSF)`, perl = TRUE))\n  \n  return(ds)\n}\n\n# Apply the function to each dataset\nds1 &lt;- prepare_dataset(ds1)\nds2 &lt;- prepare_dataset(ds2)\nds3 &lt;- prepare_dataset(ds3)\nds4 &lt;- prepare_dataset(ds4)\nds5 &lt;- prepare_dataset(ds5)\n\n# Combine the datasets\ncombined_ds &lt;- rbind(ds1, ds2, ds3, ds4, ds5)\n\n\n\n\n# Convert Sale Date to Date format\nds1$`Sale Date` &lt;- dmy(ds1$`Sale Date`)\nds2$`Sale Date` &lt;- dmy(ds2$`Sale Date`)\nds3$`Sale Date` &lt;- dmy(ds3$`Sale Date`)\nds4$`Sale Date` &lt;- dmy(ds4$`Sale Date`)\nds5$`Sale Date` &lt;- dmy(ds5$`Sale Date`)\n\n\n\n\n\n\n\n\nThe original plot and code is as below:\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(viridis)\n\nggplot(combined_ds, aes(x = `Unit Price ($ PSF)`, y = `Planning Region`, fill = `Planning Region`)) +\n  geom_density_ridges(scale = 3) +\n  scale_fill_viridis(discrete = TRUE) +\n  labs(title = \"Unit Price ($ PSF) Distribution by Planning Region\",\n       x = \"Unit Price ($ PSF)\",\n       y = \"Planning Region\") +\n  theme_ridges()\n\n\n\n\nThe Insights\n\nThe Central Region’s peak is the most pronounced and shifted towards the high, indicating that there is a high concentration of properties with higher unit prices.\n\nThe East Region’s pea is lower and more towards the middle of the x-axis compared to the Central Region. This implies that while the East Region has properties with moderate unit prices, it does not reach as high of a price point as frequently as the Central Region.\n\nNorth East and North Regions peaks are less sharp and positioned towards the lower end of the price scale, suggesting a more affordable housing options and a wider distribution of unit prices. North Region appears to have a slightly broader distribution than North East.\n\nWest Region has the lowest peak among all the regions and is positioned towards the far left of the plot. This indicates that the West Region is the most affordable in Singapore.\n\n\n\n\n\nThe distribution without the quantile lines is unable to let audience know where is the median and make it hard to conclude just using peak: “West Region has the lowest peak among all the regions and is positioned towards the far left of the plot. This indicates that the West Region is the most affordable in Singapore.” While the peak of the respective region does indicate the highest frequency of data points, it does not reveal the overall median price and other aspect such as the the 4 quantiles. Hence, it might be easier for the audience if quantiles were included in the aesthetics of the plot. With this enhancement, it might be easier to define which Region is more affordable, rather than just saying that West is most affordable as it is “most far left”.\n\nThe legend for the Planning Region to the right of the plot is redundant, as it is very clear from the y-axis which plot belongs to which region. If we choose to retain the legend, perhaps it could be compressed slightly smaller to the top right corner of the plot (as there are ample empty space between the 4000-6000 mark along x-axis), and the free up spaces could be used to insert another plot that enhance the story-telling for this plot.\n\nThe current distribution is 100% opaque and parts of the North Region graph is overlapping the West Region. While the peak of the West Region is still visible, it could be better for audience if the distribution is slightly transparent to allow the audience to see the full distribution of the West Region. It will be complementing the enhancement with quantiles added, as by adjusting the transparency, we will be able to see all the quantiles without guessing through the overlapped areas.\n\n\n\n\nAs most of the distribution in this plot falls between the $1,000-$3,000 mark along the x-axis, the current axis ticks did not allow the audience to have a clear sense of where the peaks are, except for a rough sense of gauging the peak from the neares two x-axis ticks, which is 0 or 2000. To alleviate this issue, the frequency of the x-axis ticks could be increased. Also, visually the plot for the Central Region at the most right, seems to end at just below the 5,000 mark. Space could be saved by ending the x-axis ticks at 5,000 instead, without affecting the clarity and truthfulness of the plot.\n\nInstead of guessing from the distribution which Region is the most affordable and which is the highest priced, it would be clearer if we add in another plot with comparison of the exact mean and median price across the 5 regions using a horizontal bar chart to complement the existing plot.\n\nThe plot is able to let audience understand the overall comparison of distribution against the other 4 regions, but did not provide any comparison of each region against the overall median Unit Price in Singapore. To help audience understand the distribution with respect to the overall median, an overall median line for the Unit Price could be added. In this way, not only we put perspective of region against region, we also include the perspective of each region against the overall market in Singapore.\n\n\n\n\n\n\n\n\nTask: Prepare a sketch for the alternative design by using the data visualisation design principles and best practices you had learned in Lesson 1 and 2.\nUsing the 6 critics mentioned in section 3, I will attempt to enhance the plot by including/removing the following features:\n\nInclude quantile lines for each Region.\nRemove the legend.\nIncrease the transparency of the distribution to ensure all parts of the distribution is not hidden behind other distribution. Alternatively, we can adjust the scale of the geom_density_ridges() to prevent overlaps. Decision can be made after generating both types for consideration.\nIncrease the frequency of the x-axis ticks with soft grid lines to improve readability of distribution.\nRemove the excess space at the right of the plot after 5,000 mark of x-axis.\nAdd in a horizontal bar chart to the right of the existing plot to include the comparison with overall median Unit Price ($ PSF) in Singapore with each of the region.\nAdd in a overall Median Unit Price ($ PSF) for the existing ridgeline plot.\nInclude median value annotations for each region in the existing ridgeline plot.\nRename the y-axis to just the region name, i.e. “West” instead of “West Region”.\nUsing patchwork to patch both plots side by side. The left and right plot should have the same region in the same line for consistency.\n\n\n\n\n\nThe first sketch below shows the intended implementation of the enhancement as mentioned in section 4.1. After this quick sketch with the median values, I realise that there are some further steps to take for a more coherent picture. (caveat: the median value is a rough calculation to facilitate the sketch, it may not be the same value as the final plot reproduced)\n\nThe horizontal bar plot should be sorted according from largest negative value to the highest positive value (red at the top to blue at the bottom). This allows the audience to know the most affordable region to the most expensive region from top down.\nThe distribution plot to the left must synchronised in the same sequence based on the Region, so that the price deviation on the right plot is correctly reflected as the same Region.\n\n\n\n\n\n\n\n# Calculate median values\nmedian_values &lt;- combined_ds %&gt;%\n  group_by(`Planning Region`) %&gt;%\n  summarise(median_value = median(`Unit Price ($ PSF)`))\n\n# Recode `Planning Region`\ncombined_ds &lt;- combined_ds %&gt;%\n  mutate(`Planning Region` = recode(`Planning Region`, \n                                  \"Central Region\" = \"Central\",\n                                  \"East Region\" = \"East\",\n                                  \"North Region\" = \"North\",\n                                  \"North East Region\" = \"North East\",\n                                  \"West Region\" = \"West\"\n                                  ))\n\n\ncombined_ds1 &lt;- combined_ds %&gt;%\n  mutate(`Planning Region` = factor(`Planning Region`, levels = median_values$`Planning Region`[order(median_values$median_value)]))\n\n\np1 &lt;- ggplot(combined_ds, \n       aes(x = `Unit Price ($ PSF)`, \n           y = `Planning Region`,\n           fill = `Planning Region`)) +\n   geom_density_ridges(alpha = 0.2, quantile_lines = TRUE, quantile_fun = function(x, ...) median(x))  +\n  geom_text(data = median_values, \n            aes(x = median_value, \n                y = `Planning Region`, \n                label = paste(\"Median:\", round(median_value, 2))),\n            hjust = -0.2, \n            vjust = -2.8, \n            color = \"black\", \n            size = 3) +\n  labs(title = \"Unit Price ($ PSF) Distribution by Planning Region\",\n       x = \"Unit Price ($ PSF)\",\n       y = \"Planning Region\") +\n  geom_vline(aes(xintercept = median(`Unit Price ($ PSF)`)), col=\"red\", linewidth=0.5, linetype = \"dashed\") +\n  annotate(\"text\", x=2700, y=0.7, label=paste0(\"Singapore's Median:\",median(combined_ds$`Unit Price ($ PSF)`)), \n           size=3, color=\"red\") +\n   guides(fill = FALSE) + # Remove legend for Planning Region\n  xlim(0, 4000) \np1\n\n\n\n\n\n\n\n\n\n# Classifying the Median into Above and Below Singapore's Median\ndf_Aavg &lt;- combined_ds %&gt;%\n  group_by(`Planning Region`) %&gt;%\n  summarise(avg_Aprice = median(`Unit Price ($ PSF)`))\ndf_Aavg$p_z &lt;- round((df_Aavg$avg_Aprice - median(combined_ds$`Unit Price ($ PSF)`)), 2)\ndf_Aavg$p_ztype &lt;- ifelse(df_Aavg$p_z &lt; 0, \"below\", \"above\")\n\np2 &lt;- ggplot(df_Aavg, aes(x = `Planning Region`, y = p_z, label = p_z)) +\n  geom_bar(stat = \"identity\", aes(fill = p_ztype), position = position_dodge2(width = 2), width = 0.5) +\n  scale_fill_manual(name = \"Average Price\", labels = c(\"Above Singapore's Median\", \"Below Singapore's Median\"), values = c(\"below\" = \"#C83E4D\", \"above\" = \"#4A5859\")) +\n  labs(title = \"Unit Price ($PSF) Deviations\", y = \"\", subtitle = \"by Planning Region\") +\n  coord_flip() +\n  theme(legend.position = \"None\", text = element_text(size = 8), plot.title = element_text(size = 10, face = \"bold\"))  +\n  theme(text = element_text(size = 7),\n        legend.title = element_blank(),\n        legend.position = c(0.8, 0.7),\n        legend.key.size = unit(0.3, 'cm'),\n        legend.key.height = unit(0.3, 'cm'),\n        legend.key.width = unit(0.3, 'cm')) +\n  geom_text(aes(x = `Planning Region`, \n                y = df_Aavg$p_z, \n                label = abs(df_Aavg$p_z)),\n            hjust = 0.5,\n            vjust = -2.5,\n            size = 3)\np2\n\n\n\n\n\n\n\n\n\np1+p2\n\n\n\n\n\n\n\n\n\n\n\n\nAlignment of Planning Region for the left and right plot is important for visual perception of audience, as the alignment allows them to have a direct translation of the differences (the right side plot) without doing mental calculation.\nThe two colors of the horizontal bar chart gives an immediate answer to which region is above and below the Singapore’s average.\n\n\n\n1. T.S. Kam, R for Visual Analytics Chapter 9 for visualisation of Ridgeline plots with quantile lines.\n2. Claus O. Wilke, Fundamentals of Data Visualization Chapter 2 for understanding and classification of variables.\n3. Stack Overflow, “Filter rows which contain a certain string” for removing en bloc sales from the data frame."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#overview",
    "href": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#overview",
    "title": "Take-home Ex 02",
    "section": "",
    "text": "There are two major residential property market in Singapore, namely public and private housing. Public housing aims to meet the basic need of the general public with monthly household income less than or equal to S$14,000. For families with monthly household income more than S$14,000, they need to turn to the private residential market.\n\n\n\nIn this take-home exercise, you are required to:\n\nselect one data visualisation from the Take-home Exercise 1 submission prepared by your classmate,\ncritic the submission in terms of clarity and aesthetics,\nprepare a sketch for the alternative design by using the data visualisation design principles and best practices you had learned in Lesson 1 and 2.\nremake the original design by using ggplot2, ggplot2 extensions and tidyverse packages.\n\n\n\n\nThe original visualisation can be found in this link. I will be using EDA 3 for this take home exercise. Below shows the original plot and the insights by the original creator:\n\nThe Insights - The Central Region’s peak is the most pronounced and shifted towards the high, indicating that there is a high concentration of properties with higher unit prices.\n- The East Region’s pea is lower and more towards the middle of the x-axis compared to the Central Region. This implies that while the East Region has properties with moderate unit prices, it does not reach as high of a price point as frequently as the Central Region.\n- North East and North Regions peaks are less sharp and positioned towards the lower end of the price scale, suggesting a more affordable housing options and a wider distribution of unit prices. North Region appears to have a slightly broader distribution than North East.\n- West Region has the lowest peak among all the regions and is positioned towards the far left of the plot. This indicates that the West Region is the most affordable in Singapore."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#set-up",
    "href": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#set-up",
    "title": "Take-home Ex 02",
    "section": "",
    "text": "We will now set up according to the original code provided in the link.\n\n\nT he pac::p_load() function to load the required R packages in the working environment. The following packages are used in this set up:\n\nggthemes: Extra themes, geoms, and scales for ggplot2.\ntidyverse: A collection of core packages designed for data science, used extensively for data preparation and wrangling.\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\ncolorspace:\nggiraph: for making ‘ggplot’ graphics interactive.\nplotly: R library for plotting interactive statistical graphs.\npatchwork: specially designed for combining separate ggplot2 graphs into a single figure.\nlubridate: for easy and fast parsing of Date / Time\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggdist: a ggplot2 extension specially design for visualising distribution and uncertainty\n\n\npacman::p_load(tidyverse, haven,\n               ggrepel, ggthemes,\n               ggridges, ggdist,\n               patchwork, scales,\n               viridis, cowplot, \n               dplyr, plotly,\n               tidyr, lubridate, \n               ggplot2, ggExtra)\n\n\n\n\nIdentify the missing values in the data set and removed any missing elements. ‘Type of Sale’ and ‘Property type’ has been converted to factor format. ‘Transacted Price ($)’ and ‘Area (SQFT)’ are converted to numeric data types. ‘Type of Sale’ has been group into three categories. ‘Sale Date’ has been converted to Date format.\nThe process had been repeated for all five data sets.\n\nImport Data SetConverting DataConverting Dates\n\n\n\nds1 &lt;- read_csv(\"data/ResidentialTransaction20240308160536.csv\")\nds2 &lt;- read_csv(\"data/ResidentialTransaction20240308160736.csv\")\nds3 &lt;- read_csv(\"data/ResidentialTransaction20240308161009.csv\")\nds4 &lt;- read_csv(\"data/ResidentialTransaction20240308161109.csv\")\nds5 &lt;- read_csv(\"data/ResidentialTransaction20240414220633.csv\")\n\n\n\n\nprepare_dataset &lt;- function(ds) {\n  colSums(is.na(ds))\n  ds &lt;- na.omit(ds)\n  \n  ds$`Type of Sale` &lt;- tolower(as.character(ds$`Type of Sale`))\n  ds$`Type of Sale` &lt;- ifelse(ds$`Type of Sale` %in% c(\"new sale\", \"resale\"), ds$`Type of Sale`, \"other\")\n  ds$`Type of Sale` &lt;- as.factor(ds$`Type of Sale`)\n  \n  ds$`Property Type` &lt;- as.factor(ds$`Property Type`)\n  \n  ds$`Transacted Price ($)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Transacted Price ($)`, perl = TRUE))\n  ds$`Area (SQFT)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Area (SQFT)`, perl = TRUE))\n  ds$`Unit Price ($ PSF)` &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", ds$`Unit Price ($ PSF)`, perl = TRUE))\n  \n  return(ds)\n}\n\n# Apply the function to each dataset\nds1 &lt;- prepare_dataset(ds1)\nds2 &lt;- prepare_dataset(ds2)\nds3 &lt;- prepare_dataset(ds3)\nds4 &lt;- prepare_dataset(ds4)\nds5 &lt;- prepare_dataset(ds5)\n\n# Combine the datasets\ncombined_ds &lt;- rbind(ds1, ds2, ds3, ds4, ds5)\n\n\n\n\n# Convert Sale Date to Date format\nds1$`Sale Date` &lt;- dmy(ds1$`Sale Date`)\nds2$`Sale Date` &lt;- dmy(ds2$`Sale Date`)\nds3$`Sale Date` &lt;- dmy(ds3$`Sale Date`)\nds4$`Sale Date` &lt;- dmy(ds4$`Sale Date`)\nds5$`Sale Date` &lt;- dmy(ds5$`Sale Date`)"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#critic-on-the-eda-3",
    "href": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#critic-on-the-eda-3",
    "title": "Take-home Ex 02",
    "section": "",
    "text": "The original plot and code is as below:\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(viridis)\n\nggplot(combined_ds, aes(x = `Unit Price ($ PSF)`, y = `Planning Region`, fill = `Planning Region`)) +\n  geom_density_ridges(scale = 3) +\n  scale_fill_viridis(discrete = TRUE) +\n  labs(title = \"Unit Price ($ PSF) Distribution by Planning Region\",\n       x = \"Unit Price ($ PSF)\",\n       y = \"Planning Region\") +\n  theme_ridges()\n\n\n\n\nThe Insights\n\nThe Central Region’s peak is the most pronounced and shifted towards the high, indicating that there is a high concentration of properties with higher unit prices.\n\nThe East Region’s pea is lower and more towards the middle of the x-axis compared to the Central Region. This implies that while the East Region has properties with moderate unit prices, it does not reach as high of a price point as frequently as the Central Region.\n\nNorth East and North Regions peaks are less sharp and positioned towards the lower end of the price scale, suggesting a more affordable housing options and a wider distribution of unit prices. North Region appears to have a slightly broader distribution than North East.\n\nWest Region has the lowest peak among all the regions and is positioned towards the far left of the plot. This indicates that the West Region is the most affordable in Singapore."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#critics-on-aesthetics",
    "href": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#critics-on-aesthetics",
    "title": "Take-home Ex 02",
    "section": "",
    "text": "The distribution without the quantile lines is unable to let audience know where is the median and make it hard to conclude just using peak: “West Region has the lowest peak among all the regions and is positioned towards the far left of the plot. This indicates that the West Region is the most affordable in Singapore.” While the peak of the respective region does indicate the highest frequency of data points, it does not reveal the overall median price and other aspect such as the the 4 quantiles. Hence, it might be easier for the audience if quantiles were included in the aesthetics of the plot. With this enhancement, it might be easier to define which Region is more affordable, rather than just saying that West is most affordable as it is “most far left”.\n\nThe legend for the Planning Region to the right of the plot is redundant, as it is very clear from the y-axis which plot belongs to which region. If we choose to retain the legend, perhaps it could be compressed slightly smaller to the top right corner of the plot (as there are ample empty space between the 4000-6000 mark along x-axis), and the free up spaces could be used to insert another plot that enhance the story-telling for this plot.\n\nThe current distribution is 100% opaque and parts of the North Region graph is overlapping the West Region. While the peak of the West Region is still visible, it could be better for audience if the distribution is slightly transparent to allow the audience to see the full distribution of the West Region. It will be complementing the enhancement with quantiles added, as by adjusting the transparency, we will be able to see all the quantiles without guessing through the overlapped areas.\n\n\n\n\nAs most of the distribution in this plot falls between the $1,000-$3,000 mark along the x-axis, the current axis ticks did not allow the audience to have a clear sense of where the peaks are, except for a rough sense of gauging the peak from the neares two x-axis ticks, which is 0 or 2000. To alleviate this issue, the frequency of the x-axis ticks could be increased. Also, visually the plot for the Central Region at the most right, seems to end at just below the 5,000 mark. Space could be saved by ending the x-axis ticks at 5,000 instead, without affecting the clarity and truthfulness of the plot.\n\nInstead of guessing from the distribution which Region is the most affordable and which is the highest priced, it would be clearer if we add in another plot with comparison of the exact mean and median price across the 5 regions using a horizontal bar chart to complement the existing plot.\n\nThe plot is able to let audience understand the overall comparison of distribution against the other 4 regions, but did not provide any comparison of each region against the overall median Unit Price in Singapore. To help audience understand the distribution with respect to the overall median, an overall median line for the Unit Price could be added. In this way, not only we put perspective of region against region, we also include the perspective of each region against the overall market in Singapore."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#the-data-visualisation-makeover",
    "href": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#the-data-visualisation-makeover",
    "title": "Take-home Ex 02",
    "section": "",
    "text": "Task: Prepare a sketch for the alternative design by using the data visualisation design principles and best practices you had learned in Lesson 1 and 2.\nUsing the 6 critics mentioned in section 3, I will attempt to enhance the plot by including/removing the following features:\n\nInclude quantile lines for each Region.\nRemove the legend.\nIncrease the transparency of the distribution to ensure all parts of the distribution is not hidden behind other distribution. Alternatively, we can adjust the scale of the geom_density_ridges() to prevent overlaps. Decision can be made after generating both types for consideration.\nIncrease the frequency of the x-axis ticks with soft grid lines to improve readability of distribution.\nRemove the excess space at the right of the plot after 5,000 mark of x-axis.\nAdd in a horizontal bar chart to the right of the existing plot to include the comparison with overall median Unit Price ($ PSF) in Singapore with each of the region.\nAdd in a overall Median Unit Price ($ PSF) for the existing ridgeline plot.\nInclude median value annotations for each region in the existing ridgeline plot.\nRename the y-axis to just the region name, i.e. “West” instead of “West Region”.\nUsing patchwork to patch both plots side by side. The left and right plot should have the same region in the same line for consistency.\n\n\n\n\n\nThe first sketch below shows the intended implementation of the enhancement as mentioned in section 4.1. After this quick sketch with the median values, I realise that there are some further steps to take for a more coherent picture. (caveat: the median value is a rough calculation to facilitate the sketch, it may not be the same value as the final plot reproduced)\n\nThe horizontal bar plot should be sorted according from largest negative value to the highest positive value (red at the top to blue at the bottom). This allows the audience to know the most affordable region to the most expensive region from top down.\nThe distribution plot to the left must synchronised in the same sequence based on the Region, so that the price deviation on the right plot is correctly reflected as the same Region.\n\n\n\n\n\n\n\n# Calculate median values\nmedian_values &lt;- combined_ds %&gt;%\n  group_by(`Planning Region`) %&gt;%\n  summarise(median_value = median(`Unit Price ($ PSF)`))\n\n# Recode `Planning Region`\ncombined_ds &lt;- combined_ds %&gt;%\n  mutate(`Planning Region` = recode(`Planning Region`, \n                                  \"Central Region\" = \"Central\",\n                                  \"East Region\" = \"East\",\n                                  \"North Region\" = \"North\",\n                                  \"North East Region\" = \"North East\",\n                                  \"West Region\" = \"West\"\n                                  ))\n\n\ncombined_ds1 &lt;- combined_ds %&gt;%\n  mutate(`Planning Region` = factor(`Planning Region`, levels = median_values$`Planning Region`[order(median_values$median_value)]))\n\n\np1 &lt;- ggplot(combined_ds, \n       aes(x = `Unit Price ($ PSF)`, \n           y = `Planning Region`,\n           fill = `Planning Region`)) +\n   geom_density_ridges(alpha = 0.2, quantile_lines = TRUE, quantile_fun = function(x, ...) median(x))  +\n  geom_text(data = median_values, \n            aes(x = median_value, \n                y = `Planning Region`, \n                label = paste(\"Median:\", round(median_value, 2))),\n            hjust = -0.2, \n            vjust = -2.8, \n            color = \"black\", \n            size = 3) +\n  labs(title = \"Unit Price ($ PSF) Distribution by Planning Region\",\n       x = \"Unit Price ($ PSF)\",\n       y = \"Planning Region\") +\n  geom_vline(aes(xintercept = median(`Unit Price ($ PSF)`)), col=\"red\", linewidth=0.5, linetype = \"dashed\") +\n  annotate(\"text\", x=2700, y=0.7, label=paste0(\"Singapore's Median:\",median(combined_ds$`Unit Price ($ PSF)`)), \n           size=3, color=\"red\") +\n   guides(fill = FALSE) + # Remove legend for Planning Region\n  xlim(0, 4000) \np1\n\n\n\n\n\n\n\n\n\n# Classifying the Median into Above and Below Singapore's Median\ndf_Aavg &lt;- combined_ds %&gt;%\n  group_by(`Planning Region`) %&gt;%\n  summarise(avg_Aprice = median(`Unit Price ($ PSF)`))\ndf_Aavg$p_z &lt;- round((df_Aavg$avg_Aprice - median(combined_ds$`Unit Price ($ PSF)`)), 2)\ndf_Aavg$p_ztype &lt;- ifelse(df_Aavg$p_z &lt; 0, \"below\", \"above\")\n\np2 &lt;- ggplot(df_Aavg, aes(x = `Planning Region`, y = p_z, label = p_z)) +\n  geom_bar(stat = \"identity\", aes(fill = p_ztype), position = position_dodge2(width = 2), width = 0.5) +\n  scale_fill_manual(name = \"Average Price\", labels = c(\"Above Singapore's Median\", \"Below Singapore's Median\"), values = c(\"below\" = \"#C83E4D\", \"above\" = \"#4A5859\")) +\n  labs(title = \"Unit Price ($PSF) Deviations\", y = \"\", subtitle = \"by Planning Region\") +\n  coord_flip() +\n  theme(legend.position = \"None\", text = element_text(size = 8), plot.title = element_text(size = 10, face = \"bold\"))  +\n  theme(text = element_text(size = 7),\n        legend.title = element_blank(),\n        legend.position = c(0.8, 0.7),\n        legend.key.size = unit(0.3, 'cm'),\n        legend.key.height = unit(0.3, 'cm'),\n        legend.key.width = unit(0.3, 'cm')) +\n  geom_text(aes(x = `Planning Region`, \n                y = df_Aavg$p_z, \n                label = abs(df_Aavg$p_z)),\n            hjust = 0.5,\n            vjust = -2.5,\n            size = 3)\np2\n\n\n\n\n\n\n\n\n\np1+p2"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#conclusion",
    "href": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#conclusion",
    "title": "Take-home Ex 02",
    "section": "",
    "text": "Alignment of Planning Region for the left and right plot is important for visual perception of audience, as the alignment allows them to have a direct translation of the differences (the right side plot) without doing mental calculation.\nThe two colors of the horizontal bar chart gives an immediate answer to which region is above and below the Singapore’s average."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#references",
    "href": "Take-home Exercise/Take-home_Ex02/Take-home_Ex02.html#references",
    "title": "Take-home Ex 02",
    "section": "",
    "text": "1. T.S. Kam, R for Visual Analytics Chapter 9 for visualisation of Ridgeline plots with quantile lines.\n2. Claus O. Wilke, Fundamentals of Data Visualization Chapter 2 for understanding and classification of variables.\n3. Stack Overflow, “Filter rows which contain a certain string” for removing en bloc sales from the data frame."
  },
  {
    "objectID": "In-class Exercise/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class Exercise/In-class_Ex06/In-class_Ex06.html",
    "title": "In-class Ex 06",
    "section": "",
    "text": "Example from corporaexplorer\n\npacman::p_load(tidyverse, stringi,\n               rvest, corporaexplorer,\n               readtext,\n               quanteda, tidytext)\n\n\nbible &lt;- readr::read_lines(\"http://www.gutenberg.org/cache/epub/10/pg10.txt\")\n\nTo tidy the text, we need to know where does it start and end.\n\n# collapse all into one string\nbible &lt;- paste(bible, collapse = \"\\n\")\n\n\n# Identifying the beginning and end of the Bible / stripping PJ metadata\n # (technique borrowed from https://quanteda.io/articles/pkgdown/replication/digital-humanities.html).\nstart_v &lt;- stri_locate_first_fixed(bible, \"The First Book of Moses: Called Genesis\")[1]\nend_v &lt;- stri_locate_last_fixed(bible, \"Amen.\")[2]\nbible &lt;- stri_sub(bible, start_v, end_v)\n\n\n# In the file, every book in the bible is preceded by five newlines,\n  # which we use to split our string into a vector where each element is a book.\nbooks &lt;- stri_split_regex(bible, \"\\n{5}\") %&gt;%\n    unlist %&gt;%\n    .[-40]  # Removing the heading \"The New Testament of the King James Bible\",\n              # which also was preceded by five newlines.\n\n\n# Because of the structure of the text in the file:\n  # Replacing double or more newlines with two newlines, and a single newline with space.\nbooks &lt;- str_replace_all(books, \"\\n{2,}\", \"NEW_PARAGRAPH\") %&gt;%\n    str_replace_all(\"\\n\", \" \") %&gt;%\n    str_replace_all(\"NEW_PARAGRAPH\", \"\\n\\n\")\nbooks &lt;- books[3:68]  # The two first elements are not books\n\n\n# Identifying new chapters within each book and split the text into chapters.\n# (The first characters in chapter 2 will e.g. be 2:1)\nchapters &lt;- str_replace_all(books, \"(\\\\d+:1 )\", \"NEW_CHAPTER\\\\1\") %&gt;%\n    stri_split_regex(\"NEW_CHAPTER\")\n\n\n# Removing the chapter headings from the text (we want them as metadata).\nchapters &lt;- lapply(chapters, function(x) x[-1])\n\n\n# We are not quite happy with the long book titles in the King James Bible,\n  # so we retrieve shorter versions from esv.org which will take up less\n  # space in the corpus map plot.\nbook_titles &lt;- read_html(\"https://www.esv.org/resources/esv-global-study-bible/list-of-abbreviations\") %&gt;%\n  html_nodes(\"td:nth-child(1)\") %&gt;%\n  html_text() %&gt;%\n  .[13:78]  # Removing irrelevant elements after manual inspection.\n\n\n# We add a column indicating whether a book belongs to the Old or New Testament,\n#   knowing that they contain respectively 39 and 27 books.\ntestament &lt;- c(rep(\"Old\", 39), rep(\"New\", 27))\n\n\n# Data frame with one book as one row.\nbible_df &lt;- tibble::tibble(Text = chapters,\n                           Book = book_titles,\n                           Testament = testament)\n\n\n# We want each chapter to be one row, but keep the metadata (book and which testament).\nbible_df &lt;- tidyr::unnest(bible_df, Text)\n\n\nKJB &lt;- prepare_data(dataset = bible_df,\n                    date_based_corpus = FALSE,\n                    grouping_variable = \"Book\",\n                    columns_doc_info = c(\"Testament\", \"Book\"))\n\n\ntext field = text we want to analyse\n\nMake sure you check the class of the object. The below should show “corporaexplorerobject”.\n\nclass(KJB)\n\n[1] \"corporaexplorerobject\"\n\n\n\nexplore(KJB)\n\n\npacman::p_load(jsonlite, tidygraph,ggraph,\n               visNetwork, graphlayouts,\n               ggforce, skimr, tidytext,\n               tidyverse)\n\n\nmc3_data &lt;- fromJSON(\"data/MC3.json\")\n\nThis data is a list, not a dataframe.\n\nclass(mc3_data)\n\n[1] \"list\"\n\n\nUse distinct to remove duplicate\n\nmc3_edges &lt;-\n  as_tibble(mc3_data$links) %&gt;%\n  distinct() %&gt;%\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type)) %&gt;%\n  group_by(source, target, type) %&gt;%\n  summarise(weights = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  ungroup\n\n\nclean edge file\n\nmc3_nodes &lt;- as_tibble(mc3_data$nodes) %&gt;% \n  mutate(country = as.character(country),\n         id = as.character(id),\n         product_services = as.numeric(as.character(revenue_omu)),\n         type = as.character(type)) %&gt;% \n  select(id, country, type, revenue_omu,\n         product_services)\n\n\nskim(mc3_edges)\n\n\nData summary\n\n\nName\nmc3_edges\n\n\nNumber of rows\n24036\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsource\n0\n1\n6\n700\n0\n12856\n0\n\n\ntarget\n0\n1\n6\n28\n0\n21265\n0\n\n\ntype\n0\n1\n16\n16\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nweights\n0\n1\n1\n0\n1\n1\n1\n1\n1\n▁▁▇▁▁\n\n\n\n\n\n\n\nAdd the columns back in after cleaning\n\nid1 &lt;- mc3_edges %&gt;% \n  select(source) %&gt;% \n  rename(id = source)\n\nid2 &lt;- mc3_edges %&gt;% \n  select(target) %&gt;% \n  rename(id = target)\n\nmc3_nodes1 &lt;- rbind(id1,id2) %&gt;% \n  distinct() %&gt;% \n  left_join(mc3_nodes,\n            unmatched = \"drop\")\n\n\nmc3_graph &lt;- tbl_graph(nodes = mc3_nodes1,\n                       edges = mc3_edges,\n                       directed = FALSE) %&gt;% \n  mutate(betweenness_centrality = centrality_betweenness(),\n                                                         closeness_centrality = centrality_closeness())\n\n\nmc3_graph %&gt;% \n  filter(betweenness_centrality &gt;= 300000) %&gt;% \nggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha=0.5)) +\n  geom_node_point(aes(\n    size = betweenness_centrality,\n    colors = \"lightblue\",\n    alpha = 0.5)) +\n  scale_size_continuous(range=c(1,10)) +\n  theme_graph()"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#aggregating-weight-for-edges",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#aggregating-weight-for-edges",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "To get aggregated weight for the edges by unique source, target and type.\n\nmc2_edges_agg &lt;-\n  mc2_edges_raw %&gt;%\n  distinct() %&gt;%\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type)) %&gt;%\n  group_by(source, target, type) %&gt;%\n  summarise(weights = n()) %&gt;%\n  filter(source!=target) \n  ungroup\n\nfunction (x, ...) \n{\n    UseMethod(\"ungroup\")\n}\n&lt;bytecode: 0x00000252ca5223f8&gt;\n&lt;environment: namespace:dplyr&gt;"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#the-flow-of-cargo",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#the-flow-of-cargo",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "transponder_ping_edge_agg &lt;- mc2_edges_agg %&gt;% \n  filter(type == \"Event.TransportEvent.TransponderPing\")\n\n\ntransponder_ping_edge_agg %&gt;% \n  filter(str_detect(target, \"cargo\")) %&gt;%\n  group_by(target) %&gt;% \n  ggplot(aes(x = target, y = source)) +\n  geom_point(aes(size = weights, color = weights)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\ncargo_nodes &lt;- mc2_nodes_raw %&gt;% \n  filter(type == \"Entity.Vessel.CargoVessel\")\n\n\ndelivery_nodes &lt;- mc2_nodes_raw %&gt;% \n  filter(type == \"Entity.Document.DeliveryReport\")\n\n\ntransaction_edges &lt;- mc2_edges_raw %&gt;% \n  filter(type == \"Event.Transaction\")\n\n\nid1 &lt;- transaction_edges %&gt;% \n  select(source) %&gt;% \n  rename(id = source)\n\nid2 &lt;- transaction_edges %&gt;% \n  select(target) %&gt;% \n  rename(id = target)\n\ndelivery_nodes1 &lt;- rbind(id1,id2) %&gt;% \n  distinct() %&gt;% \n  left_join(delivery_nodes,\n            unmatched = \"drop\")\n\n\ndelivery_nodes1 \n\n# A tibble: 5,322 × 20\n   id    type  `_last_edited_by` `_date_added` `_last_edited_date` `_raw_source`\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;             &lt;date&gt;        &lt;date&gt;              &lt;chr&gt;        \n 1 carg… Enti… Junior Shurdlu    2035-11-04    2035-11-06          Tuna Shelf/e…\n 2 carg… Enti… Harvey Janus      2035-08-17    2035-08-19          Tuna Shelf/e…\n 3 carg… Enti… Junior Shurdlu    2035-08-21    2035-08-23          Tuna Shelf/e…\n 4 carg… Enti… Melinda Manning   2035-11-07    2035-11-09          Tuna Shelf/e…\n 5 carg… Enti… Harvey Janus      2035-08-24    2035-08-25          Tuna Shelf/e…\n 6 carg… Enti… Harvey Janus      2035-08-28    2035-08-28          Tuna Shelf/e…\n 7 carg… Enti… Jack Inch         2035-09-01    2035-09-01          Tuna Shelf/e…\n 8 carg… Enti… Junior Shurdlu    2035-09-05    2035-09-06          Tuna Shelf/e…\n 9 carg… Enti… Harvey Janus      2035-09-07    2035-09-07          Tuna Shelf/e…\n10 carg… Enti… Jack Inch         2035-09-12    2035-09-13          Tuna Shelf/e…\n# ℹ 5,312 more rows\n# ℹ 14 more variables: `_algorithm` &lt;chr&gt;, name &lt;chr&gt;, Name &lt;chr&gt;,\n#   Description &lt;chr&gt;, Activities &lt;list&gt;, kind &lt;chr&gt;, qty_tons &lt;dbl&gt;,\n#   date &lt;date&gt;, flag_country &lt;chr&gt;, company &lt;chr&gt;, tonnage &lt;int&gt;,\n#   length_overall &lt;int&gt;, style &lt;chr&gt;, fish_species_present &lt;list&gt;"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#task-2",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#task-2",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Develop visualizations that illustrate the inappropriate behavior of SouthSeafood Express Corp vessels. How do their movement and catch contents compare to other fishing vessels? When and where did SouthSeafood Express Corp vessels perform their illegal fishing? How many different types of suspicious behaviors are observed? Use visual evidence to justify your conclusions."
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#illegal-fishing-by-southseafood-express-corp",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#illegal-fishing-by-southseafood-express-corp",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "SouthSeafood Express Corp operates two fishing vessels by the id of “snappersnatcher7be” and “roachrobberdb6”.\n\ntransponder_ping_edge_agg %&gt;% \n  filter(target %in% c(\"snappersnatcher7be\",\"roachrobberdb6\")) %&gt;%\n  group_by(target) %&gt;% \n  ggplot(aes(x=target, y=source,\n             size = weights)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nsouthseafood_edge &lt;- mc2_edges_raw %&gt;% \n  filter(type == \"Event.TransportEvent.TransponderPing\") %&gt;% \n  filter(target %in% c(\"snappersnatcher7be\",\"roachrobberdb6\")) %&gt;% \n  arrange(target,time)\n\n\nssf_edges_agg &lt;-\n  southseafood_edge %&gt;%\n  distinct() %&gt;%\n  group_by(source, target, type) %&gt;%\n  summarise(weights = n()) %&gt;%\n  filter(source!=target) %&gt;% \n  ungroup\n\n\nid1 &lt;- ssf_edges_agg %&gt;% \n  select(source) %&gt;% \n  rename(id = source) \n\nid2 &lt;- ssf_edges_agg %&gt;% \n  select(target) %&gt;% \n  rename(id = target)\n\nmc2_nodes1 &lt;- rbind(id1,id2) %&gt;% \n  distinct() \n\n\nssf_graph &lt;- tbl_graph(nodes = mc2_nodes1,\n                       edges = ssf_edges_agg,\n                       directed = TRUE)\n\n\n# Add a color column to nodes\nssf_graph &lt;- ssf_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(color = case_when(\n    id %in% c(\"snappersnatcher7be\", \"roachrobberdb6\") ~ \"blue\",\n    TRUE ~ \"\"\n  ))\n\n# Create the plot\nssf_graph %&gt;% \n  activate(edges) %&gt;%\n  arrange(desc(weights)) %&gt;% \n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(color = \"\", \n                     linewidth = weights)) +\n  geom_node_point(aes(color = color, size = 10)) + \n  theme_graph() +\n  theme(\n    plot.background = element_rect(fill = \"white\", color = NA),\n    text = element_text(color = \"black\"))+\n  geom_node_text(aes(label = id), \n                 repel = TRUE, \n                 vjust = 1, \n                 hjust = 1,\n                 size = 3)"
  },
  {
    "objectID": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#task-4",
    "href": "Take-home Exercise/Take-home_Ex03/Take-home_Ex03.html#task-4",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "The Questions:\n\nHow did fishing activity change after SouthSeafood Express Corp was caught?\nWhat new behaviors in the Oceanus commercial fishing community are most suspicious and why?\n\nIn order to understand the change in fishing activities, we first have to determine the date where SouthSeafood Express Corp was caught. We will use this timeline as the\nThe final activities of SouthSeafood’s vessels are on 2035-05-16 (snappersnatcher7be) and 2035-05-16 (roachrobberdb6) according to the transponder pings. Hence, we can conclude that the SouthSeafood is caught for illegal fishing, and had ceased operating its fishing vessels since 2035-05-16."
  }
]