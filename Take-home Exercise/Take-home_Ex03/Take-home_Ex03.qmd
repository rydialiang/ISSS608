---
title: "Take Home Exercise 3"
author: "Rydia"
date: "May 13, 2024"
date-modified: "last-modified"
execute:
  eval: true
  echo: true
  warning: false
  freeze: true
---

# VAST 2024 Mini Challenge 2

## Mini-Challenge 2: Creating Signatures for Geo-Temporal Patterns

Mini-challenge 2 focuses on analyzing ship movements and shipping records to understand illegal fishing practices. FishEye analysts need help creating visualizations to show patterns of ship movements and identify suspicious behaviors. They also want to understand how the commercial fishing community changed after a company was caught fishing illegally.

The details of the mini challenge can be found [here](https://vast-challenge.github.io/2024/MC2.html).

## Tasks and Questions

FishEye analysts need your help to perform geographic and temporal analysis of the CatchNet data so they can prevent illegal fishing from happening again. Your task is to develop new visual analytics tools and workflows that can be used to discover and understand signatures of different types of behavior. Can you use your tool to visualize a signature of SouthSeafood Express Corp’s illegal behavior? FishEye needs your help to develop a workflow to find other instances of illegal behavior.

1.  FishEye analysts have long wanted to better understand the flow of commercially caught fish through Oceanus’s many ports. But as they were loading data into CatchNet, they discovered they had purchased the wrong port records. They wanted to get the ship off-load records, but they instead got the port-exit records (essentially trucks/trains leaving the port area). Port exit records do not include which vessel that delivered the products. Given this limitation, develop a visualization system to associate vessels with their probable cargos. Which vessels deliver which products and when? What are the seasonal trends and anomalies in the port exit records?

2.  Develop visualizations that illustrate the inappropriate behavior of SouthSeafood Express Corp vessels. How do their movement and catch contents compare to other fishing vessels? When and where did SouthSeafood Express Corp vessels perform their illegal fishing? How many different types of suspicious behaviors are observed? Use visual evidence to justify your conclusions.

3.  To support further Fisheye investigations, develop visual analytics workflows that allow you to discover other vessels engaging in behaviors similar to SouthSeafood Express Corp’s illegal activities? Provide visual evidence of the similarities.

4.  How did fishing activity change after SouthSeafood Express Corp was caught? What new behaviors in the Oceanus commercial fishing community are most suspicious and why?

## 1.0 Data Preparation

## 1.1 Loading R Packages

```{r}
pacman::p_load(tidyverse, jsonlite, DT, lubridate,
               igraph, tidygraph, ggraph, 
               visNetwork, sf)
```

## 1.2 Loading the Data

Loading the .json data using `jsonlite` package.

```{r}
mc2_data <- fromJSON("data/MC2/mc2.json")
```

mc2 is a directed multigraph, consists of nodes dataframe and links dataframe.

```{r}
oceanus_map <- read_sf("data/MC2/Oceanus Information/Oceanus Geography.geojson")
```

Loading the oceanus map:

```{r}
ggplot(oceanus_map) +
  geom_sf(color = "black",
          ) +
  theme_void() +
  geom_sf_text(aes(label = Name), size = 2,
               vjust = 1.5)
```

## 1.3 **Wrangling and tidying edges**

### 1.3.1 Extracting edges

First, we extract only distinct edges from the tibble *links* data.frame of *mc2_data* and save it as a tibble data.frame called *mc2_edges*.

```{r}
mc2_edges <- mc2_data$links %>% 
  distinct()
```

Next, `glimpse()` of dplyr will be used to reveal the structure of *mc2_edges* tibble data.table.

```{r}
glimpse(mc2_edges)
```

From the table above, we can identify some issues with the data:

1.  The columns with date data type are all in character format.

2.  Some columns have names that starts with "\_". These need to be rename to avoid coding issues.

### 1.3.2 Correcting the date data type with `lubridate()`

```{r}
mc2_edges$time <- as_datetime(mc2_edges$time)
mc2_edges$`_last_edited_date` <- as_datetime(mc2_edges$`_last_edited_date`)
mc2_edges$`_date_added` <- as_datetime(mc2_edges$`_date_added`)
mc2_edges$date <- as_datetime(mc2_edges$date)
```

Next, glimpse() will be used to confirm if the process have been performed correctly.

```{r}
glimpse(mc2_edges)
```

### 1.3.3 Changing field name

In the code chunk below, rename() of dplyr package is used to change the following fields.

```{r}
mc2_edges <- mc2_edges %>%
  rename("last_edited_by" = "_last_edited_by",
         "date_added" = "_date_added",
         "last_edited_date" = "_last_edited_date",
         "raw_source" = "_raw_source",
         "algorithm" = "_algorithm") 
```

### 1.3.4 Splitting words in `type` column

The code chunk below combined the following steps:

1.  Splitting the words by "." - after observing that the format for type is as such: "Event.TransportEvent.TransponderPing"

2.  The `max(lengths(word_list))` will be used to find the maximum number of elements in any split.

3.  Apply function(x) to pad shorter splits with NA values to make them all the same length.

4.  Create word_df and changing column names to event1 etc.

5.  Convert word_df from matrix into tibble data.frame, and checks its class.

6.  Append word_df to mc2_edges tibble data.frame.

7.  Saving mc2_edges into R **rds** format as a physical file, so that there is no need to repeat the following code chunk to access a tidy mc2_edges tibble data frame.

```{r}
word_list <- strsplit(mc2_edges$type, "\\.")

max_elements <- max(lengths(word_list))

word_list_padded <- lapply(word_list, 
function(x) c(x, rep(NA, max_elements - length(x))))

word_df <- do.call(rbind, word_list_padded)
colnames(word_df) <- paste0("event", 1:max_elements)

word_df <- as_tibble(word_df) %>%
  select(event2, event3)
class(word_df)

mc2_edges <- mc2_edges %>%
  cbind(word_df)

# prior to running this code, create an rds folder in data folder to ensure files are saved in the correct directory
write_rds(mc2_edges, "data/rds/mc2_edges.rds")

```

## 1.4 **Wrangling and tidying nodes**

### 1.4.1 Extracting nodes

The code chunk below will be used to extract the nodes data.frame of mc2_data and parses it as a tibble data.frame called mc2_nodes.

```{r}
mc2_nodes <- as_tibble(mc2_data$nodes) %>%
  distinct()
```

Next, take a `glimpse()` to understand the data structure.

```{r}
glimpse(mc2_nodes)
```

From the table above, beside the date data type, inappropriate field name, and treatment for `type` column issues we discussed earlier, two additional data issues can be observed. They are:

-   The values in Activities and fish_species_present fields are in **list** data type, which will affect the ability to process and to analyse the data.

-   Some values in the Activities field are not ready to be analyse without further tidying (i.e. removing c(““)).

We will first repeat the steps similar steps to wrangling the mc2_edges, before proceeding to tackle the issues for Activities and fish_species_present field.

### 1.4.2 Correcting the date data type with `lubridate()`

Correct the date data type and take a `glimpse()` to confirm changes.

```{r}
mc2_nodes$`_last_edited_date` <- as_datetime(mc2_nodes$`_last_edited_date`)
mc2_nodes$`_date_added` <- as_datetime(mc2_nodes$`_date_added`)
mc2_nodes$date <- as_datetime(mc2_nodes$date)
glimpse(mc2_nodes)
```

### 1.4.3 Changing field name

In the code chunk below, rename() of dplyr package is used to change the following fields.

```{r}
mc2_nodes <- mc2_nodes %>%
  rename("last_edited_by" = "_last_edited_by",
         "date_added" = "_date_added",
         "last_edited_date" = "_last_edited_date",
         "raw_source" = "_raw_source",
         "algorithm" = "_algorithm") 
```

### 1.4.4 Splitting words in `type` column

Details on the code chunk can be found in section 1.3.4. At this point, we will not be saving the mc2_nodes as R **rds** format yet, as there are more works to be done to clean up the dataframe.

```{r}
word_list <- strsplit(mc2_nodes$type, "\\.")

max_elements <- max(lengths(word_list))

word_list_padded <- lapply(word_list, 
function(x) c(x, rep(NA, max_elements - length(x))))

word_df <- do.call(rbind, word_list_padded)
colnames(word_df) <- paste0("entity", 1:max_elements)

word_df <- as_tibble(word_df) %>%
  select(entity2, entity3)
class(word_df)

mc2_nodes <- mc2_nodes %>%
  cbind(word_df)
```

### 1.4.5 Tidying text field

Using `mutate()` of dplyr and `gsub()` of Base R to tidy up the values in the cell. Essentially, the unwanted characters like `c`, `(`, `)`, and `\` are removed by substituting with empty value `""` for both Activities and fish_species_present columns. What is left in the columns will be characters separated by `,`.

```{r}
mc2_nodes <- mc2_nodes %>%
  mutate(Activities = gsub("c[(]", "", Activities)) %>% 
  mutate(Activities = gsub("\"", "", Activities)) %>%
  mutate(Activities = gsub("[)]", "", Activities)) 
```

```{r}
mc2_nodes <- mc2_nodes %>%
  mutate(fish_species_present = gsub("c[(]", "", fish_species_present)) %>% 
  mutate(fish_species_present = gsub("\"", "", fish_species_present)) %>%
  mutate(fish_species_present = gsub("[)]", "", fish_species_present)) 
```

Lastly, we will save the tidied mc2_nodes

```{r}
# prior to running this code, create an rds folder in data folder to ensure files are saved in the correct directory
write_rds(mc2_nodes, "data/rds/mc2_nodes.rds")
```

## 1.5 Extracting the required columns for each graph

In this section, we will extract the required column for the following graphs:

1.  Vessel Movements

2.  Harbor Reports

3.  Harbor Import Records

### 1.5.1 Vessel Movements

**Vessel Movements:** Oceanus is outfitted with a transponder/ping system named the Oceanus Vessel Locator System (OVLS).  Vessels are outfitted with a transponder and periodic 'pings' from base-stations results in a report of vessel locations at any time.  The raw ping granularity is at the minute-level but post-processing has converted it into visit/dwell times. OVLS is generally reliable, though vessel records may be missing for a variety of reasons.

Node/Edge types and properties present

1.  Entity.Vessel: Description of the vessel
2.  Entity.Location: Description of a geographic location
3.  Event.TransponderPing: Links a vessel to a location

First, we will extract the relevant nodes, namely the vessels and locations from `mc2_nodes`. As we are only concerned about the fishing vessels, we will only extract values matching "Vessel" in entity2 column and values matching "FishingVessel" in entity3 column. For locations, we will match values of "Location" in entity2 column, and match values of "City", "Point" and "Region" in entity3 column.

```{r}
vessel_mvmt_nodes <- mc2_nodes %>% 
  filter(entity2 %in% c("Vessel","Location")) %>% 
  filter(entity3 %in% c("FishingVessel","City","Point","Region"))
```

Next, we will extract the vessel movement edges from mc2_edges, by filtering the "TransponderPing" from event3 column.

```{r}
vessel_mvmt_edges <- mc2_edges %>% 
  filter(event3 %in% c("TransponderPing"))
```

### 1.5.2 Harbor Reports

**Harbor Reports:** Harbor masters regularly report the vessels found in their purview anytime during the day.  This data is derived from a different system than OVLS (see "Vessel Movements"), though the data overlaps.  Harbor Reports are provided on a different schedule from different harbors. Since no harbor reports every day, this data has lower temporal granularity than vessel movement data. Additionally, the Harbor Master is also responsible for proximate navigational beacon(s), so this data has lower spatial granularity as well.  However, the list of vessels observed is considered canonical.

Node/edge types present:

1.  Entity.Vessel

2.  Entity.location

3.  Event.HarborReport

Since the node type are the same as the vessel movements, we will make a copy of the node from vessel movement nodes.

```{r}
harbor_report_nodes <- vessel_mvmt_nodes
```

Next, we will extract the harbor report edges from mc2_edges, by filtering the "HarborReport" from event3 column.

```{r}
harbor_report_edges <- mc2_edges %>% 
  filter(event3 %in% c("HarborReport"))
```

### 1.5.3 Harbor Import Records

**Harbor Import Records**: Vessels deliver cargo to the ports, and that cargo is brought into Oceanus.  These records reflect the goods that \*leave\* the harbor to go to businesses in Oceanus or to be exported.  It was filtered pre-ingest to focus on the delivery of raw fish.  Because it is raw, fish leave the port quickly (generally one day after delivery).  Due to clerical error, the records purchased by FishEye do not include the vessel that delivered the cargo.

Node/Edge types present:

1.  Entity.location

2.  Entity.Commodity.Fish

3.  Entity.Document.DeliveryReport

4.  Event.Transaction

First, we will extract the relevant nodes, namely the location, commodity.fish and document.delivery report from `mc2_nodes`.

```{r}
harbor_import_records_nodes <- mc2_nodes %>% 
  filter(entity2 %in% c("Location","Commodity","Document"))
```

Next, we will extract harbor import records edges, by filtering the event2 with value of "Transaction".

```{r}
harbor_import_records_edges <- mc2_edges %>% 
  filter(event2 == "Transaction")
```

Before we move on to exploring the data, we will save the 3 sources edges and nodes tibble data frame as R **rds** format in the data/rds folder:

```{r}
write_rds(vessel_mvmt_nodes, "data/rds/vessel_mvmt_nodes.rds")
write_rds(vessel_mvmt_edges, "data/rds/vessel_mvmt_edges.rds")
write_rds(harbor_report_nodes, "data/rds/harbor_report_nodes.rds")
write_rds(harbor_report_edges, "data/rds/harbor_report_edges.rds")
write_rds(harbor_import_records_nodes, "data/rds/harbor_import_records_nodes.rds")
write_rds(harbor_import_records_edges, "data/rds/harbor_import_records_edges.rds")
```

## 2.0 Task 1: Flow of Commercially Caught Fish

In this section, we focus on a few key areas to understand how the commercially caught fish flows from the vessels through the various ports:

1.  Associating the vessels with their probable cargoes
2.  Which vessels deliver which products and when?
3.  Examine the seasonal trends and anomalies in the port exit records

## 2.1 Associating the Vessels and their Probable Cargoes

To find out which commodity goes to which ports, we first create a cargo list that links the cargo to the cities and commodities.

```{r}
cargo_port_list <- harbor_import_records_edges %>% 
  select(source,target) %>% 
  filter(target %in% c("City of Haacklee",
                       "City of Lomark",
                       "City of Himark",
                       "City of Paackland",
                       "City of South Paackland",
                       "City of Port Grove"))

cargo_commodity_list <- harbor_import_records_edges %>% 
  select(source,target) %>% 
  filter(!target %in% c("City of Haacklee",
                       "City of Lomark",
                       "City of Himark",
                       "City of Paackland",
                       "City of South Paackland",
                       "City of Port Grove")) %>% 
  rename(commodity = target)

cargo_list <- cargo_port_list %>% 
  left_join(cargo_commodity_list) %>%
  left_join(harbor_import_records_edges) %>% 
  select(source, target, commodity, date) %>% 
  rename(cargo = source, city = target) 

cargo_list<- harbor_import_records_nodes %>% 
  rename(commodity = id) %>% 
  select(name, commodity) %>% 
  left_join(cargo_list)

```

```{r}
cargo_list
```

```{r}

```
